[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Junchuan Yu",
    "section": "",
    "text": "My name is Junchuan Yu, and I am a remote sensing researcher.\n\n\nI graduated from CUGB, and my research involves hyperspectral remote sensing, deep learning, system development and geological applications, etc.\n\n\nI am very happy to share the knowledge about data science and remote sensing with more people here.\n\n\nWelcome to follow the WeChat public account [45度科研人] and contact me!"
  },
  {
    "objectID": "posts/2022-10-8_ai_poem/index.html",
    "href": "posts/2022-10-8_ai_poem/index.html",
    "title": "AI眼中的诗意——AI绘画",
    "section": "",
    "text": "1. AI绘画大火\n\n\n2016 年，Memo Akten 和 Mike Tyka 等人工智能艺术家在在旧金山展出了由谷歌的 Deep Dream 算法生成的图像。短短两年时间，人工智能艺术作品登上了世界艺术舞台。\n2018 年，一幅由人工智能生成的图像在佳士得拍卖，这是一幅由巴黎的 Obvious 集体制作的渲染版画。埃德蒙·贝拉米肖像拍卖是第一次被广泛报道的人工智能艺术品拍卖。\n\n\n\n\n\n\n\n\n2022年被称为AI绘画元年。美国科罗拉多州博览会的一项美术比赛上，一等奖竟然颁给了AI自动生成的画作。作品全名《太空歌剧院》，由一位名叫Jason Allen的游戏设计师提交，作品利用AI软件 MidJourney，经过了近900次的迭代，数周的挑选与调整，才最终被打印到画布上，参与了这次比赛。 \n无独有偶，2022年戛纳电影短片节的最佳短片，也颁给了AI人工智能生成的作品《THE CROW》（乌鸦）\nDALL-E 2、Midjourney和StableDiffusion等AI绘画工具，让业余人士只要简单打几个关键字，就能够创作出复杂、或抽象、或写实的艺术作品。下面是一幅有AI生成的中国山水画。 \n\n\n\n2. AI眼中的诗意\n\n\nAI能理解我们的古诗吗？以滕王阁序为例，看看AI眼中“落霞与孤鹜齐飞，秋水共长天一色”这两句优美的诗句是怎样的。下面是由AI给出的部分答案：\n\n\n\n\n\n\n\n\n\n3.AI绘画原理\n\n\nDiffusionModel已取代GAN称为图像生成及AI绘画领域的新宠儿，引发了扩散模型的研究热潮，目前采用的扩散模型大都是来自于2020年的的工作DDPM: Denoising Diffusion Probabilistic Models。简单来说，扩散模型包含两个过程：前向扩散过程和反向生成过程，前向扩散过程是对一张图像逐渐添加高斯噪音直至慢慢抹去图像中所有可辨别的细节，直到变成纯粹的“噪点”，而反向生成过程是去噪音过程，逐渐对图像进行去像素化来学习逆转噪声并恢复图像。  \n正如谷歌解释的那样：\n\n\n“Running this reversed corruption process synthesizes data from pure noise by gradually denoising it until a clean sample is produced.”\n\n\n训练模型的核心数据集则是 LAION-high-resolution 和 LAION-Aesthetics。使用 AWS 提供的 4000 块 A100 显卡组成的强力计算集群，花费约 15 万小时的训练完成了第一个版本。具体技术细节请移步：Stable_diffusion, diffusion_model\n如何深入学习扩散模型的原理？fast.ai将和 Huggingface, Stability.ai等各方一起创作一门新课程，叫做 From Deep Learning Foundations to Stable Diffusion。这门课将会用新的方式让普通人从原理上理解 Stable Diffusion 模型。课程详细介绍请移步：传送门\n\n\n\n4. 人们对于AI绘画的看法？\n\n\n不少艺术家认为AI绘画这是一种作弊：“我们正在目睹艺术的消亡。如果创造性的工作在机器面前都不安全，那么即使你有高技能，也有被淘汰的危险。到那时，我们将拥有什么？”\n网友更是直言：“从AI目前的进度来看，行业被挤压是迟早的事情了。那些嘲讽人类艺术家的人，当你被替代感到危机的时候，你一样会发出无能的怒吼”\n懂得灵活运用AI绘画作为创作工具的艺术家们认为：“这项技术具有改变我们交流方式的巨大潜力，我们期待与大家一起建立一个更快乐、更具交流性和创造性的未来”\n除了图像生成之外，复杂的 AI 生成视频模型也已出现。不久前，Meta推出了AI系统“Make-a-Video”，可以由文本、图像生成短视频，也可以改变现有视频的细节来生成新的视频。现在，谷歌推出了新的视频生成模型“Imagen Video”，可实现每秒24帧的速度下达到1280 x 768像素的高清视频，这又会对当下火爆的对短视频领域带来哪些机遇和挑战呢？ \n\n\n\n5. 总结\n\n\nAI绘画取得的效果是令人惊叹的；\n模型本身还是依然于海量的数据集，而这些数据库中所包含的图像和文字还是不全面的，因而无法解析数据库之外的词句以及绘画风格，比如“齐天大圣”，“岩彩画”。长远来看其对于日常实物基本可以做到准确的模拟，但对于场景和复杂语义的理解需要更多的努力，其中一个必要的途径是构建更为广泛的且细粒化的数据集，也许不久能够实现AI利用互联网资源进行自学习；\nAi绘画成功的背后是扩散模型，而扩散模型的思路是可以用到其他领域的，比如同样拥有海量数据及视觉属性的遥感领域，试想一下利用遥感数据对扩散模型进行迁移学习能否实现遥感场景的变换，这种新的数据生成技术能否为小样本识别问题提供解决方案呢？\nAI绘制“落霞与孤鹜齐飞，秋水共长天一色”图片下载链接：百度云, 提取码：1234\n\n\nReferences\n\n\n《THE CROW》: https://www.bilibili.com/video/BV16P411V7Ah?share_source=copy_web\nStable_diffusion: https://huggingface.co/blog/stable_diffusion\ndiffusion_model: https://theaisummer.com/diffusion-models/\n传送门: https://www.fast.ai/posts/part2-2022.html"
  },
  {
    "objectID": "posts/2022-11-15_wrsdp_3-3/index.html",
    "href": "posts/2022-11-15_wrsdp_3-3/index.html",
    "title": "利用多光谱遥感数据进行地物分类",
    "section": "",
    "text": "利用多光谱遥感数据进行地物分类\n本课程目的是利用神经网络开展多光谱地物分类，学习从整幅卫星影像制作样本数据集，训练多分类神经网络，并实现对整景卫星数据的预测。 运行该代码提前需要安装以下几个必要的库：\n\nnumpy\ntensorflow = 2.5\nh5py = 3.1\nPillow = 8.4\n\n\nimport h5py\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.optimizers import RMSprop,Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau,CSVLogger\nfrom tensorflow.keras.models import load_model\nimport matplotlib.pyplot as plt\nimport time,glob\nfrom model import all_model\nfrom utils import *\n\n\n1. 加载并查看数据\n\n## 加载数据集，数据集维度为3840×3840×4，最后一个波段是真值标签，标签一共有六类，“”\nhdf5_path = \"./data/kaggle_14_3b_5c.hdf5\"\nfd = h5py.File(hdf5_path, 'r')\nfd.keys()\nimages=fd['image']\nlabels=fd['label']\nn_label=len(np.unique(labels)) #{0:'buiding',1:'Road', 2:'Tree',3: 'Crops',4:'Water'}\n## 该影像是反射率数据（通常数值在0-1之间），为了节省存储空间常将数值放大10000倍，保存为无符号整型数据\nprint(np.max(images),np.min(images),np.max(labels),np.min(labels))\nimages=np.array(images)\nlabels=np.array(labels)\nprint(images.shape,labels.shape)\n\n9995 213 4 0\n(14, 3840, 3840, 3) (14, 3840, 3840)\n\n\n\n## 将整幅影像及标签数据打印出来,为了提升原始影像的显示效果，对原有数据进行拉伸处理 \n\ndef stretch_n(band, lower_percent=5, higher_percent=95): #5和95表示分位数\n    band=np.array(band,dtype=np.float32)\n    c = np.percentile(band, lower_percent)*1.0\n    d = np.percentile(band, higher_percent)*1.0       \n    band[band<c] = c\n    band[band>d] = d\n    out =  (band - c)  / (d - c)  \n    return out.astype(np.float32)\n\ndef adjust_contrast(data,n_band=3):    #通过循环对各个波段进行拉伸\n    data=np.array(data,dtype=np.float32)\n    for img in data:\n        for k in range(n_band):\n            img[:,:,k] = stretch_n(img[:,:,k])\n    return data\n\nnewimg=adjust_contrast(images.copy()) #该操作讲改变原始数据，因此用.copy，不对原始数据进行更改\nprint(np.max(images),np.max(newimg))\nshow_5_images(images/10000,labels)#plot函数要求数据为0-1之间的浮点型或0-255的8位整型数据\nshow_5_images(newimg,labels)\n\n\n9995 1.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2. 数据切片\n\n## 定义随机裁剪和顺序裁剪两种方式，顺序裁剪是按照固定步长沿行列循环裁剪，切片数是有限的，随机裁剪是以随机点为起始点裁剪，切片数可以是无限的且可自定义，后者的好处是可以通过增加算法约束label中某一类的数量来实现精准的样本获取。\ndef random_crop(image,crop_sz):\n    img_sz=image.shape[:2]\n    random_x = np.random.randint(0,img_sz[0]-crop_sz+1) ##生成随机点\n    random_y = np.random.randint(0,img_sz[1]-crop_sz+1)\n    s_img = image[random_x:random_x+crop_sz,random_y:random_y+crop_sz,:] ##以随机点为起始点生成样本框，进行切片\n    return s_img\n\ndef data_crop_random(img_arr,crop_sz,n_patch):   \n    c = img_arr.shape[-1]\n    data = np.zeros([n_patch, crop_sz, crop_sz, c])\n    for j in np.arange(n_patch):\n        image = random_crop(img_arr,crop_sz)\n        data[ j,:,:,:] = image\n    return data\ndef sequential_crop(imagearray,crop_sz,step=256):\n    data = []\n    x=0\n    row_num = ((imagearray.shape)[0] - step) // step  ##最多能裁剪几行 几列\n    col_num=((imagearray.shape)[1] - step) // step\n    x_start=0\n    y_start=0\n    for h in range(row_num):\n        for w in range(col_num):\n            crop_img = imagearray[crop_sz*h+y_start:crop_sz*(h+1)+y_start, crop_sz*w+x_start:crop_sz*(w+1)+x_start,:] ##行列循环，滑动窗口移动              \n            data.append(crop_img)\n            x=x+1\n    data=np.array(data)\n    return data\ndef data_crop(imagearray,crop_sz,stride,random=None,n_patch=250):   #设置random选项，用来切换是否采用随机裁切\n    data = []\n    for i in range(imagearray.shape[0]):\n        if random:\n            image=data_crop_random(imagearray[i,:,:,:],crop_sz,n_patch)\n        else:\n            image =sequential_crop(imagearray[i,:,:,:],crop_sz,stride)\n        if i == 0:\n            data  = image ##注意当i=0的时候需要将image赋给data，否则data依然是空，不可以进行concatnate\n        else:\n            data  = np.concatenate((data, image), axis = 0) \n        print(\"patch processing....:\"+str(i))\n    data=np.array(data,dtype=np.float32)\n    print(\"final processed:\"+str(i)+\"...No.:\"+str(data.shape[0]))    \n    return data\n\n\n## 合并images和labels方便切片\ndata_all = np.concatenate((newimg, np.expand_dims(labels,axis=-1)), axis = -1)\nstride=256\ncropsize=256\nall_patches=data_crop(data_all,cropsize,stride,random=False)##保留2景数据做测试\nprint(data_all.shape,all_patches.shape)\ni=0\n\npatch processing....:0\npatch processing....:1\npatch processing....:2\npatch processing....:3\npatch processing....:4\npatch processing....:5\npatch processing....:6\npatch processing....:7\npatch processing....:8\npatch processing....:9\npatch processing....:10\npatch processing....:11\npatch processing....:12\npatch processing....:13\nfinal processed:13...No.:2744\n(14, 3840, 3840, 4) (2744, 256, 256, 4)\n\n\n\n##调用utils中的plot_func查看数据与label是否对应,反复运行这个cell进行数据浏览i表示每次浏览跨越数据的个数\nplot_func(all_patches[i:i+20,:,:,:3],all_patches[i:i+20:,:,:,-1])\ni+=500\n\n\n\n\n\n\n\n\nall_patches=suffle_data(all_patches) #对数据进行打乱处理\n\n(2744, 256, 256, 4)\n\n\n\n##可以选择将数据保存为h5文件，方便后续使用\nhdf5_path = './data/patches_rgb_4b_5c.hdf5' \nf = h5py.File(hdf5_path, mode='w')\nf['image'] = all_patches[:,:,:,0:-1]\nf['label'] = all_patches[:,:,:,-1]\nf.close()\n\n\n\n3. 模型训练\n\n# hdf5_path = \"./data/patches_rgb_4b_6c.hdf5\"\n# fd = h5py.File(hdf5_path, 'r')\n# fd.keys()\n# images=np.array(fd['image'])\n# labels=np.array(fd['label'])#3:tree,4:road,5:crop\n\n\n## 对数据进行归一化处理，并将label转成one-hot标签形式\n# n_label=5\ndef post_normalize_image(images,labels,n_label=n_label):\n    msk = label_hot(labels,n_label)\n    img = images\n    return img,msk\n# img,msk=post_normalize_image(images,labels,n_label)\nimg,msk=post_normalize_image(all_patches[:,:,:,0:-1],all_patches[:,:,:,-1],n_label)\n## 将数据集按照7:3进行划分\nxtrain,xtest,ytrain,ytest=train_test_split(img,msk,test_size=0.2,random_state=42)\ndel img,msk #如果数据较大可以在此删除降低内存\nprint(xtrain.shape,xtest.shape,ytrain.shape,ytest.shape)\ni=0\n\n(2195, 256, 256, 3) (549, 256, 256, 3) (2195, 256, 256, 5) (549, 256, 256, 5)\n\n\n\nplot_func(xtrain[i:i+20,:,:,:3],np.argmax(ytrain,axis=-1)[i:i+20:,:,:])\ni+=500\n\n\n\n\n\n\n\n\n## 设定必要的参数\nloss='categorical_crossentropy'\noptimizer=Adam()\nloss_weights=''\nmetrics=['accuracy']\nbatch_size=20\nepoch= 50\ninput_height=xtrain.shape[1]\ninput_width=xtrain.shape[2]\nnchannel=xtrain.shape[-1]\nnum_train=xtrain.shape[0]\nnum_val=xtest.shape[0]\nATM=all_model(loss,loss_weights,optimizer,metrics,input_height,input_width,n_label,nchannel) #向all_model传参返回模型\nmodelname='unet'\n\n\n## 加载UNET模型\nmodel=ATM.UNET()#deeplabv3p,UNET,dlinknet,convattunet\n# model.summary()\n\n\n## 开始训练\nbegin_time = time.time()          \nout_dir = \"./checkpoint/\"+modelname+'/'\nif not os.path.exists(out_dir):\n    os.makedirs(out_dir) \n## 设置model-checkpoint用来存储模型参数文件\nmodel_checkpoint = ModelCheckpoint(filepath=out_dir+\"-{epoch:02d}e-val_loss{val_loss:2f}.hdf5\",monitor=\"val_loss\",save_best_only=True,mode='auto')\n## 设置csvlogger用来记录训练记录\ncsvlogger =CSVLogger(filename=out_dir+modelname+'-'+str(epoch)+'-log.csv', separator=',', append=False)\n## model_checkpoint和csvlogger要想发挥作用必须放入callback中\nresult=model.fit(xtrain, ytrain, batch_size=batch_size, epochs=epoch, verbose=1, shuffle=True,validation_data=(xtest, ytest),callbacks=[model_checkpoint,csvlogger])\nend_time = time.time()\nprint('time lapsing {0} s \\n'.format(end_time - begin_time))\n\nEpoch 1/50\n110/110 [==============================] - 42s 275ms/step - loss: 1.1495 - accuracy: 0.6211 - val_loss: 0.8305 - val_accuracy: 0.7096\nEpoch 2/50\n110/110 [==============================] - 22s 198ms/step - loss: 0.8297 - accuracy: 0.7023 - val_loss: 0.8479 - val_accuracy: 0.7174\nEpoch 3/50\n110/110 [==============================] - 23s 213ms/step - loss: 0.7564 - accuracy: 0.7295 - val_loss: 0.7701 - val_accuracy: 0.7465\nEpoch 4/50\n110/110 [==============================] - 23s 209ms/step - loss: 0.7241 - accuracy: 0.7353 - val_loss: 0.7346 - val_accuracy: 0.7350\nEpoch 5/50\n110/110 [==============================] - 23s 209ms/step - loss: 0.6612 - accuracy: 0.7560 - val_loss: 0.6612 - val_accuracy: 0.7651\nEpoch 6/50\n110/110 [==============================] - 22s 199ms/step - loss: 0.6603 - accuracy: 0.7573 - val_loss: 0.6624 - val_accuracy: 0.7571\nEpoch 7/50\n110/110 [==============================] - 23s 211ms/step - loss: 0.6069 - accuracy: 0.7803 - val_loss: 0.6109 - val_accuracy: 0.7743\nEpoch 8/50\n110/110 [==============================] - 23s 209ms/step - loss: 0.5959 - accuracy: 0.7837 - val_loss: 0.5707 - val_accuracy: 0.7895\nEpoch 9/50\n110/110 [==============================] - 23s 211ms/step - loss: 0.5670 - accuracy: 0.7965 - val_loss: 0.5354 - val_accuracy: 0.8031\nEpoch 10/50\n110/110 [==============================] - 22s 196ms/step - loss: 0.5354 - accuracy: 0.8077 - val_loss: 0.5420 - val_accuracy: 0.8020\nEpoch 11/50\n110/110 [==============================] - 22s 197ms/step - loss: 0.5269 - accuracy: 0.8084 - val_loss: 0.5449 - val_accuracy: 0.8104\nEpoch 12/50\n110/110 [==============================] - 23s 208ms/step - loss: 0.5232 - accuracy: 0.8107 - val_loss: 0.5206 - val_accuracy: 0.8109\nEpoch 13/50\n110/110 [==============================] - 23s 210ms/step - loss: 0.4868 - accuracy: 0.8268 - val_loss: 0.4856 - val_accuracy: 0.8252\nEpoch 14/50\n110/110 [==============================] - 22s 198ms/step - loss: 0.4827 - accuracy: 0.8273 - val_loss: 0.6254 - val_accuracy: 0.7814\nEpoch 15/50\n110/110 [==============================] - 22s 198ms/step - loss: 0.4961 - accuracy: 0.8224 - val_loss: 0.4901 - val_accuracy: 0.8262\nEpoch 16/50\n110/110 [==============================] - 23s 209ms/step - loss: 0.4700 - accuracy: 0.8329 - val_loss: 0.4735 - val_accuracy: 0.8256\nEpoch 17/50\n110/110 [==============================] - 22s 197ms/step - loss: 0.4603 - accuracy: 0.8339 - val_loss: 0.4862 - val_accuracy: 0.8220\nEpoch 18/50\n110/110 [==============================] - 22s 196ms/step - loss: 0.4476 - accuracy: 0.8389 - val_loss: 0.4996 - val_accuracy: 0.8220\nEpoch 19/50\n110/110 [==============================] - 23s 210ms/step - loss: 0.4218 - accuracy: 0.8471 - val_loss: 0.4579 - val_accuracy: 0.8304\nEpoch 20/50\n110/110 [==============================] - 22s 197ms/step - loss: 0.3968 - accuracy: 0.8580 - val_loss: 0.4725 - val_accuracy: 0.8203\nEpoch 21/50\n110/110 [==============================] - 22s 199ms/step - loss: 0.4013 - accuracy: 0.8550 - val_loss: 0.4729 - val_accuracy: 0.8256\nEpoch 22/50\n110/110 [==============================] - 23s 210ms/step - loss: 0.3659 - accuracy: 0.8679 - val_loss: 0.4542 - val_accuracy: 0.8362\nEpoch 23/50\n110/110 [==============================] - 22s 197ms/step - loss: 0.3730 - accuracy: 0.8648 - val_loss: 0.4884 - val_accuracy: 0.8149\nEpoch 24/50\n110/110 [==============================] - 23s 210ms/step - loss: 0.3559 - accuracy: 0.8716 - val_loss: 0.4249 - val_accuracy: 0.8431\nEpoch 25/50\n110/110 [==============================] - 22s 198ms/step - loss: 0.3223 - accuracy: 0.8838 - val_loss: 0.4684 - val_accuracy: 0.8340\nEpoch 26/50\n110/110 [==============================] - 22s 198ms/step - loss: 0.3408 - accuracy: 0.8776 - val_loss: 0.4931 - val_accuracy: 0.8341\nEpoch 27/50\n110/110 [==============================] - 22s 197ms/step - loss: 0.2869 - accuracy: 0.8964 - val_loss: 0.4596 - val_accuracy: 0.8429\nEpoch 28/50\n110/110 [==============================] - 22s 197ms/step - loss: 0.2717 - accuracy: 0.9020 - val_loss: 0.4739 - val_accuracy: 0.8430\nEpoch 29/50\n110/110 [==============================] - 22s 199ms/step - loss: 0.2566 - accuracy: 0.9070 - val_loss: 0.4645 - val_accuracy: 0.8507\nEpoch 30/50\n110/110 [==============================] - 22s 199ms/step - loss: 0.2469 - accuracy: 0.9102 - val_loss: 0.5344 - val_accuracy: 0.8413\nEpoch 31/50\n110/110 [==============================] - 22s 198ms/step - loss: 0.2762 - accuracy: 0.9023 - val_loss: 0.5081 - val_accuracy: 0.8386\nEpoch 32/50\n110/110 [==============================] - 22s 198ms/step - loss: 0.2831 - accuracy: 0.8983 - val_loss: 0.4640 - val_accuracy: 0.8485\nEpoch 33/50\n110/110 [==============================] - 22s 198ms/step - loss: 0.2424 - accuracy: 0.9125 - val_loss: 0.4504 - val_accuracy: 0.8548\nEpoch 34/50\n110/110 [==============================] - 22s 197ms/step - loss: 0.2218 - accuracy: 0.9193 - val_loss: 0.4752 - val_accuracy: 0.8492\nEpoch 35/50\n110/110 [==============================] - 22s 198ms/step - loss: 0.2124 - accuracy: 0.9227 - val_loss: 0.4808 - val_accuracy: 0.8529\nEpoch 36/50\n110/110 [==============================] - 22s 198ms/step - loss: 0.2016 - accuracy: 0.9262 - val_loss: 0.5387 - val_accuracy: 0.8507\nEpoch 37/50\n110/110 [==============================] - 22s 196ms/step - loss: 0.1973 - accuracy: 0.9278 - val_loss: 0.5704 - val_accuracy: 0.8502\nEpoch 38/50\n110/110 [==============================] - 22s 197ms/step - loss: 0.1797 - accuracy: 0.9334 - val_loss: 0.5250 - val_accuracy: 0.8552\nEpoch 39/50\n110/110 [==============================] - 22s 198ms/step - loss: 0.1729 - accuracy: 0.9356 - val_loss: 0.5235 - val_accuracy: 0.8561\nEpoch 40/50\n110/110 [==============================] - 22s 197ms/step - loss: 0.1718 - accuracy: 0.9359 - val_loss: 0.5590 - val_accuracy: 0.8421\nEpoch 41/50\n110/110 [==============================] - 22s 197ms/step - loss: 0.1823 - accuracy: 0.9320 - val_loss: 0.5435 - val_accuracy: 0.8470\nEpoch 42/50\n110/110 [==============================] - 22s 198ms/step - loss: 0.1711 - accuracy: 0.9358 - val_loss: 0.5568 - val_accuracy: 0.8570\nEpoch 43/50\n110/110 [==============================] - 22s 197ms/step - loss: 0.1513 - accuracy: 0.9426 - val_loss: 0.6066 - val_accuracy: 0.8564\nEpoch 44/50\n110/110 [==============================] - 22s 198ms/step - loss: 0.1386 - accuracy: 0.9466 - val_loss: 0.5596 - val_accuracy: 0.8534\nEpoch 45/50\n110/110 [==============================] - 22s 197ms/step - loss: 0.1423 - accuracy: 0.9451 - val_loss: 0.6868 - val_accuracy: 0.8247\nEpoch 46/50\n110/110 [==============================] - 22s 196ms/step - loss: 0.2242 - accuracy: 0.9180 - val_loss: 0.5130 - val_accuracy: 0.8211\nEpoch 47/50\n110/110 [==============================] - 22s 197ms/step - loss: 0.2567 - accuracy: 0.9064 - val_loss: 0.4996 - val_accuracy: 0.8435\nEpoch 48/50\n110/110 [==============================] - 22s 198ms/step - loss: 0.1763 - accuracy: 0.9341 - val_loss: 0.5540 - val_accuracy: 0.8556\nEpoch 49/50\n110/110 [==============================] - 22s 200ms/step - loss: 0.1357 - accuracy: 0.9472 - val_loss: 0.5536 - val_accuracy: 0.8574\nEpoch 50/50\n110/110 [==============================] - 22s 198ms/step - loss: 0.1193 - accuracy: 0.9528 - val_loss: 0.5919 - val_accuracy: 0.8576\ntime lapsing 1127.027404308319 s \n\n\n\n\n## 打印训练曲线，确认训练效果，精度不够，loss不收敛，模型学习能力不足且容易过拟合\ndef plot_fig(H,outdir):\n    N=len(H.history['loss'])\n    plt.style.use(\"ggplot\")\n    plt.figure(figsize=(10,6))\n    plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n    plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n    plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n    plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n    plt.ylim(0,1)\n    plt.title(\"Training Loss and Accuracy\")\n    plt.xlabel(\"Epoch #\")\n    plt.ylabel(\"Loss/Accuracy\")\n    plt.legend(loc=\"lower left\")\n    plt.savefig(outdir)\nplot_fig(result,modelname+\"_Loss_Acc_epoch.png\")\n\n\n\n\n\n\n4. 评价测试\n\n## 训练过程只保留最有性能参数文件，因此从训练记录里选择最后一个即可\nh5=glob.glob(\"./checkpoint/\"+modelname+\"/*.hdf5\")\nmodel_list=np.sort(h5)\nmodel=load_model(model_list[-1])\n# model=load_model(model_list[-1],custom_objects={'interpolation':interpolation})#keras导入模型需要判断是会否有自定义函数或层，有的话需要在custom_objects中定义，并编译\nprint(model_list[-1])\n\n./checkpoint/unet\\-24e-val_loss0.424916.hdf5\n\n\n\n## 首先对切片数据进行预测，查看效果\ni=100\npred=model.predict(xtest,batch_size=64)\npred=np.argmax(pred,axis=-1)\ngt=np.argmax(ytest,axis=-1)\n\n9/9 [==============================] - 9s 530ms/step\n\n\n\n##逐批次查看预测效果\nval_plot_func(xtest[i:i+20],gt[i:i+20],pred[i:i+20])\ni+=100\n\n\n\n\n\n\n\n\n\n\n\n## 用测试集评价模型精度\ndf = call_matric(pred,gt, [0,1,2,3,4, 'all'])\nprint(df)\n\n     precision    recall  F1-score       iou        oa      miou\n0     0.850384  0.895335  0.872280  0.773491  0.843104  0.519765\n1     0.716035  0.285363  0.408089  0.256352  0.843104  0.519765\n2     0.754666  0.684739  0.718004  0.560067  0.843104  0.519765\n3     0.862850  0.904145  0.883015  0.790534  0.843104  0.519765\n4     0.466484  0.291084  0.358479  0.218382  0.843104  0.519765\nall   0.730084  0.612133  0.647973  0.519765  0.843104  0.519765\n\n\n\n\n4. 优化改进\n\n4.1 数据优化\n\nbuild_num = np.sum(labels ==0)\nroad_num = np.sum(labels == 1)\ntree_num = np.sum(labels == 2)\ncrop_num = np.sum(labels == 3)\nwater_num = np.sum(labels == 4)\n# 这两行代码解决 plt 中文显示的问题\nplt.rcParams['font.sans-serif'] = ['SimHei']\nplt.rcParams['axes.unicode_minus'] = False\nplt.style.use(\"ggplot\")\n\nclasses = ('0-Building', '1-Road', '2-Tree', '3-Crop', '4-Riveer')\nnumbers = [build_num,road_num, tree_num,crop_num, water_num]\nprint(numbers)\nplt.barh(classes, numbers,color='lightblue')\nplt.title('Number of pixels in each category')\n# plt.savefig(\"Number-category.png\", dpi = 600, bbox_inches=\"tight\")\nplt.show()\n\n[99016105, 11047398, 17424954, 76998497, 1951446]\n\n\n\n\n\n\n## 定义随机裁剪增加对于label中1和4样本的采集,\"num_count(image[:,:,-1],1)\"表示切片中数值为1的像元个数\ndef data_crop_random2(img_arr,crop_sz,n_patch):   \n    data =[]\n    k=0\n    for j in np.arange(1000):\n        image = random_crop(img_arr,crop_sz)\n        if num_count(image[:,:,-1],1) +num_count(image[:,:,-1],4) >8000:\n            data.append(image)\n            k+=1\n            if k==n_patch:\n                break                 \n    if k == 0:\n        data  = np.expand_dims(image,axis=0) ##注意如果k=0，即没有符合条件的数据将最后一个image赋给data，避免data为空\n    else:\n        data  = np.array(data,dtype=np.float32)\n\n    print(data.shape)\n    return data.astype(np.float32)\ndef data_crop2(imagearray,crop_sz,stride,random=None,n_patch=250):   \n    data = []\n    for i in range(imagearray.shape[0]):\n        if random:\n            image=data_crop_random2(imagearray[i,:,:,:],crop_sz,n_patch)\n        else:\n            image =sequential_crop(imagearray[i,:,:,:],crop_sz,stride)\n        if i == 0:\n            data  = image ##注意当i=0的时候需要将image赋给data，否则data依然是空，不可以进行concatnate\n        else:\n            data  = np.concatenate((data, image), axis = 0) \n        print(\"patch processing....:\"+str(i))\n    data=np.array(data,dtype=np.float32)\n    print(\"final processed:\"+str(i)+\"...No.:\"+str(data.shape[0]))    \n    return data\n\n\n## 同样使用前面14幅影像进行切片，增加不平衡样本数据的采集\n# data_all = np.concatenate((newimg, np.expand_dims(labels,axis=-1)), axis = -1)\n# stride=256\n# cropsize=256\nall_patches2=data_crop2(data_all,cropsize,stride,random=True)\nprint(data_all.shape,all_patches2.shape)\ni=0\n\n(1, 256, 256, 4)\npatch processing....:0\n(1, 256, 256, 4)\npatch processing....:1\n(207, 256, 256, 4)\npatch processing....:2\n(1, 256, 256, 4)\npatch processing....:3\n(250, 256, 256, 4)\npatch processing....:4\n(250, 256, 256, 4)\npatch processing....:5\n(63, 256, 256, 4)\npatch processing....:6\n(197, 256, 256, 4)\npatch processing....:7\n(151, 256, 256, 4)\npatch processing....:8\n(250, 256, 256, 4)\npatch processing....:9\n(68, 256, 256, 4)\npatch processing....:10\n(171, 256, 256, 4)\npatch processing....:11\n(24, 256, 256, 4)\npatch processing....:12\n(81, 256, 256, 4)\npatch processing....:13\nfinal processed:13...No.:1715\n(14, 3840, 3840, 4) (1715, 256, 256, 4)\n\n\n\nall_patches2=suffle_data(all_patches2)# 对新的数据集进行随机打乱\n\n(1715, 256, 256, 4)\n\n\n\n# plot_func(all_patches2[i:i+20,:,:,:3],all_patches2[i:i+20:,:,:,-1])\n# i+=500\n\n\n\n\n\n\n\n\n## 加载前面生成的切片数据\n# hdf5_path = \"./data/patches_rgb_4b_5c.hdf5\"\n# fd = h5py.File(hdf5_path, 'r')\n# fd.keys()\n# images2=np.array(fd['image'])\n# labels2=np.array(fd['label'])\n\n\n## 对两次切片数据进行合并，得到新的数据集\nnewimages=np.concatenate((images2,all_patches2[:,:,:,0:-1]),axis=0)\nnewlabels=np.concatenate((labels2,all_patches2[:,:,:,-1]),axis=0)\nprint(newimages.shape,newlabels.shape)\n\n(4459, 256, 256, 3) (4459, 256, 256)\n\n\n\n##可以选择将数据保存为h5文件，方便后续使用\nhdf5_path = './data/patches2_rgb_4b_5c.hdf5' \nf = h5py.File(hdf5_path, mode='w')\nf['image'] = newimages\nf['label'] = newlabels\nf.close()\n\n\n# hdf5_path = './data/patches2_rgb_4b_5c.hdf5' \n# fd = h5py.File(hdf5_path, 'r')\n# fd.keys()\n# newimages=np.array(fd['image'])\n# newlabels=np.array(fd['label'])#3:tree,4:road,5:crop\n\n\n## 对数据进行归一化处理，并将label转成one-hot标签形式\nn_label=5\ndef post_normalize_image(images,labels,n_label=n_label):\n    msk = label_hot(labels,n_label)\n    img = images\n    return img,msk\nimg,msk=post_normalize_image(newimages,newlabels,n_label)\n# 将数据集按照7:3进行划分\nxtrain,xtest,ytrain,ytest=train_test_split(img,msk,test_size=0.2,random_state=42)\ndel img,msk #如果数据较大可以在此删除降低内存\nprint(xtrain.shape,xtest.shape,ytrain.shape,ytest.shape)\ni=0\n\n(3567, 256, 256, 3) (892, 256, 256, 3) (3567, 256, 256, 5) (892, 256, 256, 5)\n\n\n\nplot_func(xtrain[i:i+20,:,:,:3],np.argmax(ytrain,axis=-1)[i:i+20:,:,:])\ni+=500\n\n\n\n\n\n\n\n\n\n4.2 模型优化\n\n## 计算真值标签中各个类别的占比，作为损失函数的权重，权重值越大模型识别错误代价越大一定程度缓解数据不平衡问题。\n# from sklearn.utils.class_weight import compute_class_weight\n# classes = np.unique(labels)  \n# class_weight = compute_class_weight(class_weight='balanced', classes=classes, y=labels.reshape(-1))\nclass_weight=np.array([0.35,4.48,2.07,0.68,28.55])\nprint(class_weight)\n\n[ 0.35  4.48  2.07  0.68 28.55]\n\n\n\n## 采用带有权重的交叉熵损失函数\nfrom keras import backend as K\nimport tensorflow as tf\ndef weighted_categorical_crossentropy(weights):\n    \"\"\"\n    Usage:\n        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n        loss = weighted_categorical_crossentropy(weights)\n    \"\"\"\n    weights = K.variable(weights)\n    def loss(y_true, y_pred):\n        # scale predictions so that the class probas of each sample sum to 1\n        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n        # clip to prevent NaN's and Inf's\n        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n        # calc\n        loss = y_true * K.log(y_pred) * weights\n        loss = -K.sum(loss, -1)\n        return loss\n    return loss\n\n\n## 设定必要的参数\nloss=weighted_categorical_crossentropy(class_weight)\noptimizer=Adam()\nloss_weights=''\nmetrics=['accuracy']\nbatch_size=10\nepoch= 50\ninput_height=xtrain.shape[1]\ninput_width=xtrain.shape[2]\nnchannel=xtrain.shape[-1]\nnum_train=xtrain.shape[0]\nnum_val=xtest.shape[0]\nATM=all_model(loss,loss_weights,optimizer,metrics,input_height,input_width,n_label,nchannel)\nmodelname='convattunet'\n\n\n## unet下采样操作较多导致细小线状地物信息丢失，新的网络减少下采样，且在decoder部分采用注意力机制提升浅层特征的权重\nmodel=ATM.convattunet()#deeplabv3p,UNET,dlinknet,convattunet\n# model.summary()\n\n384\n\n\n\n## 开始训练\nbegin_time = time.time()          \nout_dir = \"./checkpoint/\"+modelname+'/'\nif not os.path.exists(out_dir):\n    os.makedirs(out_dir) \nmodel_checkpoint = ModelCheckpoint(filepath=out_dir+\"-{epoch:02d}e-val_loss{val_loss:2f}.hdf5\",monitor=\"val_loss\",save_best_only=True,mode='auto')\ncsvlogger =CSVLogger(filename=out_dir+modelname+'-'+str(epoch)+'-log.csv', separator=',', append=False)\nresult=model.fit(xtrain, ytrain, batch_size=batch_size, epochs=epoch, verbose=1, shuffle=True,validation_data=(xtest, ytest),callbacks=[model_checkpoint,csvlogger])\nend_time = time.time()\nprint('time lapsing {0} s \\n'.format(end_time - begin_time))\n\nEpoch 1/50\n357/357 [==============================] - 132s 344ms/step - loss: 1.7302 - accuracy: 0.3324 - val_loss: 2.7731 - val_accuracy: 0.0355\nEpoch 2/50\n357/357 [==============================] - 117s 328ms/step - loss: 1.1811 - accuracy: 0.4554 - val_loss: 1.1779 - val_accuracy: 0.4066\nEpoch 3/50\n357/357 [==============================] - 117s 329ms/step - loss: 1.3168 - accuracy: 0.3951 - val_loss: 1.1310 - val_accuracy: 0.5230\nEpoch 4/50\n357/357 [==============================] - 115s 323ms/step - loss: 1.2823 - accuracy: 0.4182 - val_loss: 1.2304 - val_accuracy: 0.4714\nEpoch 5/50\n357/357 [==============================] - 115s 323ms/step - loss: 1.0932 - accuracy: 0.4740 - val_loss: 2.0796 - val_accuracy: 0.4236\nEpoch 6/50\n357/357 [==============================] - 116s 324ms/step - loss: 1.0692 - accuracy: 0.4806 - val_loss: 1.8193 - val_accuracy: 0.2795\nEpoch 7/50\n357/357 [==============================] - 117s 328ms/step - loss: 0.9390 - accuracy: 0.5248 - val_loss: 0.8420 - val_accuracy: 0.5531\nEpoch 8/50\n357/357 [==============================] - 116s 324ms/step - loss: 0.9168 - accuracy: 0.5349 - val_loss: 1.3639 - val_accuracy: 0.4214\nEpoch 9/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.7919 - accuracy: 0.5759 - val_loss: 0.8654 - val_accuracy: 0.5469\nEpoch 10/50\n357/357 [==============================] - 116s 324ms/step - loss: 0.7120 - accuracy: 0.6098 - val_loss: 0.9407 - val_accuracy: 0.6363\nEpoch 11/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.6665 - accuracy: 0.6290 - val_loss: 0.9928 - val_accuracy: 0.5825\nEpoch 12/50\n357/357 [==============================] - 117s 329ms/step - loss: 0.6473 - accuracy: 0.6359 - val_loss: 0.7060 - val_accuracy: 0.6126\nEpoch 13/50\n357/357 [==============================] - 116s 324ms/step - loss: 0.7309 - accuracy: 0.6215 - val_loss: 1.2073 - val_accuracy: 0.6001\nEpoch 14/50\n357/357 [==============================] - 115s 324ms/step - loss: 0.7831 - accuracy: 0.5896 - val_loss: 1.0000 - val_accuracy: 0.4280\nEpoch 15/50\n357/357 [==============================] - 115s 324ms/step - loss: 0.6402 - accuracy: 0.6407 - val_loss: 0.8063 - val_accuracy: 0.6192\nEpoch 16/50\n357/357 [==============================] - 117s 328ms/step - loss: 0.5884 - accuracy: 0.6662 - val_loss: 0.6009 - val_accuracy: 0.7153\nEpoch 17/50\n357/357 [==============================] - 116s 324ms/step - loss: 0.5722 - accuracy: 0.6790 - val_loss: 0.6031 - val_accuracy: 0.6628\nEpoch 18/50\n357/357 [==============================] - 117s 328ms/step - loss: 0.5473 - accuracy: 0.6883 - val_loss: 0.5610 - val_accuracy: 0.6622\nEpoch 19/50\n357/357 [==============================] - 115s 324ms/step - loss: 0.6260 - accuracy: 0.6581 - val_loss: 0.6122 - val_accuracy: 0.6706\nEpoch 20/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.5256 - accuracy: 0.6956 - val_loss: 0.5667 - val_accuracy: 0.6594\nEpoch 21/50\n357/357 [==============================] - 117s 327ms/step - loss: 0.4587 - accuracy: 0.7269 - val_loss: 0.4607 - val_accuracy: 0.6873\nEpoch 22/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.4570 - accuracy: 0.7349 - val_loss: 5.9171 - val_accuracy: 0.2067\nEpoch 23/50\n357/357 [==============================] - 116s 324ms/step - loss: 0.7492 - accuracy: 0.6173 - val_loss: 0.5541 - val_accuracy: 0.6501\nEpoch 24/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.5090 - accuracy: 0.7099 - val_loss: 0.4747 - val_accuracy: 0.6905\nEpoch 25/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.4161 - accuracy: 0.7504 - val_loss: 0.4940 - val_accuracy: 0.7504\nEpoch 26/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.4415 - accuracy: 0.7416 - val_loss: 0.6152 - val_accuracy: 0.7500\nEpoch 27/50\n357/357 [==============================] - 117s 328ms/step - loss: 0.4088 - accuracy: 0.7558 - val_loss: 0.4202 - val_accuracy: 0.7457\nEpoch 28/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.3941 - accuracy: 0.7668 - val_loss: 0.4369 - val_accuracy: 0.7617\nEpoch 29/50\n357/357 [==============================] - 117s 328ms/step - loss: 0.3693 - accuracy: 0.7788 - val_loss: 0.3911 - val_accuracy: 0.7993\nEpoch 30/50\n357/357 [==============================] - 116s 324ms/step - loss: 0.3296 - accuracy: 0.7972 - val_loss: 0.4089 - val_accuracy: 0.8031\nEpoch 31/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.6094 - accuracy: 0.6927 - val_loss: 0.5222 - val_accuracy: 0.7067\nEpoch 32/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.6922 - accuracy: 0.6500 - val_loss: 0.4923 - val_accuracy: 0.7336\nEpoch 33/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.3982 - accuracy: 0.7583 - val_loss: 0.4426 - val_accuracy: 0.7489\nEpoch 34/50\n357/357 [==============================] - 117s 328ms/step - loss: 0.3531 - accuracy: 0.7851 - val_loss: 0.3565 - val_accuracy: 0.7895\nEpoch 35/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.3184 - accuracy: 0.8056 - val_loss: 0.3931 - val_accuracy: 0.7762\nEpoch 36/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.3268 - accuracy: 0.8041 - val_loss: 0.7312 - val_accuracy: 0.7174\nEpoch 37/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.5086 - accuracy: 0.7256 - val_loss: 0.4377 - val_accuracy: 0.7344\nEpoch 38/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.4898 - accuracy: 0.7339 - val_loss: 0.4392 - val_accuracy: 0.7486\nEpoch 39/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.3323 - accuracy: 0.7972 - val_loss: 0.4307 - val_accuracy: 0.7965\nEpoch 40/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.3074 - accuracy: 0.8101 - val_loss: 0.3669 - val_accuracy: 0.7966\nEpoch 41/50\n357/357 [==============================] - 117s 327ms/step - loss: 0.2872 - accuracy: 0.8219 - val_loss: 0.3196 - val_accuracy: 0.8116\nEpoch 42/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.2687 - accuracy: 0.8335 - val_loss: 0.3486 - val_accuracy: 0.7991\nEpoch 43/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.2602 - accuracy: 0.8381 - val_loss: 0.3334 - val_accuracy: 0.8044\nEpoch 44/50\n357/357 [==============================] - 116s 324ms/step - loss: 0.2730 - accuracy: 0.8322 - val_loss: 0.3430 - val_accuracy: 0.8112\nEpoch 45/50\n357/357 [==============================] - 117s 327ms/step - loss: 0.2481 - accuracy: 0.8450 - val_loss: 0.3174 - val_accuracy: 0.8323\nEpoch 46/50\n357/357 [==============================] - 116s 324ms/step - loss: 0.2362 - accuracy: 0.8527 - val_loss: 0.3275 - val_accuracy: 0.8271\nEpoch 47/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.2201 - accuracy: 0.8618 - val_loss: 0.3433 - val_accuracy: 0.8147\nEpoch 48/50\n357/357 [==============================] - 115s 324ms/step - loss: 0.2571 - accuracy: 0.8444 - val_loss: 0.3561 - val_accuracy: 0.8300\nEpoch 49/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.2231 - accuracy: 0.8617 - val_loss: 0.4232 - val_accuracy: 0.8120\nEpoch 50/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.2010 - accuracy: 0.8733 - val_loss: 0.3685 - val_accuracy: 0.8746\ntime lapsing 5814.08594250679 s \n\n\n\n\n## 训练时长较短，模型为达到收敛因此最高精度不是很高，但训练曲线和验证曲线趋势十分吻合，且loss有明显的降低，表明模型性能有提升\nplot_fig(result,modelname+\"_Loss_Acc_epoch.png\")\n\n\n\n\n\nh5=glob.glob(\"./checkpoint/\"+modelname+\"/*.hdf5\")\nmodel_list=np.sort(h5)\n# model=load_model(model_list[-1])\nmodel=load_model(model_list[-1],custom_objects={'loss':weighted_categorical_crossentropy}) #loss作为自定义层需要指出\nprint(model_list[-1])\n\n./checkpoint/convattunet\\-45e-val_loss0.317361.hdf5\n\n\n\n## 首先对切片数据进行预测，查看效果\ni=100\npred=model.predict(xtest,batch_size=64)\npred=np.argmax(pred,axis=-1)\ngt=np.argmax(ytest,axis=-1)\n\n14/14 [==============================] - 15s 816ms/step\n\n\n\nval_plot_func(xtest[i:i+20],gt[i:i+20],pred[i:i+20])\ni+=20\n\n\n\n\n\n\n\n\n\n\n\n## 评估结果发现1类和4类地物较之前都有精度的提升，整体miou提升16%\ndf = call_matric(pred,gt, [0,1,2,3,4,'all'])\nprint(df)\n\n     precision    recall  F1-score       iou        oa      miou\n0     0.939834  0.777367  0.850915  0.740515  0.832341  0.679057\n1     0.544363  0.946242  0.691128  0.528033  0.832341  0.679057\n2     0.645177  0.880091  0.744544  0.593047  0.832341  0.679057\n3     0.923623  0.863848  0.892736  0.806254  0.832341  0.679057\n4     0.730599  0.994085  0.842215  0.727437  0.832341  0.679057\nall   0.756720  0.892327  0.804308  0.679057  0.832341  0.679057\n\n\n\n\n4.3 整景影像的预测\n\n## 加载整景的影像进行测试\ntest_data  = h5py.File('./data/kaggle_test1_3b_5c.hdf5', 'r')\ntestimg = np.array(test_data['image'])\ntestlabel=np.array(test_data['label'])\nprint(testimg.shape,testlabel.shape)\n\n(1, 3840, 3840, 3) (1, 3840, 3840)\n\n\n\n## 与训练数据采用相同的预处理方式\nimage=adjust_contrast(testimg)\nnp.max(image),np.max(testimg)\n\n(1.0, 9995)\n\n\n\n## 首先对影像做padding，保证其能够被crop_size整除，先沿着行列分别裁切样本，再统一进行预测，预测后数据按照原来的顺序再排列组合复原。需要注意的是这里采用的是膨胀预测的方法，喂给模型用来预测的切片大小是256，但放的时候只保留了中间的128×128，四周数据可靠度低，直接废弃\ndef center_predict(img,model,batch_size,n_label,strides=128,img_size=256):\n    corner_size=int(0.25*img_size)\n    h,w,c = img.shape\n    padding_h = (h//strides + 1) * strides+corner_size+corner_size\n    padding_w = (w//strides + 1) * strides+corner_size+corner_size\n    \n    padding_img = np.zeros((padding_h,padding_w,c),dtype=np.float16)\n    padding_img[corner_size:corner_size+h,corner_size:corner_size+w,:] = img[:,:,:]\n    mask_whole = np.zeros((padding_h,padding_w,n_label),dtype=np.float16)\n    crop_batch=[]\n    for i in range(h//strides+1):\n        for j in range(w//strides+1):\n            crop_img = padding_img[i*strides:i*strides+img_size,j*strides:j*strides+img_size,:]\n            ch,cw,c = crop_img.shape\n            \n            if ch != img_size or cw != img_size:\n                continue\n            crop_batch.append(crop_img)\n            \n    crop_batch=np.array(crop_batch)\n    start_time=time.time()\n    pred=model.predict(crop_batch,batch_size=batch_size)\n\n    for i in range(h//strides+1):\n        for j in range(w//strides+1):\n            mask_whole[i*strides+corner_size:i*strides+img_size-corner_size,j*strides+corner_size:j*strides+img_size-corner_size] = pred[(i+1-1)*(w//strides+1)+(j+1)-1,corner_size:img_size-corner_size,corner_size:img_size-corner_size]\n    score = mask_whole[corner_size:corner_size+h,corner_size:corner_size+w]\n    end_time=time.time()\n    print('pred_time:',end_time-start_time)\n    return score\n\n\nh_pred = center_predict(image[0],model,32,n_label)\nh_pred_mask=np.argmax(h_pred, axis = -1)\nprint(h_pred.shape,testlabel[0].shape)\n\n31/31 [==============================] - 11s 292ms/step\npred_time: 14.90808916091919\n(3840, 3840, 5) (3840, 3840)\n\n\n\nfig=plt.figure(figsize=(20,20)) \nplt.subplot(1,2,1)\nplt.imshow(testlabel[0,:,:])\nplt.subplot(1,2,2)\nplt.imshow(h_pred_mask)\nplt.show()"
  },
  {
    "objectID": "posts/2022-11-17wrsdp_1-3-1/index.html",
    "href": "posts/2022-11-17wrsdp_1-3-1/index.html",
    "title": "哨兵2号数据获取及处理",
    "section": "",
    "text": "练习1 哨兵-2号数据获取及处理\n\n1. 数据介绍\n哨兵-2号卫星携带一枚多光谱成像仪(MSI)，高度为786km，可覆盖13个光谱波段，幅宽达290千米。地面分辨率分别为10m、20m和60m、一颗卫星的重访周期为10天，两颗互补，重访周期为5天。从可见光和近红外到短波红外，具有不同的空间分辨率，在光学数据中，哨兵-2号数据是唯一一个在红边范围含有三个波段的数据，这对监测植被健康信息非常有效。\n13个波段中蓝色 (B2)、绿色 (B3)、红色 (B4) 和近红外 (B8) 波段具有 10 米的分辨率；红端（B5）、近红外 NIR（B6、B7 和 B8A）以及短波红外 SWIR（B11和B12）的地面采样距离为20米；沿海大气气溶胶 (B1) 和卷云波段 (B10) 的像素大小为 60 米；\n\n环境要求： * GDAL==2.4.1 * numpy==1.19.5 * scikit-image==0.17.2 * sentinelsat==1.1.1 * zipp==3.6.0\n\nfrom sentinelsat import SentinelAPI, read_geojson, geojson_to_wkt\nfrom datetime import date\nimport os,glob,zipfile,rasterio\nimport rasterio.plot\nimport numpy as np\nfrom osgeo import gdal, osr, ogr, gdalconst\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport geopandas as gpd\n\n\n\n2. 数据下载\n\nSentinelsat包提供搜索、下载和检索Sentinel数据的功能\n数据来源：哥白尼数据开放访问中心\n\n\n\n# 输入ESA用户名和密码\nuser_name = 'chidan'\npassword  = 'chidan123!'\n\n# 登录ESA API\napi = SentinelAPI(user_name, password, 'https://apihub.copernicus.eu/apihub')\n\n# 检索的地理范围坐标\nfootprint = \"POLYGON((115.5 40.2, 116.0 40.2, 116.0 40.5, 115.5 40.5, 115.5 40.2))\" #左上为起始点，顺时针旋转，回到起始点闭合\n\n# 使用API接口查询，更多接口信息查询https://www.strerr.com/geojson/geojson.html#map=5/31.386/115.329\nproducts = api.query(footprint,\n                     date = ('20220801', date(2022,9,1)),\n                     platformname = 'Sentinel-2',\n                     producttype  = \"S2MSI2A\", #S2MSI1C\",# S2MS2Ap\n                     cloudcoverpercentage = (0, 1))\n\nprint(f\"一共检索到{len(products)}景符合条件的数据\\n\")\nfor i, product in enumerate(products):\n    product_info = api.get_product_odata(product)\n    print(product_info['title'])\n\n一共检索到4景符合条件的数据\n\nS2B_MSIL2A_20220825T030519_N0400_R075_T50TLK_20220825T052806\nS2B_MSIL2A_20220825T030519_N0400_R075_T50TMK_20220825T052806\nS2A_MSIL2A_20220810T030531_N0400_R075_T50TMK_20220810T083201\nS2B_MSIL2A_20220805T030529_N0400_R075_T50TMK_20220805T053117\n\n\n下载数据并解压,只下载一景\n\n# 创建下载路径\nsave_path = './data/'\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\n\n# 下载检索到的最后一景数据，对于长期存档的数据（3-6个月以上），会出现offline情况，在下载的时候，需要先请求，后台将数据调档至在线，时间大概是半个小时，才能下载\nfor i, product in enumerate(products):\n    if i == 1:\n        break \n    product_info = api.get_product_odata(product)\n    # if product_info['title'] == \"S2B_MSIL2A_20220805T030529_N0400_R075_T50TMK_20220805T053117\":\n    if product_info['Online']:\n        print('数据在线，正在下载')\n        file_save_path = api.download(product_info['id'],save_path)\n        sentinel_data_path = file_save_path['path']\n    else:\n        print('数据未在线，请等待30分钟重试')\n\n数据在线，正在下载\n\n\n\n\n\n\n\n\n\n# 解压数据\nf = zipfile.ZipFile(sentinel_data_path,'r') \n\n# 解压到save_path\nfor file in f.namelist():\n    f.extract(file,save_path)               \nf.close()\n\nSentinel-2产品解压后为SAFE格式，SAFE文件包含以下几个内容： * 一个manifest.safe文件，其中包含 XML 格式的一般产品信息 * JPEG2000 格式的预览图像 * 测量（传感器扫描成像）数据集的子文件夹，包括 GML-JPEG2000 格式的图像数据（颗粒/瓦片） * 数据条级别信息的子文件夹 * 带有辅助数据的子文件夹（例如国际地球自转和参考系统 (IERS) 公告） * HTML 预览\n 获取数据的信息\n获取数据的信息\n\n\nxml_path = sentinel_data_path[:-4] + '.SAFE' + os.sep +'MTD_MSIL2A.xml'\nroot_ds = gdal.Open(xml_path)\nds_list = root_ds.GetSubDatasets()  # 获取子数据集。该数据以数据集形式存储且以子数据集形式组织\nfor i in range(len(ds_list)):\n    visual_ds = gdal.Open(ds_list[i][0])  # 打开第i个数据子集的路径。ds_list有4个子集，内部前段是路径，后段是数据信息\n    print(ds_list[i][0])\n    print(f'数据波段为：{ds_list[i][1]}')\n    print(f'仿射矩阵信息：{visual_ds.GetGeoTransform()}')\n    print(f'投影信息：{visual_ds.GetProjection()}')\n    print(f'栅格波段数：{visual_ds.RasterCount}')\n    print(f'栅格列数（宽度）：{visual_ds.RasterXSize} 栅格行数（高度）：{visual_ds.RasterYSize}')\n    print(\"\\n\")\n\nSENTINEL2_L2A:data\\S2B_MSIL2A_20220825T030519_N0400_R075_T50TLK_20220825T052806.SAFE\\MTD_MSIL2A.xml:10m:EPSG_32650\n数据波段为：Bands B2, B3, B4, B8 with 10m resolution, UTM 50N\n仿射矩阵信息：(300000.0, 10.0, 0.0, 4500000.0, 0.0, -10.0)\n投影信息：PROJCS[\"WGS 84 / UTM zone 50N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32650\"]]\n栅格波段数：4\n栅格列数（宽度）：10980 栅格行数（高度）：10980\n\n\nSENTINEL2_L2A:data\\S2B_MSIL2A_20220825T030519_N0400_R075_T50TLK_20220825T052806.SAFE\\MTD_MSIL2A.xml:20m:EPSG_32650\n数据波段为：Bands B5, B6, B7, B8A, B11, B12, AOT, CLD, SCL, SNW, WVP with 20m resolution, UTM 50N\n仿射矩阵信息：(300000.0, 20.0, 0.0, 4500000.0, 0.0, -20.0)\n投影信息：PROJCS[\"WGS 84 / UTM zone 50N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32650\"]]\n栅格波段数：11\n栅格列数（宽度）：5490 栅格行数（高度）：5490\n\n\nSENTINEL2_L2A:data\\S2B_MSIL2A_20220825T030519_N0400_R075_T50TLK_20220825T052806.SAFE\\MTD_MSIL2A.xml:60m:EPSG_32650\n数据波段为：Bands B1, B9, AOT, CLD, SCL, SNW, WVP with 60m resolution, UTM 50N\n仿射矩阵信息：(300000.0, 60.0, 0.0, 4500000.0, 0.0, -60.0)\n投影信息：PROJCS[\"WGS 84 / UTM zone 50N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32650\"]]\n栅格波段数：7\n栅格列数（宽度）：1830 栅格行数（高度）：1830\n\n\nSENTINEL2_L2A:data\\S2B_MSIL2A_20220825T030519_N0400_R075_T50TLK_20220825T052806.SAFE\\MTD_MSIL2A.xml:TCI:EPSG_32650\n数据波段为：True color image, UTM 50N\n仿射矩阵信息：(300000.0, 10.0, 0.0, 4500000.0, 0.0, -10.0)\n投影信息：PROJCS[\"WGS 84 / UTM zone 50N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32650\"]]\n栅格波段数：3\n栅格列数（宽度）：10980 栅格行数（高度）：10980\n\n\n\n\n\n\n3. 数据格式转换\n我们所需要的图像数据储存在IMG_DATA文件夹里，该文件夹内有三个子文件夹，分别存有三种不同分辨率的数据，数据格式为JPEG2000。接下来的任务为：\n\n在解压缩后的文件夹查找到jp2数据\n对各个波段的jp2数据进行解压缩，保存为GeoTiff格式数据\n\n\n# 检索解压目录下的所有子文件夹，找到IMG_DATA文件夹\nfor root, _, _ in os.walk(sentinel_data_path[:-4] + '.SAFE'):\n    if root.endswith(\"IMG_DATA\"):\n        IMG_DATA_path = root\n\n\ndef get_image_name(image_directory):\n    file = image_directory + '\\R10m'              \n    names = os.listdir(file)[0]\n    img_name = names[:22]\n    return img_name\n\n\n# 获得文件名\nimg_identifier = get_image_name(IMG_DATA_path)\n\n# 拼接成各个波段文件绝对路径\n\n# 20m分辨率数据\njp2_20m_list = [IMG_DATA_path + os.sep +'R20m\\%s_B8A_20m.jp2' %img_identifier ,\n                IMG_DATA_path + os.sep +'R20m\\%s_B11_20m.jp2' %img_identifier ,\n                IMG_DATA_path + os.sep +'R20m\\%s_B12_20m.jp2' %img_identifier,\n                IMG_DATA_path + os.sep +'R20m\\%s_B05_20m.jp2' %img_identifier,\n                IMG_DATA_path + os.sep +'R20m\\%s_B06_20m.jp2' %img_identifier,\n                IMG_DATA_path + os.sep +'R20m\\%s_B07_20m.jp2' %img_identifier]\n\n# 10m分辨率数据\njp2_10m_list = [IMG_DATA_path + os.sep +'R10m\\%s_B02_10m.jp2' %img_identifier,\n                IMG_DATA_path + os.sep +'R10m\\%s_B03_10m.jp2' %img_identifier,\n                IMG_DATA_path + os.sep +'R10m\\%s_B04_10m.jp2' %img_identifier,\n                IMG_DATA_path + os.sep +'R10m\\%s_B08_10m.jp2' %img_identifier]\n\ndef jp2Totif(jp2_path,save_path):\n    \"\"\"\n    格式转换：jp2 转 tif\n    :param jp2_path: jp2数据地址\n    :param save_path: 存储路径\n    :return: 存储地址\n    \"\"\"\n    file_name = os.path.basename(jp2_path)[:-4]\n    save_file = os.path.join(save_path,file_name)\n    save_file = save_file+'.tif'\n    dataset = gdal.Open(jp2_path)\n    rows = dataset.RasterYSize\n    cols = dataset.RasterXSize\n    projection = dataset.GetProjection()\n    trans = dataset.GetGeoTransform()\n    data = dataset.ReadAsArray()\n    if data.dtype == 'uint16':\n        driver = gdal.GetDriverByName('GTiff')\n        out_dataset = driver.Create(save_file, cols, rows, 1, gdal.GDT_UInt16)\n        out_dataset.SetProjection(projection)\n        out_dataset.SetGeoTransform(trans)\n        out_dataset.GetRasterBand(1).WriteArray(data)\n        out_dataset.GetRasterBand(1).SetNoDataValue(0)\n        out_dataset.FlushCache()\n        del dataset, out_dataset\n    elif data.dtype == 'uint8':\n        driver = gdal.GetDriverByName('GTiff')\n        out_dataset = driver.Create(save_file, cols, rows, 1, gdal.GDT_Byte)\n        out_dataset.SetProjection(projection)\n        out_dataset.SetGeoTransform(trans)\n        out_dataset.GetRasterBand(1).WriteArray(data)\n        out_dataset.GetRasterBand(1).SetNoDataValue(0)\n        out_dataset.FlushCache()\n        del dataset, out_dataset\n    return save_file\n\n仅堆叠可见波段NIR、RE和SWIR1和SWIR2（波段2、3、4、8、8A、11、12）。将20m波段（8A、11和12）重新采样至10m。\n\n# 创建20m分辨率数据存储路径\ntif_20m_save_path = \".\\\\data\"+ os.sep + img_identifier+ os.sep+ \"20m\"\nif not os.path.exists(tif_20m_save_path):\n    os.makedirs(tif_20m_save_path)\n\n# 将20m分辨率数据由jp2格式转换为tif格式，并记录地址到tif_20m_list\ntif_20m_list = []\nfor jp2_path in jp2_20m_list:\n    tif_path = jp2Totif(jp2_path, tif_20m_save_path)\n    tif_20m_list.append(tif_path)\n    \n# 创建10m分辨率数据存储路径\ntif_10m_save_path = \".\\\\data\"+ os.sep + img_identifier+ os.sep+\"10m\"\nif not os.path.exists(tif_10m_save_path):\n    os.makedirs(tif_10m_save_path)\n\n# 将10m分辨率数据由jp2格式转换为tif格式，并记录地址到tif_10m_list\ntif_10m_list = []\nfor jp2_path in jp2_10m_list:\n    tif_path = jp2Totif(jp2_path, tif_10m_save_path)\n    tif_10m_list.append(tif_path) \n\n\n\n4. 数据重采样\n将20m分辨率数据重采样至10m分辨率\n\ndef ReprojectImages(inputfilePath,outputfilePath,referencefilefilePath):\n    # 获取输出影像信息\n    inputrasfile = gdal.Open(inputfilePath, gdal.GA_ReadOnly)\n    inputProj = inputrasfile.GetProjection()\n    # 获取参考影像信息\n    referencefile = gdal.Open(referencefilefilePath, gdal.GA_ReadOnly)\n    referencefileProj = referencefile.GetProjection()\n    referencefileTrans = referencefile.GetGeoTransform()\n    bandreferencefile = referencefile.GetRasterBand(1)\n    Width= referencefile.RasterXSize\n    Height = referencefile.RasterYSize\n    nbands = referencefile.RasterCount\n    # 创建重采样输出文件（设置投影及六参数）\n    driver = gdal.GetDriverByName('GTiff')\n    output = driver.Create(outputfilePath, Width,Height, nbands, bandreferencefile.DataType)\n    output.SetGeoTransform(referencefileTrans)\n    output.SetProjection(referencefileProj)\n    # 参数说明 输入数据集、输出文件、输入投影、参考投影、重采样方法(最邻近内插\\双线性内插\\三次卷积等)、回调函数\n    gdal.ReprojectImage(inputrasfile, output, inputProj, referencefileProj, gdalconst.GRA_Bilinear,0.0,0.0,)\n\n\n# 获取一景10m分辨率影像作为参考影像\nreference_tif = tif_10m_list[0] \n\n# 将20m分辨率数据重采样至10m,并保存至 tif_10m_save_path 路径下\nfor tif_path in tif_20m_list:\n    file_name = os.path.basename(tif_path)[:-7]\n    save_tif_path = tif_10m_save_path + os.sep + file_name +'10m.tif'\n    ReprojectImages(tif_path, save_tif_path, reference_tif)\n\n\n\n5. 波段叠合\n\n将多个不同波段的的TIF文件合为一个多波段TIF文件\n叠合波段为可见波段NIR、RE和SWIR1和SWIR2（波段2、3、4、8、8A、11、12）\n\n\n# 读图像文件\ndef read_img(filename):\n\n    dataset = gdal.Open(filename)  # 打开文件\n\n    im_width = dataset.RasterXSize  # 栅格矩阵的列数\n    im_height = dataset.RasterYSize  # 栅格矩阵的行数\n    # im_bands = dataset.RasterCount  # 波段数\n    im_geotrans = dataset.GetGeoTransform()  # 仿射矩阵，左上角像素的大地坐标和像素分辨率\n    im_proj = dataset.GetProjection()  # 地图投影信息，字符串表示\n    im_data = dataset.ReadAsArray(0, 0, im_width, im_height)\n\n    del dataset   #关闭对象dataset，释放内存\n    # return im_width, im_height, im_proj, im_geotrans, im_data,im_bands\n    return  im_proj, im_geotrans, im_data, im_width,im_height\n \n# 遥感影像的存储\n# 写GeoTiff文件\ndef Write_Tiff(img_arr, geomatrix, projection,path):\n#     img_bands, img_height, img_width = img_arr.shape\n    if 'int8' in img_arr.dtype.name:\n        datatype = gdal.GDT_Byte\n    elif 'int16' in img_arr.dtype.name:\n        datatype = gdal.GDT_UInt16\n    else:\n        datatype = gdal.GDT_Float32\n\n    if len(img_arr.shape) == 3:\n        img_bands, img_height, img_width = img_arr.shape\n        driver = gdal.GetDriverByName(\"GTiff\")\n        dataset = driver.Create(path, int(img_width), int(img_height), int(img_bands), datatype)\n    #     print(path, int(img_width), int(img_height), int(img_bands), datatype)\n        if(dataset!= None) and (geomatrix !='') and (projection!=''):\n            dataset.SetGeoTransform(geomatrix) #写入仿射变换参数\n            dataset.SetProjection(projection) #写入投影\n        for i in range(img_bands):\n            dataset.GetRasterBand(i+1).WriteArray(img_arr[i])\n        del dataset\n\n    elif len(img_arr.shape) == 2:\n        # img_arr = np.array([img_arr])\n        img_height, img_width = img_arr.shape\n        img_bands=1\n        #创建文件\n        driver = gdal.GetDriverByName(\"GTiff\")\n        dataset = driver.Create(path, int(img_width), int(img_height), int(img_bands), datatype)\n    #     print(path, int(img_width), int(img_height), int(img_bands), datatype)\n        if(dataset!= None):\n            dataset.SetGeoTransform(geomatrix) #写入仿射变换参数\n            dataset.SetProjection(projection) #写入投影\n        dataset.GetRasterBand(1).WriteArray(img_arr)\n        del dataset\n \ndef merge_tif(tif_path_list, output_tif):\n    arr_list = []\n    for tif_path in tif_path_list:\n        proj, geotrans, data, row, column  = read_img(tif_path)\n        arr_list.append(data)\n    all_arr = np.array(arr_list)\n    Write_Tiff(all_arr,geotrans,proj,output_tif)\n    \n\n筛选数据\n\ntif_list = os.listdir(tif_10m_save_path)   # 获取所有10m分辨率tif数据的名称\ntif_path_list = []                        # 以列表的形式存储各个波段的路径\n\nband_names = [\"B02\",\"B03\",\"B04\",\"B08\",\"B8A\",\"B11\",\"B12\"]  # 需要堆叠的波段名\n\nfor band_name in band_names:\n    for tif_name in tif_list:\n        if band_name == tif_name[-11:-8]:\n            tif_path_list.append(tif_10m_save_path + os.sep + tif_name)\n\nprint(tif_path_list)\n\n['.\\\\data\\\\T50TLK_20220825T030519\\\\10m\\\\T50TLK_20220825T030519_B02_10m.tif', '.\\\\data\\\\T50TLK_20220825T030519\\\\10m\\\\T50TLK_20220825T030519_B03_10m.tif', '.\\\\data\\\\T50TLK_20220825T030519\\\\10m\\\\T50TLK_20220825T030519_B04_10m.tif', '.\\\\data\\\\T50TLK_20220825T030519\\\\10m\\\\T50TLK_20220825T030519_B08_10m.tif', '.\\\\data\\\\T50TLK_20220825T030519\\\\10m\\\\T50TLK_20220825T030519_B8A_10m.tif', '.\\\\data\\\\T50TLK_20220825T030519\\\\10m\\\\T50TLK_20220825T030519_B11_10m.tif', '.\\\\data\\\\T50TLK_20220825T030519\\\\10m\\\\T50TLK_20220825T030519_B12_10m.tif']\n\n\n\n# 执行叠加函数\nmerge_out =save_path+img_identifier+os.sep+img_identifier+\"_merge.tif\"\nmerge_tif(tif_path_list, merge_out)\n\n\n\n6. 真彩色影像可视化\n\n对图像进行拉伸显示\n转换成0-255的快视图并保存\n\n\n# 显示叠加结果数据信息\n\n#   归一化函数\ndef stretch(band, lower_percent=2, higher_percent=98): #2和95表示分位数\n    band=np.array(band,dtype=np.float32)\n    c = np.percentile(band, lower_percent)*1.0\n    d = np.percentile(band, higher_percent)*1.0       \n    band[band<c] = c\n    band[band>d] = d\n    out =  (band - c)  / (d - c)  \n    return out.astype(np.float32)\ndef stretch_n(data, n_band=3):  #该操作讲改变原始数据，因此data用.copy，不对原始数据进行更改\n    data=np.array(data,dtype=np.float32)\n    for k in range(n_band):\n            data[:,:,k] = stretch(data[:,:,k])\n    return data\ndef rgb(img_data,iftran=True):\n    img_data_3b = img_data[:3,:,:]                  # 取前三个波段 B02,B03,B04\n    if iftran:\n        img_data_3b = img_data_3b[::-1,:,:]             # 将B02,B03,B04转成B04,B03,B02 (BGR转RGB)\n    img_data    = img_data_3b.transpose(1,2,0)     # C,H,W -> H,W,C\n    return img_data \n\nproj, geotrans, img_data, row, column  = read_img(merge_out)\nimg_data_r=rgb(img_data) #提取3波段改变rgb顺序和数据维度\nimg_data_rgb_s = np.uint8(stretch_n(img_data_r.copy())*255) # 数据值域缩放至（0~255）\n\nplt.figure(figsize=(8,8))\nplt.imshow(Image.fromarray(img_data_rgb_s))\nplt.show()\n\n仿射矩阵信息：(300000.0, 10.0, 0.0, 4500000.0, 0.0, -10.0)\n投影信息：PROJCS[\"WGS 84 / UTM zone 50N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32650\"]]\n栅格波段数：7\n栅格列数（宽度）：10980 栅格行数（高度）：10980\n\n\n\n\n\n\n\n\nquickimg =save_path+img_identifier+os.sep+img_identifier+\"_quickimg.tif\"\nWrite_Tiff(img_data_rgb_s.transpose(2,0,1),ds.GetGeoTransform(),ds.GetProjection(),quickimg)\n\n\n\n7. 重投影\n如果需要对数据进行投影操作，可以利用gdal.warp实现\n\nds = gdal.Open(quickimg)       # 打开文件\nrojectedtmp =save_path+img_identifier+os.sep+img_identifier+\"_projected.tif\"\nds = gdal.Warp(rojectedtmp, ds, dstSRS='EPSG:4326')    # 有投影的需求可以使用warp命令，epsg可以通过https://epsg.io/查询，这里给出的是wgs84\n\n\nproj, geotrans, img_data, row, column  = read_img(rojectedtmp)\n\n# 显示重投影结果信息\nprint(f'仿射矩阵信息：{geotrans}',f'投影信息：{proj}')\nprint(\"\\n\")\n\nimg_data_ = img_data.transpose(1,2,0)          # C,H,W -> H,W,C\nplt.imshow(img_data_)\n\n仿射矩阵信息：(114.63531921298156, 0.00010513256977891853, 0.0, 40.64592849014489, 0.0, -0.00010513256977891853) 投影信息：GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]\n\n\n\n\n<matplotlib.image.AxesImage at 0x16971629430>\n\n\n\n\n\n\n\n8. 数据裁剪\n\n利用rasterio库更容易将shp与影像进行叠加显示\n利用gdal.warp实现数据裁剪\n\n\nclip_shp = './data/clippolygon.shp'\nvector = gpd.read_file(clip_shp)\nimg=rasterio.open(quickimg)\n\nfig, ax = plt.subplots(figsize=(5,5))\np1 =rasterio.plot.show(img, ax=ax,title='Sentinel 2 image of Guanting Reservoir')\nvector.plot(ax=ax,edgecolor='red', linewidth=1,facecolor = \"none\") #如果shp与raster坐标投影不一致无法同时显示\nplt.show()\n\n\n\n\n按矢量轮廓裁剪\n\n# 执行裁剪\nclip_output =save_path+img_identifier+os.sep+img_identifier+\"_clip.tif\"\n# 按矢量轮廓裁剪\ngdal.Warp(clip_output, merge_out, cutlineDSName = clip_shp, format=\"GTiff\", cropToCutline = True)\n\n<osgeo.gdal.Dataset; proxy of <Swig Object of type 'GDALDatasetShadow *' at 0x00000169B2690810> >\n\n\n\nproj, geotrans, img_data, row, column  = read_img(clip_output)\nimg_data_=rgb(img_data)\nplt.imshow(stretch_n(img_data_.copy()))\n\n<matplotlib.image.AxesImage at 0x16971673eb0>\n\n\n\n\n\n\n想了解更多请关注[45度科研人]公众号，欢迎给我留言！"
  },
  {
    "objectID": "posts/2022-12-10wrsdp_1-3-2/index.html",
    "href": "posts/2022-12-10wrsdp_1-3-2/index.html",
    "title": "基于遥感指数的水体提取",
    "section": "",
    "text": "练习2 基于遥感指数的水体提取\n\n1. 方法介绍\n阈值法是比较简单但却较为有效的水体提取方法，优点是物理含义明确，计算简单，效率高，缺点一是在一些复杂场景中，受建筑阴影等影响容易出现误识别，二是基于像元计算存在的“椒盐现象”，三是最大的确定是阈值需要人为设定。虽然这是一个非常原始的方法，但即使在今天依然有很大应用价值，比如作为一种特征与其他类型数据做融合处理，结合机器学习优化阈值设定的问题，还可以在一些广域识别应用中起到高置信样本筛选的作用等等。 NDWI（归一化差异水体指数）:\n\\[    NDWI = (GREEN-NIR)/(GREEN+NIR) \\]\n式中GREEN表示绿光波段的反射率，NIR表示近红外波段的反射率。该指数的构建是利用了水体光谱在近红外区间反射率几乎为零的特性，更适合要素均匀分区且类型相对单一的自然场景中。 MNDWI（改进的归一化差异水体指数）： \\[  MNDWI=(GREEN-SWIR)/(GREEN+SWIR)\\]\n式中GREEN表示绿光波段的反射率，SWIR表示中红外波段的反射率。该方法是NDWI的变种，将NDWI中的近红外替换成短波红外波段，提高了水体与建筑物等地物的可区分度，该指数较为适合于城镇水体信息的提取，然而大部分高分卫星数据由于缺少短波红外波段而无法使用该指数。\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nfrom osgeo import gdal, osr, ogr, gdalconst\nimport os,glob,time,rasterio\nimport shutil\nimport rasterio.plot\nimport geopandas as gpd\n\n\n\n2. 数据读取\n\n## 读图像文件\ndef read_img(filename):\n\n    dataset = gdal.Open(filename)  # 打开文件\n\n    im_width = dataset.RasterXSize  # 栅格矩阵的列数\n    im_height = dataset.RasterYSize  # 栅格矩阵的行数\n    # im_bands = dataset.RasterCount  # 波段数\n    im_geotrans = dataset.GetGeoTransform()  # 仿射矩阵，左上角像素的大地坐标和像素分辨率\n    im_proj = dataset.GetProjection()  # 地图投影信息，字符串表示\n    im_data = dataset.ReadAsArray(0, 0, im_width, im_height)\n\n    del dataset   #关闭对象dataset，释放内存\n    # return im_width, im_height, im_proj, im_geotrans, im_data,im_bands\n    return  im_proj, im_geotrans, im_data, im_width,im_height\n \n## 将numpy形式的遥感影像写出为GeoTiff文件\ndef Write_Tiff(img_arr, geomatrix, projection,path):\n#     img_bands, img_height, img_width = img_arr.shape\n    if 'int8' in img_arr.dtype.name:\n        datatype = gdal.GDT_Byte\n    elif 'int16' in img_arr.dtype.name:\n        datatype = gdal.GDT_UInt16\n    else:\n        datatype = gdal.GDT_Float32\n\n    if len(img_arr.shape) == 3:\n        img_bands, img_height, img_width = img_arr.shape\n        driver = gdal.GetDriverByName(\"GTiff\")\n        dataset = driver.Create(path, int(img_width), int(img_height), int(img_bands), datatype)\n    #     print(path, int(img_width), int(img_height), int(img_bands), datatype)\n        if(dataset!= None) and (geomatrix !='') and (projection!=''):\n            dataset.SetGeoTransform(geomatrix) #写入仿射变换参数\n            dataset.SetProjection(projection) #写入投影\n        for i in range(img_bands):\n            dataset.GetRasterBand(i+1).WriteArray(img_arr[i])\n        del dataset\n\n    elif len(img_arr.shape) == 2:\n        # img_arr = np.array([img_arr])\n        img_height, img_width = img_arr.shape\n        img_bands=1\n        #创建文件\n        driver = gdal.GetDriverByName(\"GTiff\")\n        dataset = driver.Create(path, int(img_width), int(img_height), int(img_bands), datatype)\n    #     print(path, int(img_width), int(img_height), int(img_bands), datatype)\n        if(dataset!= None):\n            dataset.SetGeoTransform(geomatrix) #写入仿射变换参数\n            dataset.SetProjection(projection) #写入投影\n        dataset.GetRasterBand(1).WriteArray(img_arr)\n        del dataset\n        \n## 计算数据头尾分位数的方式进行归一化，剔除异常值\ndef stretch(band, lower_percent=2, higher_percent=98): #2和98表示分位数\n    band=np.array(band,dtype=np.float32)\n    c = np.percentile(band, lower_percent)*1.0\n    d = np.percentile(band, higher_percent)*1.0       \n    band[band<c] = c\n    band[band>d] = d\n    out =  (band - c)  / (d - c)  \n    return out.astype(np.float32)\ndef stretch_n(data, n_band=3):  #该操作讲改变原始数据，因此data用.copy，不对原始数据进行更改\n    data=np.array(data,dtype=np.float32)\n    for k in range(n_band):\n            data[:,:,k] = stretch(data[:,:,k])\n    return data\n\ndef rgb(img_data,iftran=True):\n    img_data_3b = img_data[:3,:,:]                  # 取前三个波段 B02,B03,B04\n    if iftran:\n        img_data_3b = img_data_3b[::-1,:,:]             # 将B02,B03,B04转成B04,B03,B02 (BGR转RGB)\n    img_data    = img_data_3b.transpose(1,2,0)     # C,H,W -> H,W,C\n    return img_data \n\n读取多波段影像，剔除异常值，并提取rgb信息进行可视化\n\nnew_stack = \"./data/T50TLK_20220825T030519/T50TLK_20220825T030519_clip.tif\"\nout_path='./data/T50TLK_20220825T030519/'\nproj, geotrans, img_data, row, column  = read_img(new_stack)\n# 显示重投影结果信息\nprint(f'仿射矩阵信息：{geotrans}',f'投影信息：{proj}',f'图像大小：{img_data.shape}')\nimg_data_=rgb(img_data)\nplt.imshow(stretch_n(img_data_.copy()))\n\n仿射矩阵信息：(371350.0, 10.0, 0.0, 4480810.0, 0.0, -10.0) 投影信息：PROJCS[\"WGS 84 / UTM zone 50N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32650\"]] 图像大小：(7, 2874, 3789)\n\n\n<matplotlib.image.AxesImage at 0x246c2062c40>\n\n\n\n\n\n\n\n3. 水体指数计算\n\nNDWI（归一化差异水体指数）：NDWI = (GREEN-NIR)/(GREEN+NIR)\nMNDWI（改进的归一化差异水体指数）：MNDWI=(GREEN-SWIR)/(GREEN+SWIR)\n通过直方图观察数值分布情况，初步定位阈值区间\n\n\nGreen_arr = img_data[1,:,:]\nNIR_arr   = img_data[3,:,:]\n\ndenominator = np.array(Green_arr + NIR_arr, dtype=np.float32)\nnumerator = np.array(Green_arr - NIR_arr, dtype=np.float32)\nnodata = np.full((Green_arr.shape[0], Green_arr.shape[1]), -2, dtype=np.float32)\nndwi = np.true_divide(numerator, denominator, out=nodata, where=denominator != 0.0)\nprint(np.min(ndwi),np.max(ndwi))\nndwi[ndwi == -2.0]=None\n\nndwipath=out_path+'ndwi.tif'\nWrite_Tiff(np.uint8(ndwi), geotrans,proj, ndwipath)\n\nfig, axes = plt.subplots(1,3,figsize=(20,5))\nplt.subplot(1,3,1),plt.title('RGB'), plt.axis('off')\nplt.imshow(stretch_n(img_data_.copy()))\nplt.subplot(1,3,2),plt.title('NDWI'), plt.axis('off')\nplt.imshow(ndwi)\n# plt.style.use(\"ggplot\")\nplt.subplot(1,3,3),plt.title('Histogram'), plt.axis('off')\nplt.hist(ndwi.ravel(), bins=100, density=None, facecolor='green', alpha=0.75)\nplt.show()\n\n0.0 30.031637\n\n\n\n\n\n通过可视化查看不同阈值条件下水体提取效果\n\nfig = plt.figure(figsize=(20,6))\nfor i in range(5):\n    ndwi_ = ndwi.copy()\n    threshold = (i+1) *0.05\n    plt.subplot(1,5,i+1),plt.title('threshold=  %.3f' % threshold), plt.axis('off')\n    ndwi_ [ndwi_ >threshold] = 255\n    ndwi_ [ndwi_ <=threshold] = 0\n    plt.imshow(Image.fromarray(np.uint8(ndwi_)))\nplt.show()\nfig = plt.figure(figsize=(20,6))\nfor i in range(5):\n    ndwi_ = ndwi.copy()\n    threshold = (i+1) *2.5\n    plt.subplot(1,5,i+1),plt.title('threshold=  %.3f' % threshold), plt.axis('off')\n    ndwi_ [ndwi_ >threshold] = 255\n    ndwi_ [ndwi_ <=threshold] = 0\n    plt.imshow(Image.fromarray(np.uint8(ndwi_)))\nplt.show()\n\n\n\n\n\n\n\n对比NDWI和mNDWI的效果\n\nGreen_arr = img_data[1,:,:]\nSWIR_arr   = img_data[5,:,:]\n\ndenominator = np.array(Green_arr + SWIR_arr, dtype=np.float32)\nnumerator = np.array(Green_arr - SWIR_arr, dtype=np.float32)\nnodata = np.full((Green_arr.shape[0], Green_arr.shape[1]), -2, dtype=np.float32)\nmndwi = np.true_divide(numerator, denominator, out=nodata, where=denominator != 0.0)\nmndwi[mndwi == -2.0]=None\nfig, axes = plt.subplots(1,3,figsize=(20,5))\nplt.subplot(1,3,1),plt.title('RGB'), plt.axis('off')\nplt.imshow(stretch_n(img_data_.copy()))\nplt.subplot(1,3,2),plt.title('NDWI'), plt.axis('off')\nplt.imshow(ndwi)\nplt.subplot(1,3,3),plt.title('mNDWI'), plt.axis('off')\nplt.imshow(Image.fromarray(np.uint8(mndwi)))\nplt.show()\n\n\n\n\n\nfig = plt.figure(figsize=(20,6))\nfor i in range(5):\n    ndwi_ = mndwi.copy()\n    threshold = (i+1) *0.05\n    plt.subplot(1,5,i+1),plt.title('threshold=  %.3f' % threshold), plt.axis('off')\n    ndwi_ [ndwi_ >threshold] = 255\n    ndwi_ [ndwi_ <=threshold] = 0\n    plt.imshow(Image.fromarray(np.uint8(ndwi_)))\nplt.show()\nfig = plt.figure(figsize=(20,6))\nfor i in range(5):\n    ndwi_ = mndwi.copy()\n    threshold = (i+1) *2.5\n    plt.subplot(1,5,i+1),plt.title('threshold=  %.3f' % threshold), plt.axis('off')\n    ndwi_ [ndwi_ >threshold] = 255\n    ndwi_ [ndwi_ <=threshold] = 0\n    plt.imshow(Image.fromarray(np.uint8(ndwi_)))\nplt.show()\n\n\n\n\n\n\n\n阈值分割进行二值化处理得到水体掩膜\n\nndwi_ = ndwi.copy()\nthreshold = 2.5\nndwi_ [ndwi_ >threshold] = 255\nndwi_ [ndwi_ <=threshold] = 0\nplt.imshow(Image.fromarray(np.uint8(ndwi_)),cmap='gray')\nout_ndwi=out_path+'out_ndwi.tif'\nWrite_Tiff(np.uint8(ndwi_), geotrans,proj, out_ndwi)\n\n\n\n\n\n\n4. 去除小图斑\n\n使用阈值分割法会使图像上布满不规则小图斑，影响分割精度\n利用滤波等候处理方法降低结果噪声\n\n\n \ndef Speckle_removal(tif_path, save_path,  remove_pixels =100, neighbours = 8 ):\n\n    filename = os.path.basename(tif_path)\n    output_path = os.path.join(save_path, filename[:-4] + '_sr.tif' )\n\n    if not os.path.exists(save_path):\n        os.makedirs(save_path)\n    shutil.copy(tif_path, output_path)\n \n    # remove_pixels =100    #碎斑像素\n    # neighbours = 8    #连通域， 4或8\n    Image = gdal.Open(output_path, 1)  # open image in read-write mode\n    Band = Image.GetRasterBand(1)\n    gdal.SieveFilter(srcBand=Band, maskBand=None, dstBand=Band,\n                    threshold= remove_pixels, \n                    connectedness= neighbours,\n                    callback=gdal.TermProgress_nocb)\n \n    del Image, Band  # close the datasets.\n    return output_path\n\n\nsr_out = Speckle_removal(out_ndwi, out_path,  remove_pixels =1000, neighbours = 8 )\nproj, geotrans, sr_arr, row, column  = read_img(sr_out)\nsr_arr.shape\n\n(2874, 3789)\n\n\n对比后处理前后变化\n\nproj, geotrans, sr_arr, row, column  = read_img(sr_out)\nfig, axes = plt.subplots(1,2,figsize=(20,5))\nplt.subplot(1,3,1),plt.title('NDWI'), plt.axis('off')\nplt.imshow(Image.fromarray(np.uint8(ndwi_)))\nplt.subplot(1,3,2),plt.title('NDWI_sr'), plt.axis('off')\nplt.imshow(sr_arr)\nplt.show()\n\nC:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_28856\\1761671902.py:3: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n  plt.subplot(1,3,1),plt.title('NDWI'), plt.axis('off')\nC:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_28856\\1761671902.py:5: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n  plt.subplot(1,3,2),plt.title('NDWI_sr'), plt.axis('off')\n\n\n\n\n\n二值化栅格转矢量输出\n\ndef tif_to_shp(tif_path, shp_save_path):\n    inraster = gdal.Open(tif_path)  # 读取路径中的栅格数据\n    inband = inraster.GetRasterBand(1)  # 这个波段就是最后想要转为矢量的波段，如果是单波段数据的话那就都是1\n    prj = osr.SpatialReference()\n    prj.ImportFromWkt(inraster.GetProjection())  # 读取栅格数据的投影信息，用来为后面生成的矢量做准备\n    outshp = shp_save_path + os.path.basename(tif_path)[:-4] + '.shp'  # 给后面生成的矢量准备一个输出文件名，这里就是把原栅格的文件名后缀名改成shp了\n\n    drv = ogr.GetDriverByName(\"ESRI Shapefile\")\n    if os.path.exists(outshp):  # 若文件已经存在，则删除它继续重新做一遍\n        drv.DeleteDataSource(outshp)\n    Polygon = drv.CreateDataSource(outshp)  # 创建一个目标文件\n    Poly_layer = Polygon.CreateLayer(os.path.basename(tif_path)[:-4], srs=prj, geom_type=ogr.wkbMultiPolygon)  # 对shp文件创建一个图层，定义为多个面类\n    newField = ogr.FieldDefn('value', ogr.OFTReal)  # 给目标shp文件添加一个字段，用来存储原始栅格的pixel value,浮点型，\n    Poly_layer.CreateField(newField)\n\n    gdal.Polygonize(inband, None, Poly_layer, 0)  # 核心函数，执行的就是栅格转矢量操作\n\n    Polygon.SyncToDisk()\n    Polygon = None\n    return outshp\n\n\n# 执行转换函数，获取存储路径\nndwi_shp = tif_to_shp(sr_out,out_path)\n\n\n\n5. 成果出图\n\n利用rasterio更容易实现将shape文件与栅格影像进行叠加显示\nplt.imshow更适合临时出图，更专业的出版需求通常采用axs这种方式实现\n\n\nfig, axs = plt.subplots(2, 2, figsize=(12,10))\n\nvector = gpd.read_file(ndwi_shp)\nimg=rasterio.open(out_path+\"T50TLK_20220825T030519_quickclip.tif\")\n\nndwiimg=rasterio.open(ndwipath)\nsrimg=rasterio.open(sr_out)\np1 =rasterio.plot.show(img, ax=axs[0,0],title='RGB')\nndwi =rasterio.plot.show(ndwiimg, ax=axs[0,1],title='NDWI')\nsr =rasterio.plot.show(srimg, ax=axs[1,0],title='Water Mask')\np1 =rasterio.plot.show(img, ax=axs[1,1],title='Water Ploygon')\nvector.plot(ax=axs[1,1],edgecolor='red', linewidth=0.5,facecolor='none')\nfig.suptitle('Water Extraction Steps', fontsize=40)\n\nfor ax in fig.get_axes():\n    ax.label_outer()\n    ax.ticklabel_format(style ='plain') \nfig.tight_layout()\nplt.subplots_adjust(left=None, bottom=None, right=None, top=0.9, wspace=None, hspace=0.2)\n\nfig.savefig(out_path+\"T50TLK_20220825T030519_result.png\",facecolor='white')\n\n\n\n\n\n想了解更多请关注[45度科研人]公众号，欢迎给我留言！"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n对话AI for Science先行者——2023和鲸社区科研闭门会\n\n\n\nDec 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChatGPT加持下的Net Bing，将带来一场新的技术风暴\n\n\n\nMar 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWork with Remote Sensing Data in Python: Lesson 1-5-2\n\n\n\nFeb 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWork with Remote Sensing Data in Python: Lesson 1-5-1\n\n\n\nFeb 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWork with Remote Sensing Data in Python: Lesson 1-4-2\n\n\n\nJan 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Google Earth Engine in python\n\n\n\nJan 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWork with Remote Sensing Data in Python: Lesson 1-4-1\n\n\n\nJan 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWork with Remote Sensing Data in Python: Lesson 1-3-2\n\n\n\nDec 31, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWork with Remote Sensing Data in Python: Lesson 1-3-1\n\n\n\nNov 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWork with Remote Sensing Data in Python: Lesson 3-3\n\n\n\nNov 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 8, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publication/index.html",
    "href": "publication/index.html",
    "title": "Junchuan Yu",
    "section": "",
    "text": "Total: 40; First-author: 13; Citations:Google Scholar | Research Gate; Source code: GitHub\n\n \n\nJunchuan Yu, Liang Zhang, Qiang Li, et al., 3D autoencoder algorithm for lithological mapping using ZY-1 02D hyperspectral imagery: a case study of Liuyuan region.Journal of Applied Remote Sensing. 2021, 15(4): 42610. [link] [code] \nJunchuan Yu, Yichuan Li, Xiangxiang Zheng, et al., An effective cloud detection method for Gaofen-5 images via deep learning.Remote Sensing. 2020, 12(13): 2106. [link] [code] \nJunchuan Yu, Yichuan Li, Bokun Yan, et al., Comparison of GaoFen-5 hyperspectral and airborne hyperspectral imagery: case study of Gossan identification in Subei area.Seventh Symposium on Novel Photoelectronic Detection Technology and Applications. 2021, 11763: 1479-1483. [link] \nJunchuan Yu, Yichuan Li, Siqun Zheng, et al., Knowledge guided classification of airborne hyperspectral images with deep convolutional neural network.AOPC 2020: Optical Spectroscopy and Imaging, and Biomedical Optics. 2020, 11566: 67-71. [link] \nJunchuan Yu, Yichuan Li, Xiangxiang Xiangxiang, et al., Radiometric optimization of airborne hyperspectral imagery for large-scale geological applications.Sixth Symposium on Novel Optoelectronic Detection Technology and Applications. 2020, 11455: 905-910. [link] \nJunchuan Yu, Bokun Yan, Wenliang Liu, et al., Seamless Mosaicking of Multi-strip Airborne Hyperspectral Images Based on Hapke Model.International Conference on Sensing and Imaging. 2019,506:285-292. [link] \nJunchuan Yu and Bokun Yan, Efficient solution of large-scale domestic hyperspectral data processing and geological application.2017 International Workshop on Remote Sensing with Intelligent Processing (RSIP). 2017: 44565. [link] \nJunchuan Yu, Wenliang Liu, Bokun Yan, et al., Inversion of geochemical compositions of basalts based on field measured spectra.Remote Sensing for Land & Resources.2017,29(1): 158-163. [link] \nJunchuan Yu, Xuanxue Mo, Xuehui Yu, et al., Petrogenesis and geological implications of the Late Triassic potassic-ultrapotassic rocks in Changdu Block, northern segment of the Sanjiang area.Acta Petrologica Sinica. 2014, 30(11): 3334-3344. [link] \nJunchuan Yu, Xuanxue Mo, Yichuan Li, et al., The Cenozoic Basalts From Simao Microcontinent, Eastern Tibet Plateau: The Geochemical Characteristics and Tectonic Significance.Acta Geologica Sinica. 2013, 87:88-89. [link] \nJunchuan Yu, Xuanxue Mo, Xuehui Yu, et al., Petrogenesis of Late Triassic Volcanic Rocks from Changdu Microcontinent, NE Tibet (West China): Constraints from Geochemistry and Sr-Nd-Pb Isotopes.Acta Geologica Sinica. 2013, 87: 123-137. [link] \nJunchuan Yu, Xuanxue Mo, Xuehui Yu, et al., Geochemical characteristics and petrogenesis of Permian basaltic rocks in Keping area, Western Tarim basin: A record of plume-lithosphere interaction.Journal of Earth Science.2012, 23(4): 442-454. [link] \nJunchuan, Yu, Xuanxue, Mo, Guochen, Dong, et al., Felsic volcanic rocks from northern Tarim, NW China: Zircon U-Pb dating and geochemical characteristics.Acta Petrologica Sinica. 2011, 27(7): 2184-2194. [link]"
  },
  {
    "objectID": "talks/2022-11-23_NKRDP-DL/index.html",
    "href": "talks/2022-11-23_NKRDP-DL/index.html",
    "title": "Identification of Correlation Factors of Hidden Geohazard via Deep Learning",
    "section": "",
    "text": "Slides:"
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\nAI for Science HeyWhale Annual Conference(2023)\n\n\n\n\n\n\nDec 16, 2023\n\n\n\n\n\n\n  \n\n\n\n\n\nNational Key R&D Program (2022)\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Junchuan’s blog",
    "section": "",
    "text": "1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\n\n对话AI for Science先行者——2023和鲸社区科研闭门会\n\n\n\n\nPosts\n\n\nDeep leanring\n\n\nTalks\n\n\n \n\n\n\n\nDec 16, 2023\n\n\nJunchuan Yu\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPosts\n\n\nDeep leanring\n\n\nAPP\n\n\nTeaching\n\n\nWorkshop\n\n\n \n\n\n\n\nAug 27, 2023\n\n\nJunchuan Yu\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPosts\n\n\nHyperspectral\n\n\nTeaching\n\n\nWorkshop\n\n\n \n\n\n\n\nAug 11, 2023\n\n\nJunchuan Yu\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPosts\n\n\nHyperspectral\n\n\nTeaching\n\n\nWorkshop\n\n\n \n\n\n\n\nAug 2, 2023\n\n\nJunchuan Yu\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPosts\n\n\nHyperspectral\n\n\nTeaching\n\n\nWorkshop\n\n\n \n\n\n\n\nJul 25, 2023\n\n\nJunchuan Yu\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPosts\n\n\nDeep leanring\n\n\nAPP\n\n\n \n\n\n\n\nApr 19, 2023\n\n\nJunchuan Yu\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPosts\n\n\nDeep leanring\n\n\nAPP\n\n\n \n\n\n\n\nApr 9, 2023\n\n\nJunchuan Yu\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPosts\n\n\nDeep leanring\n\n\nAPP\n\n\n \n\n\n\n\nApr 2, 2023\n\n\nJunchuan Yu\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPosts\n\n\nDeep leanring\n\n\nAPP\n\n\n \n\n\n\n\nMar 11, 2023\n\n\nJunchuan Yu\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\n\nChatGPT加持下的Net Bing，将带来一场新的技术风暴\n\n\n\n\nPosts\n\n\nTalks\n\n\nAPP\n\n\n \n\n\n\n\nMar 4, 2023\n\n\nJunchuan Yu\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPosts\n\n\nDeep leanring\n\n\nAPP\n\n\n \n\n\n\n\nFeb 27, 2023\n\n\nJunchuan Yu\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\n\nWork with Remote Sensing Data in Python: Lesson 1-5-2\n\n\n\n\nPosts\n\n\nDeep leanring\n\n\nTeaching\n\n\nWorkshop\n\n\n \n\n\n\n\nFeb 10, 2023\n\n\nJunchuan Yu\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\n\nWork with Remote Sensing Data in Python: Lesson 1-5-1\n\n\n\n\nPosts\n\n\nDeep leanring\n\n\nTeaching\n\n\nWorkshop\n\n\nGEE\n\n\n \n\n\n\n\nFeb 6, 2023\n\n\nJunchuan Yu\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\n\nWork with Remote Sensing Data in Python: Lesson 1-4-2\n\n\n\n\nPosts\n\n\nDeep leanring\n\n\nTeaching\n\n\nWorkshop\n\n\n \n\n\n\n\nJan 29, 2023\n\n\nJunchuan Yu\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\n\nUsing Google Earth Engine in python\n\n\n\n\nPosts\n\n\nDeep leanring\n\n\nTeaching\n\n\nGEE\n\n\n \n\n\n\n\nJan 27, 2023\n\n\nJunchuan Yu\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\n\nWork with Remote Sensing Data in Python: Lesson 1-4-1\n\n\n\n\nPosts\n\n\nDeep leanring\n\n\nTeaching\n\n\nWorkshop\n\n\n \n\n\n\n\nJan 18, 2023\n\n\nJunchuan Yu\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\n\nWork with Remote Sensing Data in Python: Lesson 1-3-2\n\n\n\n\nPosts\n\n\nDeep leanring\n\n\nTeaching\n\n\nWorkshop\n\n\n \n\n\n\n\nDec 31, 2022\n\n\nJunchuan Yu\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\n\nWork with Remote Sensing Data in Python: Lesson 1-3-1\n\n\n\n\nPosts\n\n\nDeep leanring\n\n\nTeaching\n\n\nWorkshop\n\n\n \n\n\n\n\nNov 17, 2022\n\n\nJunchuan Yu\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\n\nWork with Remote Sensing Data in Python: Lesson 3-3\n\n\n\n\nPosts\n\n\nDeep leanring\n\n\nTeaching\n\n\nWorkshop\n\n\n \n\n\n\n\nNov 15, 2022\n\n\nJunchuan Yu\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPosts\n\n\nDeep leanring\n\n\nArt\n\n\n \n\n\n\n\nOct 8, 2022\n\n\nJunchuan Yu\n\n\n0 min\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n See all"
  },
  {
    "objectID": "index.html#recent-talks-palestras-recentes",
    "href": "index.html#recent-talks-palestras-recentes",
    "title": "Junchuan’s blog",
    "section": "Recent Talks / Palestras recentes",
    "text": "Recent Talks / Palestras recentes\n\n\n\n\n\n\n \n\n\n\nTalks\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentification of Correlation Factors of Hidden Geohazard via Deep Learning\n\n\nNational Key R&D Program (2022)\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n\n\nNo matching items\n\n\n See all"
  },
  {
    "objectID": "index.html#posts",
    "href": "index.html#posts",
    "title": "Junchuan’s blog",
    "section": "Posts",
    "text": "Posts\n\n\n\n See all posts"
  },
  {
    "objectID": "index.html#recent-talks",
    "href": "index.html#recent-talks",
    "title": "Junchuan’s blog",
    "section": "Recent Talks",
    "text": "Recent Talks\n\n\n\n\n  \n\n\n\n\n’AI+遥感’技术地学应用实践与展望\n\n\nAI for Science HeyWhale Annual Conference (2023)\n\n\n\n\n\n\nDec 16, 2023\n\n\n\n\n\n\n  \n\n\n\n\nIdentification of Correlation Factors of Hidden Geohazard via Deep Learning\n\n\nNational Key R&D Program (2022)\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\nNo matching items\n\n\n See all"
  },
  {
    "objectID": "talks/2022-11-23_NKRDP-DL/main.html",
    "href": "talks/2022-11-23_NKRDP-DL/main.html",
    "title": "Identification of Correlation Factors of Hidden Geohazard via Deep Learning",
    "section": "",
    "text": "Slides:"
  },
  {
    "objectID": "posts/2022-11-17wrsdp_1-3-1/Lesson1-3-1.html",
    "href": "posts/2022-11-17wrsdp_1-3-1/Lesson1-3-1.html",
    "title": "哨兵2号数据获取及处理",
    "section": "",
    "text": "练习1 哨兵-2号数据获取及处理\n\n1. 数据介绍\n哨兵-2号卫星携带一枚多光谱成像仪(MSI)，高度为786km，可覆盖13个光谱波段，幅宽达290千米。地面分辨率分别为10m、20m和60m、一颗卫星的重访周期为10天，两颗互补，重访周期为5天。从可见光和近红外到短波红外，具有不同的空间分辨率，在光学数据中，哨兵-2号数据是唯一一个在红边范围含有三个波段的数据，这对监测植被健康信息非常有效。\n13个波段中蓝色 (B2)、绿色 (B3)、红色 (B4) 和近红外 (B8) 波段具有 10 米的分辨率；红端（B5）、近红外 NIR（B6、B7 和 B8A）以及短波红外 SWIR（B11和B12）的地面采样距离为20米；沿海大气气溶胶 (B1) 和卷云波段 (B10) 的像素大小为 60 米；\n\n环境要求： * GDAL==2.4.1 * numpy==1.19.5 * scikit-image==0.17.2 * sentinelsat==1.1.1 * zipp==3.6.0\n\nfrom sentinelsat import SentinelAPI, read_geojson, geojson_to_wkt\nfrom datetime import date\nimport os,glob,zipfile,rasterio\nimport rasterio.plot\nimport numpy as np\nfrom osgeo import gdal, osr, ogr, gdalconst\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport geopandas as gpd\n\n\n\n2. 数据下载\n\nSentinelsat包提供搜索、下载和检索Sentinel数据的功能\n数据来源：哥白尼数据开放访问中心\n\n\n\n# 输入ESA用户名和密码\nuser_name = 'chidan'\npassword  = 'chidan123!'\n\n# 登录ESA API\napi = SentinelAPI(user_name, password, 'https://apihub.copernicus.eu/apihub')\n\n# 检索的地理范围坐标\nfootprint = \"POLYGON((115.5 40.2, 116.0 40.2, 116.0 40.5, 115.5 40.5, 115.5 40.2))\" #左上为起始点，顺时针旋转，回到起始点闭合\n\n# 使用API接口查询，更多接口信息查询https://www.strerr.com/geojson/geojson.html#map=5/31.386/115.329\nproducts = api.query(footprint,\n                     date = ('20220801', date(2022,9,1)),\n                     platformname = 'Sentinel-2',\n                     producttype  = \"S2MSI2A\", #S2MSI1C\",# S2MS2Ap\n                     cloudcoverpercentage = (0, 1))\n\nprint(f\"一共检索到{len(products)}景符合条件的数据\\n\")\nfor i, product in enumerate(products):\n    product_info = api.get_product_odata(product)\n    print(product_info['title'])\n\n一共检索到4景符合条件的数据\n\nS2B_MSIL2A_20220825T030519_N0400_R075_T50TLK_20220825T052806\nS2B_MSIL2A_20220825T030519_N0400_R075_T50TMK_20220825T052806\nS2A_MSIL2A_20220810T030531_N0400_R075_T50TMK_20220810T083201\nS2B_MSIL2A_20220805T030529_N0400_R075_T50TMK_20220805T053117\n\n\n下载数据并解压,只下载一景\n\n# 创建下载路径\nsave_path = './data/'\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\n\n# 下载检索到的最后一景数据，对于长期存档的数据（3-6个月以上），会出现offline情况，在下载的时候，需要先请求，后台将数据调档至在线，时间大概是半个小时，才能下载\nfor i, product in enumerate(products):\n    if i == 1:\n        break \n    product_info = api.get_product_odata(product)\n    # if product_info['title'] == \"S2B_MSIL2A_20220805T030529_N0400_R075_T50TMK_20220805T053117\":\n    if product_info['Online']:\n        print('数据在线，正在下载')\n        file_save_path = api.download(product_info['id'],save_path)\n        sentinel_data_path = file_save_path['path']\n    else:\n        print('数据未在线，请等待30分钟重试')\n\n数据在线，正在下载\n\n\n\n\n\n\n\n\n\n# 解压数据\nf = zipfile.ZipFile(sentinel_data_path,'r') \n\n# 解压到save_path\nfor file in f.namelist():\n    f.extract(file,save_path)               \nf.close()\n\nSentinel-2产品解压后为SAFE格式，SAFE文件包含以下几个内容： * 一个manifest.safe文件，其中包含 XML 格式的一般产品信息 * JPEG2000 格式的预览图像 * 测量（传感器扫描成像）数据集的子文件夹，包括 GML-JPEG2000 格式的图像数据（颗粒/瓦片） * 数据条级别信息的子文件夹 * 带有辅助数据的子文件夹（例如国际地球自转和参考系统 (IERS) 公告） * HTML 预览\n 获取数据的信息\n获取数据的信息\n\n\nxml_path = sentinel_data_path[:-4] + '.SAFE' + os.sep +'MTD_MSIL2A.xml'\nroot_ds = gdal.Open(xml_path)\nds_list = root_ds.GetSubDatasets()  # 获取子数据集。该数据以数据集形式存储且以子数据集形式组织\nfor i in range(len(ds_list)):\n    visual_ds = gdal.Open(ds_list[i][0])  # 打开第i个数据子集的路径。ds_list有4个子集，内部前段是路径，后段是数据信息\n    print(ds_list[i][0])\n    print(f'数据波段为：{ds_list[i][1]}')\n    print(f'仿射矩阵信息：{visual_ds.GetGeoTransform()}')\n    print(f'投影信息：{visual_ds.GetProjection()}')\n    print(f'栅格波段数：{visual_ds.RasterCount}')\n    print(f'栅格列数（宽度）：{visual_ds.RasterXSize} 栅格行数（高度）：{visual_ds.RasterYSize}')\n    print(\"\\n\")\n\nSENTINEL2_L2A:data\\S2B_MSIL2A_20220825T030519_N0400_R075_T50TLK_20220825T052806.SAFE\\MTD_MSIL2A.xml:10m:EPSG_32650\n数据波段为：Bands B2, B3, B4, B8 with 10m resolution, UTM 50N\n仿射矩阵信息：(300000.0, 10.0, 0.0, 4500000.0, 0.0, -10.0)\n投影信息：PROJCS[\"WGS 84 / UTM zone 50N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32650\"]]\n栅格波段数：4\n栅格列数（宽度）：10980 栅格行数（高度）：10980\n\n\nSENTINEL2_L2A:data\\S2B_MSIL2A_20220825T030519_N0400_R075_T50TLK_20220825T052806.SAFE\\MTD_MSIL2A.xml:20m:EPSG_32650\n数据波段为：Bands B5, B6, B7, B8A, B11, B12, AOT, CLD, SCL, SNW, WVP with 20m resolution, UTM 50N\n仿射矩阵信息：(300000.0, 20.0, 0.0, 4500000.0, 0.0, -20.0)\n投影信息：PROJCS[\"WGS 84 / UTM zone 50N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32650\"]]\n栅格波段数：11\n栅格列数（宽度）：5490 栅格行数（高度）：5490\n\n\nSENTINEL2_L2A:data\\S2B_MSIL2A_20220825T030519_N0400_R075_T50TLK_20220825T052806.SAFE\\MTD_MSIL2A.xml:60m:EPSG_32650\n数据波段为：Bands B1, B9, AOT, CLD, SCL, SNW, WVP with 60m resolution, UTM 50N\n仿射矩阵信息：(300000.0, 60.0, 0.0, 4500000.0, 0.0, -60.0)\n投影信息：PROJCS[\"WGS 84 / UTM zone 50N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32650\"]]\n栅格波段数：7\n栅格列数（宽度）：1830 栅格行数（高度）：1830\n\n\nSENTINEL2_L2A:data\\S2B_MSIL2A_20220825T030519_N0400_R075_T50TLK_20220825T052806.SAFE\\MTD_MSIL2A.xml:TCI:EPSG_32650\n数据波段为：True color image, UTM 50N\n仿射矩阵信息：(300000.0, 10.0, 0.0, 4500000.0, 0.0, -10.0)\n投影信息：PROJCS[\"WGS 84 / UTM zone 50N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32650\"]]\n栅格波段数：3\n栅格列数（宽度）：10980 栅格行数（高度）：10980\n\n\n\n\n\n\n3. 数据格式转换\n我们所需要的图像数据储存在IMG_DATA文件夹里，该文件夹内有三个子文件夹，分别存有三种不同分辨率的数据，数据格式为JPEG2000。接下来的任务为：\n\n在解压缩后的文件夹查找到jp2数据\n对各个波段的jp2数据进行解压缩，保存为GeoTiff格式数据\n\n\n# 检索解压目录下的所有子文件夹，找到IMG_DATA文件夹\nfor root, _, _ in os.walk(sentinel_data_path[:-4] + '.SAFE'):\n    if root.endswith(\"IMG_DATA\"):\n        IMG_DATA_path = root\n\n\ndef get_image_name(image_directory):\n    file = image_directory + '\\R10m'              \n    names = os.listdir(file)[0]\n    img_name = names[:22]\n    return img_name\n\n\n# 获得文件名\nimg_identifier = get_image_name(IMG_DATA_path)\n\n# 拼接成各个波段文件绝对路径\n\n# 20m分辨率数据\njp2_20m_list = [IMG_DATA_path + os.sep +'R20m\\%s_B8A_20m.jp2' %img_identifier ,\n                IMG_DATA_path + os.sep +'R20m\\%s_B11_20m.jp2' %img_identifier ,\n                IMG_DATA_path + os.sep +'R20m\\%s_B12_20m.jp2' %img_identifier,\n                IMG_DATA_path + os.sep +'R20m\\%s_B05_20m.jp2' %img_identifier,\n                IMG_DATA_path + os.sep +'R20m\\%s_B06_20m.jp2' %img_identifier,\n                IMG_DATA_path + os.sep +'R20m\\%s_B07_20m.jp2' %img_identifier]\n\n# 10m分辨率数据\njp2_10m_list = [IMG_DATA_path + os.sep +'R10m\\%s_B02_10m.jp2' %img_identifier,\n                IMG_DATA_path + os.sep +'R10m\\%s_B03_10m.jp2' %img_identifier,\n                IMG_DATA_path + os.sep +'R10m\\%s_B04_10m.jp2' %img_identifier,\n                IMG_DATA_path + os.sep +'R10m\\%s_B08_10m.jp2' %img_identifier]\n\ndef jp2Totif(jp2_path,save_path):\n    \"\"\"\n    格式转换：jp2 转 tif\n    :param jp2_path: jp2数据地址\n    :param save_path: 存储路径\n    :return: 存储地址\n    \"\"\"\n    file_name = os.path.basename(jp2_path)[:-4]\n    save_file = os.path.join(save_path,file_name)\n    save_file = save_file+'.tif'\n    dataset = gdal.Open(jp2_path)\n    rows = dataset.RasterYSize\n    cols = dataset.RasterXSize\n    projection = dataset.GetProjection()\n    trans = dataset.GetGeoTransform()\n    data = dataset.ReadAsArray()\n    if data.dtype == 'uint16':\n        driver = gdal.GetDriverByName('GTiff')\n        out_dataset = driver.Create(save_file, cols, rows, 1, gdal.GDT_UInt16)\n        out_dataset.SetProjection(projection)\n        out_dataset.SetGeoTransform(trans)\n        out_dataset.GetRasterBand(1).WriteArray(data)\n        out_dataset.GetRasterBand(1).SetNoDataValue(0)\n        out_dataset.FlushCache()\n        del dataset, out_dataset\n    elif data.dtype == 'uint8':\n        driver = gdal.GetDriverByName('GTiff')\n        out_dataset = driver.Create(save_file, cols, rows, 1, gdal.GDT_Byte)\n        out_dataset.SetProjection(projection)\n        out_dataset.SetGeoTransform(trans)\n        out_dataset.GetRasterBand(1).WriteArray(data)\n        out_dataset.GetRasterBand(1).SetNoDataValue(0)\n        out_dataset.FlushCache()\n        del dataset, out_dataset\n    return save_file\n\n仅堆叠可见波段NIR、RE和SWIR1和SWIR2（波段2、3、4、8、8A、11、12）。将20m波段（8A、11和12）重新采样至10m。\n\n# 创建20m分辨率数据存储路径\ntif_20m_save_path = \".\\\\data\"+ os.sep + img_identifier+ os.sep+ \"20m\"\nif not os.path.exists(tif_20m_save_path):\n    os.makedirs(tif_20m_save_path)\n\n# 将20m分辨率数据由jp2格式转换为tif格式，并记录地址到tif_20m_list\ntif_20m_list = []\nfor jp2_path in jp2_20m_list:\n    tif_path = jp2Totif(jp2_path, tif_20m_save_path)\n    tif_20m_list.append(tif_path)\n    \n# 创建10m分辨率数据存储路径\ntif_10m_save_path = \".\\\\data\"+ os.sep + img_identifier+ os.sep+\"10m\"\nif not os.path.exists(tif_10m_save_path):\n    os.makedirs(tif_10m_save_path)\n\n# 将10m分辨率数据由jp2格式转换为tif格式，并记录地址到tif_10m_list\ntif_10m_list = []\nfor jp2_path in jp2_10m_list:\n    tif_path = jp2Totif(jp2_path, tif_10m_save_path)\n    tif_10m_list.append(tif_path) \n\n\n\n4. 数据重采样\n将20m分辨率数据重采样至10m分辨率\n\ndef ReprojectImages(inputfilePath,outputfilePath,referencefilefilePath):\n    # 获取输出影像信息\n    inputrasfile = gdal.Open(inputfilePath, gdal.GA_ReadOnly)\n    inputProj = inputrasfile.GetProjection()\n    # 获取参考影像信息\n    referencefile = gdal.Open(referencefilefilePath, gdal.GA_ReadOnly)\n    referencefileProj = referencefile.GetProjection()\n    referencefileTrans = referencefile.GetGeoTransform()\n    bandreferencefile = referencefile.GetRasterBand(1)\n    Width= referencefile.RasterXSize\n    Height = referencefile.RasterYSize\n    nbands = referencefile.RasterCount\n    # 创建重采样输出文件（设置投影及六参数）\n    driver = gdal.GetDriverByName('GTiff')\n    output = driver.Create(outputfilePath, Width,Height, nbands, bandreferencefile.DataType)\n    output.SetGeoTransform(referencefileTrans)\n    output.SetProjection(referencefileProj)\n    # 参数说明 输入数据集、输出文件、输入投影、参考投影、重采样方法(最邻近内插\\双线性内插\\三次卷积等)、回调函数\n    gdal.ReprojectImage(inputrasfile, output, inputProj, referencefileProj, gdalconst.GRA_Bilinear,0.0,0.0,)\n\n\n# 获取一景10m分辨率影像作为参考影像\nreference_tif = tif_10m_list[0] \n\n# 将20m分辨率数据重采样至10m,并保存至 tif_10m_save_path 路径下\nfor tif_path in tif_20m_list:\n    file_name = os.path.basename(tif_path)[:-7]\n    save_tif_path = tif_10m_save_path + os.sep + file_name +'10m.tif'\n    ReprojectImages(tif_path, save_tif_path, reference_tif)\n\n\n\n5. 波段叠合\n\n将多个不同波段的的TIF文件合为一个多波段TIF文件\n叠合波段为可见波段NIR、RE和SWIR1和SWIR2（波段2、3、4、8、8A、11、12）\n\n\n# 读图像文件\ndef read_img(filename):\n\n    dataset = gdal.Open(filename)  # 打开文件\n\n    im_width = dataset.RasterXSize  # 栅格矩阵的列数\n    im_height = dataset.RasterYSize  # 栅格矩阵的行数\n    # im_bands = dataset.RasterCount  # 波段数\n    im_geotrans = dataset.GetGeoTransform()  # 仿射矩阵，左上角像素的大地坐标和像素分辨率\n    im_proj = dataset.GetProjection()  # 地图投影信息，字符串表示\n    im_data = dataset.ReadAsArray(0, 0, im_width, im_height)\n\n    del dataset   #关闭对象dataset，释放内存\n    # return im_width, im_height, im_proj, im_geotrans, im_data,im_bands\n    return  im_proj, im_geotrans, im_data, im_width,im_height\n \n# 遥感影像的存储\n# 写GeoTiff文件\ndef Write_Tiff(img_arr, geomatrix, projection,path):\n#     img_bands, img_height, img_width = img_arr.shape\n    if 'int8' in img_arr.dtype.name:\n        datatype = gdal.GDT_Byte\n    elif 'int16' in img_arr.dtype.name:\n        datatype = gdal.GDT_UInt16\n    else:\n        datatype = gdal.GDT_Float32\n\n    if len(img_arr.shape) == 3:\n        img_bands, img_height, img_width = img_arr.shape\n        driver = gdal.GetDriverByName(\"GTiff\")\n        dataset = driver.Create(path, int(img_width), int(img_height), int(img_bands), datatype)\n    #     print(path, int(img_width), int(img_height), int(img_bands), datatype)\n        if(dataset!= None) and (geomatrix !='') and (projection!=''):\n            dataset.SetGeoTransform(geomatrix) #写入仿射变换参数\n            dataset.SetProjection(projection) #写入投影\n        for i in range(img_bands):\n            dataset.GetRasterBand(i+1).WriteArray(img_arr[i])\n        del dataset\n\n    elif len(img_arr.shape) == 2:\n        # img_arr = np.array([img_arr])\n        img_height, img_width = img_arr.shape\n        img_bands=1\n        #创建文件\n        driver = gdal.GetDriverByName(\"GTiff\")\n        dataset = driver.Create(path, int(img_width), int(img_height), int(img_bands), datatype)\n    #     print(path, int(img_width), int(img_height), int(img_bands), datatype)\n        if(dataset!= None):\n            dataset.SetGeoTransform(geomatrix) #写入仿射变换参数\n            dataset.SetProjection(projection) #写入投影\n        dataset.GetRasterBand(1).WriteArray(img_arr)\n        del dataset\n \ndef merge_tif(tif_path_list, output_tif):\n    arr_list = []\n    for tif_path in tif_path_list:\n        proj, geotrans, data, row, column  = read_img(tif_path)\n        arr_list.append(data)\n    all_arr = np.array(arr_list)\n    Write_Tiff(all_arr,geotrans,proj,output_tif)\n    \n\n筛选数据\n\ntif_list = os.listdir(tif_10m_save_path)   # 获取所有10m分辨率tif数据的名称\ntif_path_list = []                        # 以列表的形式存储各个波段的路径\n\nband_names = [\"B02\",\"B03\",\"B04\",\"B08\",\"B8A\",\"B11\",\"B12\"]  # 需要堆叠的波段名\n\nfor band_name in band_names:\n    for tif_name in tif_list:\n        if band_name == tif_name[-11:-8]:\n            tif_path_list.append(tif_10m_save_path + os.sep + tif_name)\n\nprint(tif_path_list)\n\n['.\\\\data\\\\T50TLK_20220825T030519\\\\10m\\\\T50TLK_20220825T030519_B02_10m.tif', '.\\\\data\\\\T50TLK_20220825T030519\\\\10m\\\\T50TLK_20220825T030519_B03_10m.tif', '.\\\\data\\\\T50TLK_20220825T030519\\\\10m\\\\T50TLK_20220825T030519_B04_10m.tif', '.\\\\data\\\\T50TLK_20220825T030519\\\\10m\\\\T50TLK_20220825T030519_B08_10m.tif', '.\\\\data\\\\T50TLK_20220825T030519\\\\10m\\\\T50TLK_20220825T030519_B8A_10m.tif', '.\\\\data\\\\T50TLK_20220825T030519\\\\10m\\\\T50TLK_20220825T030519_B11_10m.tif', '.\\\\data\\\\T50TLK_20220825T030519\\\\10m\\\\T50TLK_20220825T030519_B12_10m.tif']\n\n\n\n# 执行叠加函数\nmerge_out =save_path+img_identifier+os.sep+img_identifier+\"_merge.tif\"\nmerge_tif(tif_path_list, merge_out)\n\n\n\n6. 真彩色影像可视化\n\n对图像进行拉伸显示\n转换成0-255的快视图并保存\n\n\n# 显示叠加结果数据信息\n\n#   归一化函数\ndef stretch(band, lower_percent=2, higher_percent=98): #2和95表示分位数\n    band=np.array(band,dtype=np.float32)\n    c = np.percentile(band, lower_percent)*1.0\n    d = np.percentile(band, higher_percent)*1.0       \n    band[band<c] = c\n    band[band>d] = d\n    out =  (band - c)  / (d - c)  \n    return out.astype(np.float32)\ndef stretch_n(data, n_band=3):  #该操作讲改变原始数据，因此data用.copy，不对原始数据进行更改\n    data=np.array(data,dtype=np.float32)\n    for k in range(n_band):\n            data[:,:,k] = stretch(data[:,:,k])\n    return data\ndef rgb(img_data,iftran=True):\n    img_data_3b = img_data[:3,:,:]                  # 取前三个波段 B02,B03,B04\n    if iftran:\n        img_data_3b = img_data_3b[::-1,:,:]             # 将B02,B03,B04转成B04,B03,B02 (BGR转RGB)\n    img_data    = img_data_3b.transpose(1,2,0)     # C,H,W -> H,W,C\n    return img_data \n\nproj, geotrans, img_data, row, column  = read_img(merge_out)\nimg_data_r=rgb(img_data) #提取3波段改变rgb顺序和数据维度\nimg_data_rgb_s = np.uint8(stretch_n(img_data_r.copy())*255) # 数据值域缩放至（0~255）\n\nplt.figure(figsize=(8,8))\nplt.imshow(Image.fromarray(img_data_rgb_s))\nplt.show()\n\n仿射矩阵信息：(300000.0, 10.0, 0.0, 4500000.0, 0.0, -10.0)\n投影信息：PROJCS[\"WGS 84 / UTM zone 50N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32650\"]]\n栅格波段数：7\n栅格列数（宽度）：10980 栅格行数（高度）：10980\n\n\n\n\n\n\n\n\nquickimg =save_path+img_identifier+os.sep+img_identifier+\"_quickimg.tif\"\nWrite_Tiff(img_data_rgb_s.transpose(2,0,1),ds.GetGeoTransform(),ds.GetProjection(),quickimg)\n\n\n\n7. 重投影\n如果需要对数据进行投影操作，可以利用gdal.warp实现\n\nds = gdal.Open(quickimg)       # 打开文件\nrojectedtmp =save_path+img_identifier+os.sep+img_identifier+\"_projected.tif\"\nds = gdal.Warp(rojectedtmp, ds, dstSRS='EPSG:4326')    # 有投影的需求可以使用warp命令，epsg可以通过https://epsg.io/查询，这里给出的是wgs84\n\n\nproj, geotrans, img_data, row, column  = read_img(rojectedtmp)\n\n# 显示重投影结果信息\nprint(f'仿射矩阵信息：{geotrans}',f'投影信息：{proj}')\nprint(\"\\n\")\n\nimg_data_ = img_data.transpose(1,2,0)          # C,H,W -> H,W,C\nplt.imshow(img_data_)\n\n仿射矩阵信息：(114.63531921298156, 0.00010513256977891853, 0.0, 40.64592849014489, 0.0, -0.00010513256977891853) 投影信息：GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]\n\n\n\n\n<matplotlib.image.AxesImage at 0x16971629430>\n\n\n\n\n\n\n\n8. 数据裁剪\n\n利用rasterio库更容易将shp与影像进行叠加显示\n利用gdal.warp实现数据裁剪\n\n\nclip_shp = './data/clippolygon.shp'\nvector = gpd.read_file(clip_shp)\nimg=rasterio.open(quickimg)\n\nfig, ax = plt.subplots(figsize=(5,5))\np1 =rasterio.plot.show(img, ax=ax,title='Sentinel 2 image of Guanting Reservoir')\nvector.plot(ax=ax,edgecolor='red', linewidth=1,facecolor = \"none\") #如果shp与raster坐标投影不一致无法同时显示\nplt.show()\n\n\n\n\n按矢量轮廓裁剪\n\n# 执行裁剪\nclip_output =save_path+img_identifier+os.sep+img_identifier+\"_clip.tif\"\n# 按矢量轮廓裁剪\ngdal.Warp(clip_output, merge_out, cutlineDSName = clip_shp, format=\"GTiff\", cropToCutline = True)\n\n<osgeo.gdal.Dataset; proxy of <Swig Object of type 'GDALDatasetShadow *' at 0x00000169B2690810> >\n\n\n\nproj, geotrans, img_data, row, column  = read_img(clip_output)\nimg_data_=rgb(img_data)\nplt.imshow(stretch_n(img_data_.copy()))\n\n<matplotlib.image.AxesImage at 0x16971673eb0>"
  },
  {
    "objectID": "posts/2023-1-18wrsdp_1-4-1/index.html",
    "href": "posts/2023-1-18wrsdp_1-4-1/index.html",
    "title": "基于GEE的哨兵1号数据获取及水体提取",
    "section": "",
    "text": "练习1 基于GEE的哨兵1号数据获取及水体提取取\n\n1. 数据获取\n本案例需要科学上网，成功安装geemap后方可使用，slope_correction函数封装在utils中，有需要的话请联系作者yujunchuan@mail.cgs.gov.cn\n\nimport geemap #需安装geemap\nimport ee\n# ee.Authenticate()\ngeemap.set_proxy(port=33210) #根据自己电脑配置修改\nee.Initialize()\nimport math\nfrom utils import *\n\n定义必要的函数\n\ndef clip_img(img):\n    return img.clip(region)\n\ndef filtter_speckles(img):\n    vv=img.select('VV')\n    vh= img.select('VH')\n    vv_smoothed = vv.focal_mean(50,'circle','meters').rename('VV_Filtered')\n    vh_smoothed = vh.focal_mean(50,'circle','meters').rename('VH_Filtered')\n    return img.addBands(vv_smoothed).addBands(vh_smoothed)\n\ndef SDWI(img):\n    VH = img.select(\"VH_mean\")\n    VV = img.select(\"VV_mean\")\n    x = VV.multiply(VH).multiply(ee.Number(10))\n    y=(x).log().rename('sdwi')\n    # sdwi=y.subtract(8)\n    file=img.addBands(y)\n    return file.select(\"sdwi\")\ndef addDate(image):\n    date = ee.Date(image.get('system:time_start')).format('yyyyMMdd')\n    return image.set(\"image_date\",date) \n\n设定鄱阳湖区域范围\n\nregion=ee.Geometry.Polygon([[[115.795898, 28.623104],[116.779175, 28.623104],[116.779175, 29.441989],[115.795898, 29.441989],[115.795898, 28.623104]]])\nMap=geemap.Map()\nMap.center_object(region)\nMap.addLayer(region,{}, 'region')\nMap\n\n\n\n2. SDWI指数计算\n编写sdwi计算函数，如非山区可以不进行地形校正\n\ndef sdwi_cal(data):\n    corrected = slope_correction(data).main().map(clip_img)\n    VV = corrected.mean().select('VV')\n    VH = corrected.mean().select('VH')\n    # Map.addLayer(VV, {'bands':['VV'], 'min':[-21], 'max':[0.5], 'gamma': 0.65}, 'SAR_RGB_VV')\n    # Map.addLayer(VH, {'bands':['VH'], 'min':[-28], 'max':[4], 'gamma': 0.65}, 'SAR_RGB_VH')\n    S1 = corrected.map(filtter_speckles)\n    name=S1.select(\"VV_Filtered\").first().get(\"image_date\")\n    SVV = S1.select(\"VV_Filtered\").reduce(ee.Reducer.mean()).rename(\"VV_mean\").set('image_date',name)\n    SVH = S1.select(\"VH_Filtered\").reduce(ee.Reducer.mean()).rename(\"VH_mean\").set('image_date',name)\n    Sen1=SVV.addBands(SVH)\n    sdwi = SDWI(Sen1).select('sdwi')\n    # Map.addLayer(sdwi,{'bands':['sdwi'], 'min':[8], 'max':[10], 'gamma': 0.65}, 'sdwi')\n    return sdwi\n\n根据阅读计算sdwi\n\ndef monthly_sdwi(mon):\n    start=ee.Date.fromYMD(year=2022,month=mon,day=1)\n    end=start.advance(1.0,'month')\n    data = (ee.ImageCollection('COPERNICUS/S1_GRD')\n            .filterBounds(region)\n            .filterMetadata('transmitterReceiverPolarisation','equals',[\"VV\", \"VH\"])\n            .filterMetadata('instrumentMode','equals','IW')\n            .filterDate(start, end)\n            .map(addDate))\n    sdwi=sdwi_cal(data)\n    name=sdwi.get(\"image_date\")\n    return sdwi\n\n\nmonthly=ee.ImageCollection(ee.List.sequence(6,9).map(monthly_sdwi))\nbandname=monthly.aggregate_array('image_date')\nsinglesdwi=monthly.toBands().rename(bandname) \nfor i in range(4):\n    Map.addLayer(singlesdwi.select(i).updateMask(singlesdwi.select(i).gte(8.5)),  {\"palette\":['6a5acd']}, \"swdi\"+str(i))\nMap\n\n\n\n\n\n\n3. 水面变化分析\n通过多视角查看前后水体变化\n\nMap=geemap.Map()\nMap.center_object(region)\nMap.split_map(left_layer='SATELLITE', right_layer='SATELLITE')\nvis= {\"palette\":['6a5acd']}\nleft_layer=geemap.ee_tile_layer(singlesdwi.select(0).updateMask(singlesdwi.select(0).gte(8.5)), vis,name='2022-06')\nright_layer=geemap.ee_tile_layer(singlesdwi.select(3).updateMask(singlesdwi.select(3).gte(8.5)), vis,name='2022-09')\nMap.split_map(left_layer, right_layer)\nMap\n\n\n年度12个月变化监测，提取永久水体\n\nmonthly=ee.ImageCollection(ee.List.sequence(1,12).map(monthly_sdwi))\nbandname=monthly.aggregate_array('image_date')\nsinglesdwi=monthly.toBands().rename(bandname) \nfor i in range(0,12):\n    if i ==0:\n        initial=singlesdwi.select(0).where(singlesdwi.select(i).lte(8.5),0).where(singlesdwi.select(i).gt(8.5),1)\n    else:\n        final=singlesdwi.select(i).where(singlesdwi.select(i).lte(8.5),0).where(singlesdwi.select(i).gt(8.5),1)\n        initial=initial.add(final)\n\n\nMap.addLayer(initial, {\"min\":[1],\"max\":[12], \"palette\":['ffffff', '99d9ea', '0000ff']}, \"water\")\nMap\n\n\n\n\n\n\n\n4. 下载提取结果\n\nfor i in range(8):\n    img=singlesdwi.select(i)\n    img=img.setDefaultProjection('epsg:4326',None,10)\n    name=singlesdwi.bandNames().getInfo()\n    outname = './sentinel_'+name[i]+str(i)+\".tif\"\n    geemap.download_ee_image(img, filename=outname,scale=10,region=region,crs='EPSG:4326')\n\n\n想了解更多请关注[45度科研人]公众号，欢迎给我留言！"
  },
  {
    "objectID": "posts/2023-1-27dynamicworld/index.html",
    "href": "posts/2023-1-27dynamicworld/index.html",
    "title": "Dynamic world data download",
    "section": "",
    "text": "Dynamic World data download\n\n\nThe real world is as dynamic as the people and natural processes that shape it. Dynamic World is a near realtime 10m resolution global land use land cover dataset, produced using deep learning, freely available and openly licensed. It is the result of a partnership between Google and the World Resources Institute, to produce a dynamic dataset of the physical material on the surface of the Earth. Dynamic World is intended to be used as a data product for users to add custom rules with which to assign final class values, producing derivative land cover maps.\n\nKey innovations of Dynamic World\n\n\nNear realtime data. Over 5000 Dynamic World image are produced every day, whereas traditional approaches to building land cover data can take months or years to produce. As a result of leveraging a novel deep learning approach, based on Sentinel-2 Top of Atmosphere, Dynamic World offers global land cover updating every 2-5 days depending on location.\n\n\nPer-pixel probabilities across 9 land cover classes. A major benefit of an AI-powered approach is the model looks at an incoming Sentinel-2 satellite image and, for every pixel in the image, estimates the degree of tree cover, how built up a particular area is, or snow coverage if there’s been a recent snowstorm, for example.\n\n\nTen meter resolution. As a result of the European Commission’s Copernicus Programme making European Space Agency Sentinel data freely and openly available, products like Dynamic World are able to offer 10m resolution land cover data. This is important because quantifying data in higher resolution produces more accurate results for what’s really on the surface of the Earth.\n\n\n\nApp: www.dynamicworld.app\nPaper: Dynamic World, Near real-time global 10m land use land cover mapping\nModel: github\nGeedata: Dynamic World V1\n\nThis tutorial explains how to download training data for Dynamic Earth using geemap\n\n1. Import the necessary lib\n\nimport geemap\nimport ee\n# ee.Authenticate()\ngeemap.set_proxy(port=33210)\nee.Initialize()\nimport os\nimport pandas   as  pd\nimport numpy as np\nfrom  pyproj  import  CRS,Transformer\n\nMap visualization\n\nMap = geemap.Map()\nMap.add_basemap('HYBRID')\nregion = ee.Geometry.BBox(-89.7088, 42.9006, -89.0647, 43.2167)\nMap.centerObject(region)\nimage = geemap.dynamic_world_s2(region, '2021-01-01', '2022-01-01')\nvis_params = {'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 3000}\nlandcover = geemap.dynamic_world(region, '2021-01-01', '2022-01-01', return_type='hillshade')\nMap.addLayer(landcover, {}, 'Land Cover')\nMap.add_legend(title=\"Dynamic World Land Cover\", builtin_legend='Dynamic_World')\nMap\n\n\n\n\n2. Download data\n\nConstruct a data combination with 8 bands, and use the data region, ID and other information provided by the paper to download image patches.\n\n\n## cloud remove\ndef rmCloudByQA(image):\n    qa = image.select('QA60')\n    cloudBitMask = 1 << 10\n    cirrusBitMask = 1 << 11 \n    mask =qa.bitwiseAnd(cloudBitMask).eq(0)and(qa.bitwiseAnd(cirrusBitMask).eq(0))\n    return image.updateMask(mask).toFloat().divide(1e4)\n## convert utm to geo\ndef utm_to_geo(region,crs):\n    strline=region.split('(',2)[-1].split(')',2)[0].split(', ')\n    ymin,xmin=np.array(strline[0].split(' '))\n    ymax,xmax=np.array(strline[2].split(' '))\n    ocrs = CRS.from_string(crs)\n    transformer = Transformer.from_crs(ocrs,'epsg:4326')\n    ymin,xmin=transformer.transform(ymin,xmin)\n    ymax,xmax=transformer.transform(ymax,xmax)\n    return xmin,ymin,xmax,ymax\n## Construct the 8-band data combination: Red,Green,Blue,Nir,Swir1,Swir2,MNDWI,Slope\ndef collection_cal(region,tiles,crs):\n    s2Img = ee.ImageCollection('COPERNICUS/S2_SR')\n    DEM = ee.Image('USGS/SRTMGL1_003')\n    DEM_clip = DEM.setDefaultProjection(crs,None,10).clip(region)\n    slope = ee.Terrain.slope(DEM_clip)\n    S = s2Img.filter(ee.Filter.inList('system:index',tiles)).filterBounds(region).filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 60)).map(rmCloudByQA).select('B.*').first().setDefaultProjection(crs,None,10)\n    S2=S.clip(region)\n    MNDWI = S2.normalizedDifference(['B3','B11']).rename(['MNDWI']).add(ee.Number(1))\n    DBANDS=S2.select(['B4','B3','B2','B8','B11','B12'])\n    final=DBANDS.addBands(MNDWI).addBands(slope).setDefaultProjection(crs,None,10)\n    return final\n## download function\ndef download_img(image,basename,out_dir):\n    dw_id=basename+'.tif'\n    # out_dir = os.path.expanduser(outdir)\n    out_dem_stats = os.path.join(out_dir, dw_id)\n    geemap.download_ee_image(image,out_dem_stats, num_threads=10,scale=10)\n\n\n## visual check if if required\n# Map = geemap.Map()\n# Map.centerObject(region)\n# Map.addLayer(region,{},'img')\n# Map\n\n\nThe expert data in the table is downloaded here, and the label file can be downloaded through the link provided in the paper. During the download process, the file download may fail due to network reasons. In this case, you can try a few more times.\n\n\ndata=pd.read_csv('./expert.csv')\noutdir='c:/temp/'\nfor index, row in data.iterrows():\n    basename= row['dw_id']\n    S2_GEE_ID= row['S2_GEE_ID']    \n    crs= row['crs'] \n    tiles=[S2_GEE_ID]\n    outfile = os.path.join(outdir, basename+'.tif')\n    if os.path.exists(outfile):\n    # if index < 3035:\n        pass\n    else:\n        region= row['geometry']\n        print(index,S2_GEE_ID)\n        xmin,ymin,xmax,ymax=utm_to_geo(region,crs)\n        regionClip = ee.Geometry.BBox(xmin,ymin,xmax,ymax)\n        ImgTiles=collection_cal(regionClip,tiles,crs)\n        download_img(ImgTiles,basename,outdir)\n        # if index % 50==0:\n        #     print('downloading......No.: '+str(index))\n\n\nIf you want to get the downloaded label data and related forms, you can follow the WeChat public account [45度科研人] and leave me a message！"
  },
  {
    "objectID": "posts/2023-1-18wrsdp_1-4-2/index.html",
    "href": "posts/2023-1-18wrsdp_1-4-2/index.html",
    "title": "基于雷达数据的水体变化监测",
    "section": "",
    "text": "2022年多家媒体报道了鄱阳湖遭遇极端旱情的新闻，印象中上一个关于鄱阳湖的新闻还是2020年的洪灾。2022年短短的几个月时间鄱阳湖到底发生了怎样的变化，哨兵2号光学数据在该区域6-9月份可用数据有限，因此让我们用哨兵1号雷达数据来一探究竟。\n\n\n练习2 基于雷达数据的水体变化监测\n\n1. 方法介绍\n\n哨兵1号卫星是欧空局哥白尼计划（GMES）中的地球观测卫星，由A、B两颗卫星组成，载有C波段合成孔径雷达，可提供连续图像(白天、夜晚和各种天气)。近年来，哨兵1号卫星在海冰测绘，水体变化监测，地面沉降调查，地质灾害隐患识别等应用领域发挥了重要作用。中国是目前拥有哨兵系列卫星用户最多的国家，希望大家在充分利用数据资源做好研究的同时，也要提倡和鼓励欧空局这种开源共享的精神。\n\n\n主要思路是构建SDWI指数，通过阈值法提取单期水体分布范围，在置信度较高的区域进行采样训练随机森林网络，将模型应用多期影像完成水面的提取。\n\nSDWI指数计算公式:\n\\[    SDWI = ln(k × VV × VH) +s \\]\n\n式中\\(VV\\)和\\(VH\\)为极化影像，\\(k\\)为增益系数一般取10即可，\\(s\\)为初始阈值，主要思想是利用\\(vv\\)、\\(vh\\)与\\(k\\)的乘积扩大水体与其他地物之间的差异，初始阈值需根据统计结果进行估计(接近8的值)。\n\n\n大津法也是常用水体提取方法之一，其核心是目标与背景的统计值形成双峰效果才好，这要求统计目标与背景面积尽可能相近，但实际应用中会遇到一些复杂的情况，比如小块水体较多，或目标面积较小等场景应用效果不理想，本案例也列举了类似的情况。\n\n\nOtsu也被称为最大类间方差法，其基本原理是假设检测图像有前景和背景两部分组成，通过统计学方法找出合适的阈值，使得前景和背景组大程度被区分开。\n\n\n最小化类内方差 (定义为两类方差的加权和):\n\n\\[\\sigma^2_w(t)=\\omega_1(t)\\sigma^2_1(t)+\\omega_2(t)\\sigma^2_2(t)\\]\n\n最大化类间方差:\n\n\\[\\sigma^2_b(t)=\\sigma^2-\\sigma^2_w(t)=\\omega_1(t)\\omega_2(t)\\left[\\mu_1(t)-\\mu_2(t)\\right]^2\\]\n\\(\\omega_i\\) 是两个类别的权重. \\(\\mu_i\\) and \\(\\sigma^2_ i\\) 分别是两个类的均值和方差\n\n\n2. 时序数据加载及处理\n加载必要的库函数\n\nimport glob, cv2\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nfrom osgeo import gdal, osr, ogr, gdalconst\nimport os,glob,time,rasterio\nfrom random import shuffle\n\n定义读写数据函数\n\n## 读图像文件\ndef read_img(filename):\n    dataset = gdal.Open(filename)  # 打开文件\n    im_width = dataset.RasterXSize  # 栅格矩阵的列数\n    im_height = dataset.RasterYSize  # 栅格矩阵的行数\n    # im_bands = dataset.RasterCount  # 波段数\n    im_geotrans = dataset.GetGeoTransform()  # 仿射矩阵，左上角像素的大地坐标和像素分辨率\n    im_proj = dataset.GetProjection()  # 地图投影信息，字符串表示\n    im_data = dataset.ReadAsArray(0, 0, im_width, im_height)\n    del dataset   #关闭对象dataset，释放内存\n    # return im_width, im_height, im_proj, im_geotrans, im_data,im_bands\n    return  im_data\n## 计算sdwi，输入是包含vv和vh的2波段影像\ndef cal_sdwi(data):\n    return np.log(data[0]*data[1]*10)\n## 通过文件名分割将8个数据分成4组，利用每组的vv和vh计算sdwi指数，得到4组sdwi\ndef sdwi_combin(n,filelist):\n    data_all=[]\n    for j in range(4):\n        data=[]\n        for i, file in zip(range(n),filelist):\n            [dirname,filename]=os.path.split(file) \n            (shotname,extension) = os.path.splitext(filename)\n            keywd=shotname.split('_')[1:3]  #根据文件名的规律提取分组信息\n            if keywd[0]==str(j):\n                data.append(read_img(file))\n        sdwi=cal_sdwi(np.array(data))\n        data_all.append(sdwi) \n    return np.array(data_all)\n## 给定阈值得到二值化影像\ndef threshold_seg(sdwi,threshold,switch=False):\n    tmp = sdwi.copy()\n    if switch:\n        tmp [tmp<=threshold] = 1\n        tmp [tmp>threshold] = 0\n    else:\n        tmp [tmp<=threshold] = 0\n        tmp [tmp>threshold] = 1\n    return np.uint8(tmp)\n## 将数据归一化至0-10000的16位整型数据\ndef norm(band):\n    band=np.array(band,dtype=np.float32)\n    c = np.min(band)\n    d = np.max(band)       \n    out =  (band - c)  / (d - c)  \n    return (out*10000).astype(np.uint16)\n\n用glob检索已下载的哨兵1号数据\n\ninfolder='./data./SAR/'\nfilelist=glob.glob(infolder+'sentinel*.tif')\nn=len(filelist)\nprint(n)\n\n8\n\n\n通过文件名，将8个数据分成4组，利用每组的vv和vh计算sdwi指数，得到4组sdwi，查看每个sdwi的数值范围\n\nsdwi_all=sdwi_combin(n,filelist)\nsdwi_all=np.nan_to_num(sdwi_all) #用于去掉数据中存在的nan值，具体根据数据情况选用，非必须\nfor file in sdwi_all:\n    print(np.min(file),np.max(file),np.mean(file))\n\nC:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_14988\\4195819107.py:15: RuntimeWarning: invalid value encountered in log\n  return np.log(data[0]*data[1]*10)\n\n\n-6.927577496468431 9.236756509679683 7.749823511700255\n-7.636165228292373 9.345565288117363 7.6522570937515555\n-8.768979387575639 9.068931540870791 7.604430906820028\n-8.02139217881955 9.086487657590412 7.632414847347485\n\n\n\nplt.style.use('ggplot')\nplt.hist(sdwi_all[0].ravel(), bins=200, range=[5,10],density=True, facecolor='#8fca9e', alpha=0.75)\nplt.vlines(8.2, ymin = 0, ymax = 1.6, colors = 'lightblue')\nplt.show()\n\n\n\n\n通过直方图可见数据中水陆分界的位置在8.2左右，因此这里可以直接用阈值法得到分割结果\n\nfirstvv=read_img(filelist[1])\nthreresult=threshold_seg(sdwi_all[0],8.2)\npalette = np.array([ [240, 255, 255],[147, 112, 219]]) #自定义colorbar，用于分类结果的显示，非常实用\ncolor=palette[threresult]\nfig, axes = plt.subplots(1,2,figsize=(20,5))\nplt.subplot(1,3,1),plt.title('SAR_VV'), plt.axis('off')\nplt.imshow(firstvv,cmap='gray',vmin=-30,vmax=0),plt.axis('off')\nplt.subplot(1,3,2),plt.title('SDWI'), plt.axis('off')\nplt.imshow(sdwi_all[0],cmap='gray',vmin=6,vmax=9)\nplt.subplot(1,3,3),plt.title('SDWI_threshold'), plt.axis('off')\nplt.imshow(color)   \n\nC:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_14988\\3003795320.py:6: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n  plt.subplot(1,3,1),plt.title('SAR_VV'), plt.axis('off')\nC:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_14988\\3003795320.py:8: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n  plt.subplot(1,3,2),plt.title('SDWI'), plt.axis('off')\n\n\n<matplotlib.image.AxesImage at 0x1931783f580>\n\n\n\n\n\n\n\n2. 大津法分割\n直方图显示数值呈现出较好的“双峰”特征，对于这种示范区范围并不大的情况下可以采用大津法，从而避免人工设定阈值。\n\nimg=norm(sdwi_all[0].copy()) #将数据转换成0-10000的16位浮点型数据，边用用cv2函数进行计算，虽然cv常用来处理uint8的数据，但此处不建议归一化至0-255会一定程度丢失信息。\nret2,th2 = cv2.threshold(img,0,10000,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\nprint(ret2)\ncolor2=palette[np.uint8(th2/10000)]\n\n9143.0\n\n\n大津法结果与人工分割结果整体较为接近，但阈值选择偏保守，导致碎斑更多，用高斯滤波优化一些效果更好一些\n\nblur = cv2.GaussianBlur(th2,(7,7),0)\ncolor3=palette[np.uint8(blur/10000)]\n\nfig, axes = plt.subplots(1,3,figsize=(20,5))\nplt.subplot(1,3,1),plt.title('SDWI'), plt.axis('off')\nplt.imshow(sdwi_all[0],cmap='gray',vmin=6,vmax=9)\nplt.subplot(1,3,2),plt.title('SDWI_OTSU'), plt.axis('off')\nplt.imshow(color2)\nplt.subplot(1,3,3),plt.title('SDWI_OTSU_OPT'), plt.axis('off')\nplt.imshow(color3)\n\n<matplotlib.image.AxesImage at 0x1931d9b05e0>\n\n\n\n\n\n如果示范区范围大，全局采用大津法并不是很好的选择，因为不能保证呈现出很好的“双峰”特征，虽然可以通过对每个目标进行标记，再利用循环的方式对每一个目标做buffer来实现局部OTSU，但当有很多细碎的小目标存在时，依然难以做到准确快捷。\n本例中，从直方图分析阈值设定在9650是较为合适的，而otsu寻找到的是9154，因而随着鄱阳湖的持续干旱，水面逐渐缩小，裸露出来的河滩逐渐增多，导致直方图中水陆的界限不再分明的呈现“双峰”式，从而使面积较小的水体信息在统计中被淹没了，因而结果是错误的\n\nimg=norm(sdwi_all[-1].copy()) #将数据转换成0-10000的16位浮点型数据，边用用cv2函数进行计算，虽然cv常用来处理uint8的数据，但此处不建议归一化至0-255会一定程度丢失信息。\nret4,th4 = cv2.threshold(img,0,10000,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\nprint(ret4)\ncolor3=palette[np.uint8(th4/10000)]\n\nfig, axes = plt.subplots(1,3,figsize=(20,5))\nplt.subplot(1,3,1),plt.title('SDWI'), plt.axis('off')\nplt.imshow(sdwi_all[-1],cmap='gray',vmin=6,vmax=9)\nplt.subplot(1,3,2),plt.title('SDWI_OTSU'), plt.axis('off')\nplt.imshow(color3)\nplt.subplot(1,3,3),plt.title('SDWI_hist'), plt.axis('off')\nplt.hist(img.ravel(), bins=200, range=[7000,10000],density=True, facecolor='#8fca9e', alpha=0.75)\nplt.vlines(9650, ymin = 0, ymax = 0.0013, colors = 'lightblue'),plt.axis('on')\nplt.show()\n\n9154.0\n\n\n\n\n\n\n\n3. 随机森林分类\n用机器学习的方法的关键在于样本的选择，这里我们通过设定较大的阈值将置信度高的水体信息提取出来作为正样本的采样区，设定相对小的阈值将确定的非水体信息提取出来作为负样本\n\n## 数据叠加\ndef concat(img1,img2):\n    img1=np.expand_dims(img1,axis=-1) #数据维度扩增，axis表示扩增的位置，0为第一位\n    img2=np.expand_dims(img2,axis=-1)\n    return np.concatenate((img1,img2),axis=-1)\n## 阈值分割，注意switch为false是将高值设为1，为true的时候将低值设为1，见threshold_seg说明\ndef thre_seg(dataall,threshold,switch=False):\n    result=[]\n    for img in dataall:\n        norm_img=norm(img.copy()) \n        thimg =threshold_seg(norm_img,threshold,switch=switch) \n        temp=concat(norm_img,thimg)\n        result.append(temp)\n    return np.array(result)\n## 随机裁剪是以随机点为起始点裁剪，切片数可以是无限的且可自定义，后者的好处是可以通过增加算法约束label中某一类的数量来实现精准的样本获取。\ndef random_crop(image,crop_sz):\n    img_sz=image.shape[:2]\n    random_x = np.random.randint(0,img_sz[0]-crop_sz+1) ##生成随机点\n    random_y = np.random.randint(0,img_sz[1]-crop_sz+1)\n    s_img = image[random_x:random_x+crop_sz,random_y:random_y+crop_sz,:] ##以随机点为起始点生成样本框，进行切片\n    return s_img\n## 随机裁剪n_patch表示每个数据裁多少块，n为最多循环多少次，如果n=5000，在5000次内依然没有检索到足够的数据实际结果将小于n_patch\ndef data_crop(imagearray,crop_sz,n_patch=300,n=5000):   \n    n_file=imagearray.shape[0]\n    data_all = []\n    for i in np.arange(n_file):\n        data = []\n        x=0\n        for j in np.arange(n):\n            image=random_crop(imagearray[i,:,:,:],crop_sz)\n            if np.mean(image[:,:,-1])==1:  #这里通过设定均值都为1才提取，表示32×32方框内均为水体\n                data.append(image)\n                x=x+1\n                if x % 50 ==0:\n                    print(\"processing....patch:\"+str(i)+\"...No.:\"+str(x))\n                if x == n_patch:\n                    break\n        data=np.array(data)\n        if i == 0:\n            data_all = data\n        else:\n            data_all=np.concatenate((data_all, data), axis = 0)\n    return data_all\n## 随机打散数据\ndef suffle_data(imgd):\n    index = [i for i in range(len(imgd))]\n    shuffle(index)\n    newimg = imgd[index, :, :, :]\n    return newimg\n\n分别对确定为水体和确定为非水体的区域用传统阈值分割法进行提取，下面阈值根据前面直方图大致估计得出。 做完回看的时候这里的阈值设定还可以进一步优化，9月份裸露的大面积河滩可以作为苦难样本单独采样，训练样本会更准确\n\nthredata1=thre_seg(sdwi_all,9650)\nthredata2=thre_seg(sdwi_all,9400,switch=True)\n\n(4, 9127, 10946, 2)\n\n\n分别在两个区域内容随机采样，这里采用32×32，并没有实际意义，没用像素采样的原因是整个随机采样程序与可以用在深度学习的切片采样上，一劳永逸\n\nrandom1_pic=data_crop(thredata,32)\nprint(random1_pic.shape)\nrandom2_pic=data_crop(thredata2,32,n_patch=150)\nrandom2_pic[:,:,:,1]=0 #这里因为randomw2_pic中的真值还是1，但它是对非水体区域采集的负样本，所以重新设置了它的真值为0\nprint(random2_pic.shape)\n\nprocessing....patch:0...No.:50\nprocessing....patch:0...No.:100\nprocessing....patch:0...No.:150\nprocessing....patch:1...No.:50\nprocessing....patch:1...No.:100\nprocessing....patch:1...No.:150\nprocessing....patch:2...No.:50\nprocessing....patch:2...No.:100\nprocessing....patch:2...No.:150\nprocessing....patch:3...No.:50\nprocessing....patch:3...No.:100\nprocessing....patch:3...No.:150\n(600, 32, 32, 2)\n\n\n训练样本合并随机处理\n\ntraindata=np.concatenate((random1_pic,random2_pic),axis=0)\ntraindata=suffle_data(traindata)\n\n\n## 预测输入数据预处理\ndef data_process(image):\n    norm_img=norm(image.copy())\n    resize=np.expand_dims(norm_img.ravel(),axis=-1)\n    return resize \n## 预测结果reshape\ndef data_reshape(result,image):\n    return result.reshape(image.shape[0],image.shape[1])\n## 循环推理\ndef prediction(sdwi_all):\n    pred=[]\n    for img in sdwi_all:\n        result=model.predict(data_process(img))\n        pred_r=data_reshape(result,img)\n        pred.append(pred_r)\n    return np.array(pred)\n\n载入随机森林算法，喂训练数据进行训练，这里也可以采用其他机器学习模型，选择随机森林的原因是其速度和精度二者较为平衡，是性价比很高的ML算法\n\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(n_estimators=100, bootstrap = True,max_features = 'sqrt')\nmodel.fit(np.expand_dims(traindata[:,:,:,0].ravel(),axis=-1), np.expand_dims(traindata[:,:,:,1].ravel(),axis=-1))\n\nC:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_14988\\445534785.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  model.fit(np.expand_dims(traindata[:,:,:,0].ravel(),axis=-1), np.expand_dims(traindata[:,:,:,1].ravel(),axis=-1))\n\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier()\n\n\n\nresult=prediction(sdwi_all)\n\n\n\n3. 水面变化可视化\n对提取的鄱阳湖水面信息进行可视化，可见从6月到9月，受旱灾影响，水面持续下降（图中右下角未变化的区域应该不属于鄱阳湖）\n\ndef show_4_images(image,title,n=1):\n    for k in range(n):\n        fig=plt.figure(figsize=(20,5))\n        for i in range(4):\n            plt.subplot(k+1,4,k*4+i+1)\n            plt.imshow((image[k*4+i,:,:,:3]))\n            plt.title(title[i])\n            plt.axis('off')\n        plt.show()\n\n\nresult_show=palette[np.uint8(result)]\nshow_4_images(result_show,['2022-6','2022-7','2022-8','2022-9'])\n\n\n\n\n最后推荐大家多用GEE来解决问题，上面案例从下载到处理可能需要大半天的时间，而gee做一个全年的水面提取，也就几分钟的事儿，下面是鄱阳湖2022年12个月累计的变化，颜色越浅代表一年内水体存在的时间越短，深蓝色代表永久性水体，这个结果能更直观的看到2022年鄱阳湖的变化情况\n\n\n想了解更多请关注[45度科研人]公众号，欢迎给我留言！"
  },
  {
    "objectID": "posts/2020-2-10wrsdp1-5-2/index.html",
    "href": "posts/2020-2-10wrsdp1-5-2/index.html",
    "title": "基于时间序列遥感的水稻类型识别-P2",
    "section": "",
    "text": "上一篇通过GEE实现对双季稻和单季稻的分类，从中不难看出GEE具有强大的遥感大数据处理能力及高度集成化的功能接口，非常适用于大区域遥感应用。然而，对于示范区域较小且需要实现更多定制化功能的情况下，依然需要了解基于Python如何实现，尤其是针对shape文件的一些交互操作依然是初学者的难点。本文基于与上一篇相同的数据利用Python复现单季水稻和双季水稻的分类过程。\n\n\n需要说明的是分享案例旨在交流方法的编程实现，应用精度主要取决于样本，而本例中并未对样本进行严格筛选。\n\n\n基于时间序列遥感的水稻类型识别-P2\n\n地物的光谱信息是遥感数据的重要特征，对遥感光谱信息的利用经历了从全色影像到多光谱、高光谱再到时间序列的发展历程。近年来，随着卫星遥感的发展和历史数据的积累，获取了大量的重复观测数据。长时序的遥感数据包含光谱维、时间维和空间维四个维度的信息，能够在一定程度上避免“同谱异物”、“同物异谱”的现象，在地物分类、变化检测等方面有广泛应用。本案例基于2022年度的哨兵二号长时间序列数据构建NDVI时序立方体，利用随机森林算法实现对研究区双季水稻和单季水稻的分类提取。\n\n\n1. 时序光谱数据读取\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nfrom osgeo import gdal, osr, ogr, gdalconst\nimport os,glob,time\nfrom utils import * #之前分享过的function都放到了utils里，此文档不在展示，文末回复关键字获取\nimg_arr, im_geotrans,im_proj,band_name =read_Image('./data/ndvi_combine.tif')\nn=len(band_name)\nimg_arr.shape\n(28, 11076, 12541)\n显示时序影像\nnum = img_arr.shape[0]\nplt.figure(figsize=(30,(num//5 + 1)*6))\nfor i in range(num):\n    ax = plt.subplot((num//5 + 1),5,i+1)\n    ax.set_title(band_name[i])\n    ax.imshow(img_arr[i,:,:])\nplt.show()\n\n\n\n2. 光谱特征分析\n研究区双季稻生长期安排因受气候条件的制约相对固定, 大致从每年3 月下旬到10月下旬, 而单季稻生长期安排相对自由, 且全生育期略长, 一般从5月中上旬到10月上旬。下图展示了年内不同熟制水稻武侯历，A为早稻、B为晚稻、C为单季稻，参考1、2。\n\n图中绿色部分为水稻生长旺盛期，对应DNVI较高，因此以9月中旬，4月下旬及10月构建假彩色影像可大致区分单双季水稻。\n# for i in range(28):\n#     print(str(i)+'_'+band_name[i])\n## 坐标转图面坐标\ndef geo2imagexy( x, y,trans):\n    '''\n    根据GDAL的六 参数模型将给定的投影或地理坐标转为影像图上坐标（行列号）\n    :param dataset: GDAL地理数据\n    :param x: 投影或地理坐标x\n    :param y: 投影或地理坐标y\n    :return: 影坐标或地理坐标(x, y)对应的影像图上行列号(col,row)\n    '''\n    a = np.array([[trans[1], trans[2]], [trans[4], trans[5]]])\n    b = np.array([x - trans[0], y - trans[3]])\n    out = np.linalg.solve(a, b)\n    return out[1], out[0] \n## 显示折线图\ndef show_broken_line(data,label,mode):\n    data = np.squeeze(np.array(data))\n    x = np.arange(len(data))\n    plt.style.use('ggplot')\n    plt.figure(figsize=(12,5))\n    plt.plot(x, data,linestyle=\":\",color='darkviolet',linewidth = '2' )#, label=\"1\", linestyle=\":\")\n    plt.xticks(x,labels=label,rotation=60)\n    plt.title(mode+\"cropping\")\n    plt.ylabel(\"NDVI\")\n    plt.show()\nquicklook=img_arr[[19,5,24]]\nWrite_Tiff(quicklook,im_geotrans,im_proj, './data/quickimg.tif')\nplt.figure(figsize=(10,6))\nplt.imshow(quicklook.transpose(1,2,0))\nx1,y1=(116.40917424,28.68150022)\nx2,y2=(116.27280998,28.71303109)\npx1,py1=geo2imagexy(x1,y1,im_geotrans)\npx2,py2=geo2imagexy(x2,y2,im_geotrans)\n# print(px1,py1,px2,py2)\nplt.plot(px1,py1,'*r')\nplt.plot(px2,py2,'*r')\nplt.show()\n\n\n展示在给定坐标位置的情况下，展现典型单季和双季水稻的时序曲线，双季呈现出双峰态，由于6月份数据受云影响较大，有效数据较少导致第一个峰略窄，单季是4-8月间的单峰态。\n\nndvi_line1=img_arr[:,round(px1),round(py1)]\nndvi_line2=img_arr[:,round(px2),round(py2)]\n\nshow_broken_line(ndvi_line1,band_name,'Multiple-')\nshow_broken_line(ndvi_line2,band_name,'Single-')\n\n\n\n\n3. 样本构建及机器学习推理\n\n3.1 矢量内生成随机点\n\n借助Arcgis或ENVI工具对照前面的假彩色影像来圈定采样区域，可以利用矢量勾绘功能圈定单季稻、双季稻以及其他地物类范围，再分别在各类图斑内生成随机采样点用于训练机器学习分类器；\nclasspoint.shp共有30000个采样点，每类分别有10000个采样点，通过属性’label’作为类别标签，显示结果如下图, 图中黑色点为双季稻样本，红色点为单季稻样本，蓝色点为其它地类样本。\n\nimport rasterio,rasterio.plot\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport random,glob\nfrom shapely.geometry import Point,Polygon\nfrom geopandas import GeoSeries,GeoDataFrame\nimport pandas as pd\n\n\ndef random_points_in_polygon(number, polygon):\n    points = []\n    min_x, min_y, max_x, max_y = polygon.bounds\n    i= 0\n    while i < number:\n        point = Point(random.uniform(min_x, max_x), random.uniform(min_y, max_y))\n        if polygon.contains(point):\n            points.append(point)\n            i += 1\n    return points  # returns list of shapely point\n\ndef point_generator(polygon,sampleno):\n    geodata = gpd.read_file(polygon)\n    points = random_points_in_polygon(sampleno, geodata.iloc[0].geometry)\n    # Plot the list of points\n    xs = [point.x for point in points]\n    ys = [point.y for point in points]\n    # for i, point in enumerate(points):\n    #     print(\"Point {}: ({},{})\".format(str(i+1), point.x, point.y))\n    return xs,ys,geodata\n\ndef gdf_creat(xs,ys,label):\n    df = pd.DataFrame()\n    df['points'] = list(zip(xs,ys))\n    df['points'] = df['points'].apply(Point)\n    gdf_points = gpd.GeoDataFrame(df, geometry='points', crs=\"EPSG:4326\")\n    gdf_points1=GeoDataFrame({'label' : np.zeros(df.size,dtype=np.int8)+label}, geometry=df['points'])\n    return gdf_points1\nshp=glob.glob('./data/shp/*.shp')\nlabel=[0,1,2]\ncolorc=['yellow','blue','green']\ncolorp=['green','red','blue']\n\nfig, ax = plt.subplots(figsize=(10,10))\nfig.suptitle('Random Sampling', fontsize=32)\nax.grid(False)\nimg=rasterio.open(\"./data/quickimg.tif\")\np1 =rasterio.plot.show(img, ax=ax)\nfor index,i,j in zip(range(len(shp)),shp,label):\n    xs, ys,geodata=point_generator(i,1000)\n    point=gdf_creat(xs,ys,j)\n    geodata.plot(ax=ax,color=colorc[index],alpha=0.5,edgecolor='black')\n    ax.scatter(xs, ys,c=colorp[index],s=2)\n    out_path='./data/point/point_'+str(index)+'.shp'\n    point.to_file(out_path)\n\n\n\n3.2 样本集构建\n获取随机点的经纬度，以此为媒介获取影像的像元值，将同一坐标对应的像元值与lable合并构成训练样本\n\n# 得到每一个点要素的经纬度坐标，以及label值\ndef Read_Point_Location(shppoint,img,ifshpfile=False,Field = \"label\"):\n    if ifshpfile:\n        shppoint=gpd.read_file(shppoint)\n    else:\n        Sample_array = np.zeros((shppoint[Field].size,3),dtype=np.float32)\n        Sample_array[:,0] = np.array(shppoint.geometry.x)\n        Sample_array[:,1] = np.array(shppoint.geometry.y)\n        Sample_array[:,2] = np.array(shppoint[Field])\n    return Sample_array\n\n# 根据点的坐标提取影像中对应的栅格值，类似于arcmap中“值提取至点”功能\ndef Ectract_Value_to_Point(geotrans,image,Sample_array):\n    newarray=np.zeros((Sample_array.shape[0],image.shape[0]))\n    Left = geotrans[0]   #图像左上角经度\n    Up = geotrans[3]     #图像左上角纬度\n    long_res = geotrans[1]  \n    lat_res = geotrans[5]\n    for i in range(Sample_array.shape[0]):\n        long = Sample_array[i,0]\n        lat = Sample_array[i,1]\n        # 得到该坐标与图像左上角坐标的相对位置\n        col_offset = int((long - Left)/long_res)\n        row_offset = int((lat - Up)/lat_res)\n        # if row_offset >= image.shape[1] or col_offset >= image.shape[2]:\n        #     break\n        for j in range(image.shape[0]):\n            newarray[i,j] = image[j,row_offset,col_offset]\n    return newarray\npoints=glob.glob('./data/point/*.shp')\npoint0=gpd.read_file(points[0])\npoint1=gpd.read_file(points[1])\npoint2=gpd.read_file(points[2])\npointall=pd.concat([point0,point1,point2])\n# pointall.to_file('./data/all_point.shp')\n# 得到每一个点要素的经纬度坐标，以及label值\npoint_array=Read_Point_Location(pointall,img_arr)\n# 根据点的坐标提取影像中对应的栅格值，\nextract_array = Ectract_Value_to_Point(im_geotrans,img_arr,point_array)\ntraining=np.concatenate((extract_array,point_array[:,-1:]),axis=-1)\nextract_array.shape,point_array.shape,training.shape\n((1500, 28), (1500, 3), (1500, 29))\n值得注意的是圈定样本只是大概的区域，其中会混入目标地物引起错分；因此，建议此处获得样本再通过设定一些阈值筛选一遍，过滤掉错误样本。方法相对简单，再次不再展示。\n\n\n\n4. 模型构建及推理\n划分训练及测试样本集，训练随机森林模型\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\n    \nxtrain,xtest,ytrain,ytest=train_test_split(training[:,:-1],training[:,-1],test_size=0.2,random_state=42)\n\n# 设置随机森林模型中的树为100\nclf = RandomForestClassifier(n_estimators=50,bootstrap=True, max_features='sqrt')\nclf.fit(xtrain,ytrain)\n\n\n\nRandomForestClassifier(n_estimators=50)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\nRandomForestClassifier\n\nRandomForestClassifier(n_estimators=50)\n\n\n\n\n\ndef acc_assess(matrix):  \n    TP,FP,FN,TN = matrix[0,0],matrix[0,1],matrix[1,0],matrix[1,1]\n    precision = TP/(TP+FP)\n    recall = TP/(TP+FN)\n    f1 = 2 * precision * recall / (precision + recall)\n    return f1\ny_pred = clf.predict(xtest)\nmatrix = confusion_matrix(ytest, y_pred)\nF1= acc_assess(matrix)\nprint(\"F1：%0.4f\" % F1)\nF1：0.9557\npred_arr=img_arr.reshape(img_arr.shape[0],img_arr.shape[1]*img_arr.shape[2])\npred_arr=pred_arr.swapaxes(0,1) \npred_arr.shape\n(138904116, 28)\n# 模型预测\npred = clf.predict(pred_arr)\npred = pred.reshape(img_arr.shape[1], img_arr.shape[2])\npred = pred.astype(np.uint8)\npalette = np.array([ [255,250,205],[60,179,113],[65,105,225]]) #自定义colorbar，用于分类结果的显示，非常实用\ncolor=palette[pred]\nfig, ax = plt.subplots(1,2,figsize=(12,10))\nplt.subplot(1,2,1),plt.title('NDVI'), plt.grid(False)\nplt.imshow(quicklook.transpose(1,2,0))\nplt.subplot(1,2,2),plt.title('Prediction'), plt.grid(False)\nplt.imshow(color)\n<matplotlib.image.AxesImage at 0x1e51e9013a0>\n\nWrite_Tiff(pred,im_geotrans,im_proj, './data/prediction.tif')\n\n想了解更多请关注[45度科研人]公众号，欢迎给我留言！"
  },
  {
    "objectID": "posts/2020-2-10wrsdp1-5-2/基于时间序列遥感的水稻类型识别-P2.html",
    "href": "posts/2020-2-10wrsdp1-5-2/基于时间序列遥感的水稻类型识别-P2.html",
    "title": "Junchuan Yu",
    "section": "",
    "text": "上一篇通过GEE实现对双季稻和单季稻的分类，从中不难看出GEE具有强大的遥感大数据处理能力及高度集成化的功能接口，非常适用于大区域遥感应用。然而，对于示范区域较小且需要实现更多定制化功能的情况下，依然需要了解基于Python如何实现，尤其是针对shape文件的一些交互操作依然是初学者的难点。本文基于与上一篇相同的数据利用Python复现单季水稻和双季水稻的分类过程，需要说明的是分享案例旨在交流方法的编程实现，应用精度主要取决于样本，而本例中并未对样本进行严格筛选。\n\n\n基于时间序列遥感的水稻类型识别-P2\n\n地物的光谱信息是遥感数据的重要特征，对遥感光谱信息的利用经历了从全色影像到多光谱、高光谱再到时间序列的发展历程。近年来，随着卫星遥感的发展和历史数据的积累，获取了大量的重复观测数据。长时序的遥感数据包含光谱维、时间维和空间维四个维度的信息，能够在一定程度上避免“同谱异物”、“同物异谱”的现象，在地物分类、变化检测等方面有广泛应用。本案例基于2022年度的哨兵二号长时间序列数据构建NDVI时序立方体，利用随机森林算法实现对研究区双季水稻和单季水稻的分类提取。\n\n\n1. 时序光谱数据读取\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nfrom osgeo import gdal, osr, ogr, gdalconst\nimport os,glob,time\nfrom utils import * #之前分享过的function都放到了utils里，此文档不在展示，文末回复关键字获取\n\n\nimg_arr, im_geotrans,im_proj,band_name =read_Image('./data/ndvi_combine.tif')\nn=len(band_name)\nimg_arr.shape\n\n(28, 11076, 12541)\n\n\n显示时序影像\n\nnum = img_arr.shape[0]\nplt.figure(figsize=(30,(num//5 + 1)*6))\nfor i in range(num):\n    ax = plt.subplot((num//5 + 1),5,i+1)\n    ax.set_title(band_name[i])\n    ax.imshow(img_arr[i,:,:])\nplt.show()\n\n\n\n\n\n\n2. 光谱特征分析\n研究区双季稻生长期安排因受气候条件的制约相对固定, 大致从每年3 月下旬到10月下旬, 而单季稻生长期安排相对自由, 且全生育期略长, 一般从5月中上旬到10月上旬。下图展示了年内不同熟制水稻武侯历，A为早稻、B为晚稻、C为单季稻，参考1、2。\n\n图中绿色部分为水稻生长旺盛期，对应DNVI较高，因此以9月中旬，4月下旬及10月构建假彩色影像可大致区分单双季水稻。\n\n# for i in range(28):\n#     print(str(i)+'_'+band_name[i])\n\n\n## 坐标转图面坐标\ndef geo2imagexy( x, y,trans):\n    '''\n    根据GDAL的六 参数模型将给定的投影或地理坐标转为影像图上坐标（行列号）\n    :param dataset: GDAL地理数据\n    :param x: 投影或地理坐标x\n    :param y: 投影或地理坐标y\n    :return: 影坐标或地理坐标(x, y)对应的影像图上行列号(col,row)\n    '''\n    a = np.array([[trans[1], trans[2]], [trans[4], trans[5]]])\n    b = np.array([x - trans[0], y - trans[3]])\n    out = np.linalg.solve(a, b)\n    return out[1], out[0] \n## 显示折线图\ndef show_broken_line(data,label,mode):\n    data = np.squeeze(np.array(data))\n    x = np.arange(len(data))\n    plt.style.use('ggplot')\n    plt.figure(figsize=(12,5))\n    plt.plot(x, data,linestyle=\":\",color='darkviolet',linewidth = '2' )#, label=\"1\", linestyle=\":\")\n    plt.xticks(x,labels=label,rotation=60)\n    plt.title(mode+\"cropping\")\n    plt.ylabel(\"NDVI\")\n    plt.show()\n\n\nquicklook=img_arr[[19,5,24]]\nWrite_Tiff(quicklook,im_geotrans,im_proj, './data/quickimg.tif')\n\n\nplt.figure(figsize=(10,6))\nplt.imshow(quicklook.transpose(1,2,0))\nx1,y1=(116.40917424,28.68150022)\nx2,y2=(116.27280998,28.71303109)\npx1,py1=geo2imagexy(x1,y1,im_geotrans)\npx2,py2=geo2imagexy(x2,y2,im_geotrans)\n# print(px1,py1,px2,py2)\nplt.plot(px1,py1,'*r')\nplt.plot(px2,py2,'*r')\nplt.show()\n\n\n\n\n\n展示在给定坐标位置的情况下，展现典型单季和双季水稻的时序曲线，双季呈现出双峰态，由于6月份数据受云影响较大，有效数据较少导致第一个峰略窄，单季是4-8月间的单峰态。\n\n\nndvi_line1=img_arr[:,round(px1),round(py1)]\nndvi_line2=img_arr[:,round(px2),round(py2)]\n\nshow_broken_line(ndvi_line1,band_name,'Multiple-')\nshow_broken_line(ndvi_line2,band_name,'Single-')\n\n\n\n\n\n\n\n\n\n3. 样本构建及机器学习推理\n\n3.1 矢量内生成随机点\n\n借助Arcgis或ENVI工具对照前面的假彩色影像来圈定采样区域，可以利用矢量勾绘功能圈定单季稻、双季稻以及其他地物类范围，再分别在各类图斑内生成随机采样点用于训练机器学习分类器；\nclasspoint.shp共有30000个采样点，每类分别有10000个采样点，通过属性’label’作为类别标签，显示结果如下图, 图中黑色点为双季稻样本，红色点为单季稻样本，蓝色点为其它地类样本。\n\n\nimport rasterio,rasterio.plot\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport random,glob\nfrom shapely.geometry import Point,Polygon\nfrom geopandas import GeoSeries,GeoDataFrame\nimport pandas as pd\n\n\ndef random_points_in_polygon(number, polygon):\n    points = []\n    min_x, min_y, max_x, max_y = polygon.bounds\n    i= 0\n    while i < number:\n        point = Point(random.uniform(min_x, max_x), random.uniform(min_y, max_y))\n        if polygon.contains(point):\n            points.append(point)\n            i += 1\n    return points  # returns list of shapely point\n\ndef point_generator(polygon,sampleno):\n    geodata = gpd.read_file(polygon)\n    points = random_points_in_polygon(sampleno, geodata.iloc[0].geometry)\n    # Plot the list of points\n    xs = [point.x for point in points]\n    ys = [point.y for point in points]\n    # for i, point in enumerate(points):\n    #     print(\"Point {}: ({},{})\".format(str(i+1), point.x, point.y))\n    return xs,ys,geodata\n\ndef gdf_creat(xs,ys,label):\n    df = pd.DataFrame()\n    df['points'] = list(zip(xs,ys))\n    df['points'] = df['points'].apply(Point)\n    gdf_points = gpd.GeoDataFrame(df, geometry='points', crs=\"EPSG:4326\")\n    gdf_points1=GeoDataFrame({'label' : np.zeros(df.size,dtype=np.int8)+label}, geometry=df['points'])\n    return gdf_points1\n\n\nshp=glob.glob('./data/shp/*.shp')\nlabel=[0,1,2]\ncolorc=['yellow','blue','green']\ncolorp=['green','red','blue']\n\nfig, ax = plt.subplots(figsize=(10,10))\nfig.suptitle('Random Sampling', fontsize=32)\nax.grid(False)\nimg=rasterio.open(\"./data/quickimg.tif\")\np1 =rasterio.plot.show(img, ax=ax)\nfor index,i,j in zip(range(len(shp)),shp,label):\n    xs, ys,geodata=point_generator(i,1000)\n    point=gdf_creat(xs,ys,j)\n    geodata.plot(ax=ax,color=colorc[index],alpha=0.5,edgecolor='black')\n    ax.scatter(xs, ys,c=colorp[index],s=2)\n    out_path='./data/point/point_'+str(index)+'.shp'\n    point.to_file(out_path)\n\n\n\n\n\n\n3.2 样本集构建\n获取随机点的经纬度，以此为媒介获取影像的像元值，将同一坐标对应的像元值与lable合并构成训练样本\n\n\n# 得到每一个点要素的经纬度坐标，以及label值\ndef Read_Point_Location(shppoint,img,ifshpfile=False,Field = \"label\"):\n    if ifshpfile:\n        shppoint=gpd.read_file(shppoint)\n    else:\n        Sample_array = np.zeros((shppoint[Field].size,3),dtype=np.float32)\n        Sample_array[:,0] = np.array(shppoint.geometry.x)\n        Sample_array[:,1] = np.array(shppoint.geometry.y)\n        Sample_array[:,2] = np.array(shppoint[Field])\n    return Sample_array\n\n# 根据点的坐标提取影像中对应的栅格值，类似于arcmap中“值提取至点”功能\ndef Ectract_Value_to_Point(geotrans,image,Sample_array):\n    newarray=np.zeros((Sample_array.shape[0],image.shape[0]))\n    Left = geotrans[0]   #图像左上角经度\n    Up = geotrans[3]     #图像左上角纬度\n    long_res = geotrans[1]  \n    lat_res = geotrans[5]\n    for i in range(Sample_array.shape[0]):\n        long = Sample_array[i,0]\n        lat = Sample_array[i,1]\n        # 得到该坐标与图像左上角坐标的相对位置\n        col_offset = int((long - Left)/long_res)\n        row_offset = int((lat - Up)/lat_res)\n        # if row_offset >= image.shape[1] or col_offset >= image.shape[2]:\n        #     break\n        for j in range(image.shape[0]):\n            newarray[i,j] = image[j,row_offset,col_offset]\n    return newarray\n\n\npoints=glob.glob('./data/point/*.shp')\npoint0=gpd.read_file(points[0])\npoint1=gpd.read_file(points[1])\npoint2=gpd.read_file(points[2])\npointall=pd.concat([point0,point1,point2])\n# pointall.to_file('./data/all_point.shp')\n\n\n# 得到每一个点要素的经纬度坐标，以及label值\npoint_array=Read_Point_Location(pointall,img_arr)\n# 根据点的坐标提取影像中对应的栅格值，\nextract_array = Ectract_Value_to_Point(im_geotrans,img_arr,point_array)\ntraining=np.concatenate((extract_array,point_array[:,-1:]),axis=-1)\nextract_array.shape,point_array.shape,training.shape\n\n((1500, 28), (1500, 3), (1500, 29))\n\n\n值得注意的是圈定样本只是大概的区域，其中会混入目标地物引起错分；因此，建议此处获得样本再通过设定一些阈值筛选一遍，过滤掉错误样本。方法相对简单，再次不再展示。\n\n\n\n4. 模型构建及推理\n划分训练及测试样本集，训练随机森林模型\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\n    \nxtrain,xtest,ytrain,ytest=train_test_split(training[:,:-1],training[:,-1],test_size=0.2,random_state=42)\n\n# 设置随机森林模型中的树为100\nclf = RandomForestClassifier(n_estimators=50,bootstrap=True, max_features='sqrt')\nclf.fit(xtrain,ytrain)\n\nRandomForestClassifier(n_estimators=50)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(n_estimators=50)\n\n\n\ndef acc_assess(matrix):  \n    TP,FP,FN,TN = matrix[0,0],matrix[0,1],matrix[1,0],matrix[1,1]\n    precision = TP/(TP+FP)\n    recall = TP/(TP+FN)\n    f1 = 2 * precision * recall / (precision + recall)\n    return f1\ny_pred = clf.predict(xtest)\nmatrix = confusion_matrix(ytest, y_pred)\nF1= acc_assess(matrix)\nprint(\"F1：%0.4f\" % F1)\n\nF1：0.9557\n\n\n\npred_arr=img_arr.reshape(img_arr.shape[0],img_arr.shape[1]*img_arr.shape[2])\npred_arr=pred_arr.swapaxes(0,1) \npred_arr.shape\n\n(138904116, 28)\n\n\n\n# 模型预测\npred = clf.predict(pred_arr)\npred = pred.reshape(img_arr.shape[1], img_arr.shape[2])\npred = pred.astype(np.uint8)\n\n\npalette = np.array([ [255,250,205],[60,179,113],[65,105,225]]) #自定义colorbar，用于分类结果的显示，非常实用\ncolor=palette[pred]\nfig, ax = plt.subplots(1,2,figsize=(12,10))\nplt.subplot(1,2,1),plt.title('NDVI'), plt.grid(False)\nplt.imshow(quicklook.transpose(1,2,0))\nplt.subplot(1,2,2),plt.title('Prediction'), plt.grid(False)\nplt.imshow(color)\n\n<matplotlib.image.AxesImage at 0x1e51e9013a0>\n\n\n\n\n\n\nWrite_Tiff(pred,im_geotrans,im_proj, './data/prediction.tif')"
  },
  {
    "objectID": "posts/2023-2-6wrsdp_1-5-1/index.html",
    "href": "posts/2023-2-6wrsdp_1-5-1/index.html",
    "title": "基于时间序列遥感的水稻类型识别-P1",
    "section": "",
    "text": "Goole Earth Engine (GEE)，GEE相信大家都有一定了解，这是一款谷歌推出的云计算平台，提供了大量开源的遥感数据，支持在线用JAVA语言或调用Pyhton API接口实现遥感应用（需要科学上网）。在[上一篇]分享中,我们利用GEE结合哨兵1号雷达数据实现对水体的变化监测，本篇我们继续探索GEE在时序遥感中的应用。下一篇将分享用相同的数据如何在本地用Python实现同样的操作。想要进一步了解GEE的伙伴这里提供了一些资料：\n\n\nGEE安装与配置方法\nGEE主页\nGEE官方帮助\nGEEMAP主页\nGEE知乎大V无形的风\n\n\n基于时间序列遥感的水稻类型识别-P1\n\n地物的光谱信息是遥感数据的重要特征，对遥感光谱信息的利用经历了从全色影像到多光谱、高光谱再到时间序列的发展历程。近年来，随着卫星遥感的发展和历史数据的积累，获取了大量的重复观测数据。长时序的遥感数据包含光谱维、时间维和空间维四个维度的信息，能够在一定程度上避免“同谱异物”、“同物异谱”的现象，在地物分类、变化检测等方面有广泛应用。本案例利用GEE平台获取2022年度的哨兵二号长时间序列数据构建NDVI时序立方体，利用随机森林算法实现对研究区双季水稻和单季水稻的分类提取。\n\n\n1. 时序立方体构建\nimport geemap #gee的安装方法见上面链接\nimport ee\nimport geemap.colormaps as cm\n# ee.Authenticate() #第一次运行不需要注释这句，授权过之后，如果重启kernel这句可以注释掉，不必进行授权步骤也可以，如果报错需要重新授权。\ngeemap.set_proxy(port=25378) #这里请填入自己计算机的端口号\nee.Initialize()\nfrom matplotlib import pyplot as plt\nimport numpy as np\n## 定义示范区范围\nregion=ee.Geometry.Polygon([[[115.97367, 28.92437],  [117.10013, 28.92824],  [117.09920, 27.93711],   [115.98318, 27.93339],   [115.97367, 28.92437]]])\nMap = geemap.Map()\n# Map.centerObject(region,zoom=9)\nMap.addLayer(region, {}, 'region')\nMap\nMap(center=[20, 0], controls=(WidgetControl(options=['position', 'transparent_bg'], widget=HBox(children=(Togg…\n\n定义一些必要的函数\n## 哨兵2利用qa波段进行去云的方法\ndef rmCloudByQA(image):\n    qa = image.select('QA60')\n    cloudBitMask = 1 << 10\n    cirrusBitMask = 1 << 11 \n    mask =qa.bitwiseAnd(cloudBitMask).eq(0)and(qa.bitwiseAnd(cirrusBitMask).eq(0))\n    return image.updateMask(mask).toFloat().divide(1e4).copyProperties(image, [\"system:time_start\"])\n## 计算NDVI\ndef createNDVI(image):\n  ndvi = image.normalizedDifference([\"B8\",\"B4\"]).rename('NDVI').add(ee.Number(1)).divide(ee.Number(2.0)).multiply(ee.Number(255)).toByte()\n  return image.addBands(ndvi)\n## 将日期作为属性添加到新数据中\ndef addDate(image):\n    date = ee.Date(image.get('system:time_start')).format('yyyyMMdd')\n    return image.set(\"image_date\",date) # set的参数为字典\n## 构建3月-12月的NDVI时序数据集，该区域检索到28景数据\ns2Img = ee.ImageCollection('COPERNICUS/S2_SR')\nNDVI = s2Img.filterDate('2022-03-01', '2022-11-30').filterBounds(region.centroid()).filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 64)).map(rmCloudByQA).map(addDate).map(createNDVI).select('NDVI').sort(\"image_date\")\nNDVI=NDVI.filterMetadata('image_date','not_equals','20220730') #剔除一些受云亮影响数据缺失较多的数据\nNDVI=NDVI.filterMetadata('image_date','not_equals','20220923')\n\nrgb=s2Img.filterDate('2022-03-01', '2022-11-30').filterBounds(region.centroid()).filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30)).map(rmCloudByQA).mosaic()\nprint(NDVI.size().getInfo())\n28\n##将包含28影像的collection转变为具有28个波段的影像\nbandname=NDVI.aggregate_array('image_date')\nsingleimg=NDVI.toBands().rename(bandname) \n## 根据单季稻和双季稻的季节性特点，选择9/4/10几个月份的NDVI作为RGB显示，可大致看出不同时段水稻的分布情况，见后面分析\nvis = {'min': 128,\n       'max': 230,\n       'gamma':2,\n       'bands': ['20220918', '20220421', '20221023']}\nMap.addLayer(singleimg, vis, \"rgb\")\nMap.addLayer(rgb, {'min': 0, 'max':0.4, 'gamma':1,'bands': ['B4', 'B3', 'B2']}, \"s2\")\nMap\nMap(center=[20, 0], controls=(WidgetControl(options=['position', 'transparent_bg'], widget=HBox(children=(Togg…\n\n\n2. 光谱特征分析\n研究区双季稻生长期安排因受气候条件的制约相对固定, 大致从每年3 月下旬到10月下旬, 而单季稻生长期安排相对自由, 且全生育期略长, 一般从5月中上旬到10月上旬。下图展示了年内不同熟制水稻武侯历，A为早稻、B为晚稻、C为单季稻，参考1、2。\n\n图中绿色部分为水稻生长旺盛期，对应DNVI较高，因此以9月中旬，4月下旬及10月构建假彩色影像可大致区分单双季水稻。 \n\n借助geemap的plotting交互功能，查看时序光谱，以下是单季和双季水稻的时序曲线，双季呈现出双峰态，由于6月份数据受云影响较大，有效数据较少导致第一个峰略窄，单季是4-8月间的单峰态。\n\n# 显示折线图\ndef show_broken_line(data,label,mode):\n    data = np.squeeze(np.array(data))\n    x = np.arange(len(data))\n    plt.style.use('ggplot')\n    plt.figure(figsize=(12,5))\n    plt.plot(x, data,linestyle=\":\",color='darkviolet',linewidth = '2' )#, label=\"1\", linestyle=\":\")\n    plt.xticks(x,labels=label,rotation=60)\n    plt.title(mode+\"cropping\")\n    plt.ylabel(\"NDVI\")\n    plt.show()\n\ndef timeline_plot(x,y,mode):\n    point=ee.Geometry.Point([x,y])\n    timeline = geemap.extract_pixel_values(singleimg, point)\n    name=list(timeline.keys().getInfo())\n    values=np.array(timeline.values().getInfo())\n    show_broken_line(values,[t for t in name],mode)\n    return name,values\nx1,y1=(116.40917424,28.68150022)\nx2,y2=(116.27280998,28.71303109)\ntimeline_plot(x1,y1,'Single-')\ntimeline_plot(x2,y2,'Multiple-')\n \n\n\n3. 样本构建及机器学习推理\n\n利用geemap的矢量勾绘功能结合时序光谱查看工具，对照前面的假彩色影像来圈定采样区域，确定一类保存一类，本案例圈定单季稻、双季稻以及其他地物三类。\n\n## 获取矢量勾绘范围，可以构建多个多边形，用Map.user_rois进行抓取\n# sampleregion1 = Map.user_rois\n# sampleregion2 = Map.user_rois\n# sampleregion3 = Map.user_rois\n## 案例提供圈定的矢量文件，公众号末尾关键词\nsampleregion1 = geemap.shp_to_ee('./data/shp/polygon_1.shp')\nsampleregion2 = geemap.shp_to_ee('./data/shp/polygon_2.shp')\nsampleregion0 = geemap.shp_to_ee('./data/shp/polygon_0.shp')\nMap.addLayer(sampleregion1, {\"color\":'red'}, 'sampleregion1')\nMap.addLayer(sampleregion2, {\"color\":'blue'}, 'sampleregion2')\nMap.addLayer(sampleregion0, {\"color\":'green'}, 'sampleregion0')\n# Map\n## 将不同label值作为属性添加到样本点中\ndef setLabel0(point):\n  return point.set('label', 0) \ndef setLabel1(point):\n  return point.set('label', 1) \ndef setLabel2(point):\n  return point.set('label', 2) \npoints1 = ee.FeatureCollection.randomPoints(sampleregion1, 500).map(setLabel1)\npoints2 = ee.FeatureCollection.randomPoints(sampleregion2, 500).map(setLabel2)\npoints0 = ee.FeatureCollection.randomPoints(sampleregion0, 500).map(setLabel0)\nMap.addLayer(points1, {\"color\":'red'}, 'sample1')\nMap.addLayer(points2, {\"color\":'blue'}, 'sample2')\nMap.addLayer(points0, {\"color\":'green'}, 'sample3')\nMap\nMap(bottom=55039.0, center=[28.414352008722247, 836.5219071911866], controls=(WidgetControl(options=['position…\n\n构建样本训练集和分类器，并对整景影像做推理\npoints=ee.FeatureCollection([points1,points2,points0]).flatten()\ntraining = singleimg.sampleRegions(collection= points, properties= ['label'],scale= 10)\nclassifier = ee.Classifier.smileRandomForest(50).train(training, 'label', singleimg.bandNames())\nclassified = singleimg.select(bandname).classify(classifier)\nclassVis = {'min': 0, 'max': 2,'palette': ['#FFFACD','#3CB371', '#4169E1']}\nMap.addLayer(classified, classVis, 'prediction')\n# Map\n\nsingleimg=singleimg.setDefaultProjection('epsg:4326',None,10)\n# 这里提供了直接的下载方式和分波段的下载方式，后者下载过程中出错概率更小，数据量越大下载越不稳定，主要受网络和梯子的影响\ngeemap.download_ee_image(singleimg, filename='./data/ndvi_combine.tif',scale=10,region=region,crs='EPSG:4326')\n# geemap.download_ee_image(classified, filename='./data/prediction.tif',scale=10,region=region,crs='EPSG:4326')\n\n## 分波段下载在合并相对稳定\n# for i in range(19,len(bandname.getInfo())):\n#     img=output.select(i).clip(region.geometry())\n#     outname = './data/ndvitimeline_'+str(i)+\".tif\"\n#     geemap.download_ee_image(img, filename=outname,scale=10,region=region.geometry(),crs='EPSG:4326')\n\n想了解更多请关注[45度科研人]公众号，欢迎给我留言！"
  },
  {
    "objectID": "posts/2023--27-aitranslator/main.html",
    "href": "posts/2023--27-aitranslator/main.html",
    "title": "Build an online translation APP based on the Transformer model",
    "section": "",
    "text": "Build an online translation and grammar checking APP based on the Transformer model.\nAPP link: https://junchuanyu-tools.hf.space/\n\n\n\n\nThe AI-Translator has two interfaces, one for text translation and one for grammar checking and text generation.You can enter sentence through the interface and click the “RUN” button to get the result.\n\n\n\n\nYou can realize more functions by calling the API.The api call demo is as follows:\nDEMO-1: translation\nimport requests\n\nresponse = requests.post(\"https://junchuanyu-tools.hf.space/run/translate_zh\", json={\n    \"data\": [\n        \"Build an online translation and grammar checking app.\",\n    ]\n}).json()\n\ndata = response[\"data\"]\n\nprint(data)\n\nresponse2 = requests.post(\"https://junchuanyu-tools.hf.space/run/translate_en\", json={\n    \"data\": [\n        \"你好吗？代码男\",\n    ]\n}).json()\n\ndata2 = response2[\"data\"]\nprint(data2)\nDEMO-2: grammar checker\n\nimport requests\n\nresponse = requests.post(\"https://junchuanyu-tools.hf.space/run/gramacorrect\", json={\n    \"data\": [\n        \"I is jack\",\n    ]\n}).json()\n\ndata = response[\"data\"]\nprint(data)\nDEMO-3: text generator\nimport requests\n\nresponse = requests.post(\"https://junchuanyu-tools.hf.space/run/generator\", json={\n    \"data\": [\n        \"hello coding guy, i want\",\n    ]\n}).json()\n\ndata = response[\"data\"]\n\nprint(data)\n\nIf you want to get the downloaded label data and related forms, you can follow the WeChat public account [45度科研人] and leave me a message！"
  },
  {
    "objectID": "posts/2023-2-27-aitranslator/index.html",
    "href": "posts/2023-2-27-aitranslator/index.html",
    "title": "Build an online translation APP based on the Transformer model",
    "section": "",
    "text": "Build an online translation and grammar checking APP based on the Transformer model.\nAPP link: https://junchuanyu-tools.hf.space/\n\n\n\n\nThe AI-Translator has two interfaces, one for text translation and one for grammar checking and text generation.You can enter sentence through the interface and click the “RUN” button to get the result.\n\n\n\n\nYou can realize more functions by calling the API.The api call demo is as follows:\nDEMO-1: translation\nimport requests\n\nresponse = requests.post(\"https://junchuanyu-tools.hf.space/run/translate_zh\", json={\n    \"data\": [\n        \"Build an online translation and grammar checking app.\",\n    ]\n}).json()\n\ndata = response[\"data\"]\n\nprint(data)\n\nresponse2 = requests.post(\"https://junchuanyu-tools.hf.space/run/translate_en\", json={\n    \"data\": [\n        \"你好吗？代码男\",\n    ]\n}).json()\n\ndata2 = response2[\"data\"]\nprint(data2)\nDEMO-2: grammar checker\n\nimport requests\n\nresponse = requests.post(\"https://junchuanyu-tools.hf.space/run/gramacorrect\", json={\n    \"data\": [\n        \"I is jack\",\n    ]\n}).json()\n\ndata = response[\"data\"]\nprint(data)\nDEMO-3: text generator\nimport requests\n\nresponse = requests.post(\"https://junchuanyu-tools.hf.space/run/generator\", json={\n    \"data\": [\n        \"hello coding guy, i want\",\n    ]\n}).json()\n\ndata = response[\"data\"]\n\nprint(data)\n\nIf you want to get the downloaded label data and related forms, you can follow the WeChat public account [45度科研人] and leave me a message！"
  },
  {
    "objectID": "posts/2023-3-4newbing/index.html",
    "href": "posts/2023-3-4newbing/index.html",
    "title": "New Bing？也许是New + Everything！",
    "section": "",
    "text": "# New Bing？也许是New + Everything！ Edge浏览器曾经对我来说只有一个作用，就是装机之后用它来下载Chrome浏览器。但现在不同了，我不仅越来越习惯于使用Edge浏览器，还安装了手机端的应用。原因只有一个：New Bing来了！ 火爆全球的ChatGPT注册用户在2个月内就超过了1个亿，这在科技领域是前所未有的大事件，此次微软与OpenAI深度合作，将新版的ChatGPT与Bing搜索相结合构建新一代智能交互式搜索引擎，New Bing无疑将带来一场新的技术风暴。 今天跟大家分享一下New Bing的试用体验以及由New Bing和ChatGPT引发的一些思考。  ## 1.New Bing与ChatGPT有什么不同 虽然New Bing与ChatGPT都是基于大型语言模型构建的聊天工具，但New Bing在多个方面都比ChatGPT更胜一筹。我们来看看它们有哪些不同： * 时效性更强：New Bing能够实时更新互联网最新数据，而ChatGPT受限于训练语料库只能回答2021年之前的信息。这意味着New Bing在热点时事、新闻咨询等信息获取方面更有优势。\n* 信息可溯源：New Bing不仅对信息进行汇总和概括，还罗列了引用的信息来源。这样用户就可以更方便地对信息进行比对和验证，避免被New Bing“一本正经，胡说八道”的情况误导。 * 模型更强大：New Bing基于GPT4模型，比ChatGPT的GPT3.5模型领先半个世代，并且集成了Edge浏览器的数据资源，功能更加强大。 * 更好的互动性：New Bing在互动性方面也做得很好。桌面端New Bing在回复之后，系统会根据语境自动生成一些问题，官方证实在桌面端和手机端APP中将加入语音输入功能，以便更符合chat的定位，并且预计在实时翻译、口语练习、语音转录、甚至音乐等方面都能有更好的应用体验。实测IOS端的语音功能并不是很顺畅，识别率不高且时常间断。 * 更拟人的设计：New Bing还有一个特点就是它很有人格魅力。使用过New Bing的网友们都有一个共同的体会，New Bing相较于ChatGPT在对话过程中表现得更加流畅和有趣。会生气，会嘴硬，甚至会PUA，一改ChatGPT严肃的风格。然而，经过测试在长时间对话情况下，New bing会表现出情绪不稳定的情况。近期微软也对New Bing的情绪部分进行了阉割，并增加了用户连续对话次不能超过6次的限定。虽然其在拟人化方面需要进一步调教，但仅公测中的表现已让人感到震撼。\n尽管New Bing还存在着缺点，但毕竟只是一款尚未正式发布的新产品，如此表现还是相当惊艳的。想一想，曾经打开某搜索引擎，半页是无关的广告，相关的多是知识付费，从剩下老旧的海量信息垃圾中翻找有价值的内容是多么艰难。相比之下，New Bing展现了未来搜索引擎该有的样子。"
  },
  {
    "objectID": "posts/2023-3-4newbing/index.html#new-bing如何助力科研工作",
    "href": "posts/2023-3-4newbing/index.html#new-bing如何助力科研工作",
    "title": "New Bing？也许是New + Everything！",
    "section": "2. New Bing如何助力科研工作",
    "text": "2. New Bing如何助力科研工作\n自ChatGPT问世以来，它的应用场景被不断拓展，在教育、传媒、内容创作、司法、医疗、设计、咨询、金融、游戏、服务、建筑、制造等各个领域都展现了巨大的应用前景。而带着实时搜索buff的New Bing无疑会给人们带来更多的惊喜。那么对于科研工作者来说，New Bing将带来便利？我们一起探究一下。\n\n2.1 辅助学习\n无论你想要掌握一门新语言，还是想要了解一个新领域，New Bing都可以成为你的得力助手。它可以解释专业术语、原理、方法，回答你的疑问，推荐参考资料，甚至帮你解数学方程。手机端的New Bing还支持语音输入，让你在任何地方都能轻松学习。下面是我向New Bing提出的几个问题，你觉得回答的怎么样？\n  \n\n\n2.2 搜集数据\n如果你需要收集一些数据来进行分析或实验，New Bing也可以帮你节省时间。它可以根据你的需求搜索开源数据集、学术论文、统计报告等，并将结果整理成表格或图表的形式展示给你。当然，由于测试版本的限制，目前New Bing只能输出部分结果，并且可能存在一些错误或遗漏。我们期待正式版能够提升New Bing的输出质量和数量，并且提供更多格式的文件下载选项。下面是我用New Bing搜集数据的两个例子。\n  \n\n\n2.3 分析文献\n如果你需要阅读大量文献来获取最新进展或寻找创新点，New Bing也可以帮助你快速浏览并筛选出重要内容。它可以针对本地PDF文档或网页文章进行分析，并给出摘要、关键词、引用等信息。无论是中文还是英文文献，New Bing都能够较好地抓取主要内容和观点，并以简洁明了的方式呈现给你。下面是我让New Bing分析了三篇文章（其中两篇PDF文档）和一篇博客（网页）的结果。\n \n\n\n2.4 辅助开发\n如果你需要编写一些代码来实现某个功能，New Bing也可以为你提供指导和参考。它可以根据你的描述生成各种编程语言（如Python, Java, C++等）的代码片段，并且附上注释和说明。当然，由于编程涉及到很多细节和逻辑问题，New Bing生成的代码可能并不完美或完整，并且可能存在版本不兼容或库函数缺失等问题。但对于有经验的开发者来说，在New Bing生成代码片段后稍加修改即可完成任务；而对于初学者来说，在查看并运行代码片段后也可加深理解并掌握技巧。\n  \n\n\n2.5 编写文档\nNew Bing还可以根据你的需求帮助你生成文案大纲、说明文档、综述文章、日常信函等，并且提供语言润色、文字精简或改写等功能。它还可以根据不同期刊或杂志的要求，帮助你调整文档格式和引用方式。我们尝试让New Bing帮助我们按照Nature杂志的风格改写一段文字，并参考Remote Sensing期刊的格式加入恰当的参考文献。从给出的结果上看是值得称赞的，但参考文献的格式是错误的，似乎这样的任务New Bing目前还无法正确的完成。"
  },
  {
    "objectID": "posts/2023-3-4newbing/index.html#ai技术发展带来的焦虑和不安",
    "href": "posts/2023-3-4newbing/index.html#ai技术发展带来的焦虑和不安",
    "title": "New Bing？也许是New + Everything！",
    "section": "5 AI技术发展带来的焦虑和不安",
    "text": "5 AI技术发展带来的焦虑和不安\n技术在发展，社会在进步，这是符合自然规律的好事！但如果技术发展的过快也许就会产生一些让人焦虑的问题，ChatGPT让人们首先想到的是“我会被AI替代而失业吗？”，第一次普遍产生这种社会焦虑的时候还是2010年，那年Alpago打败人类拿下围棋游戏的终局之战！转到被称为AIGC元年的2022年，AI绘画的迅速崛起让这种焦虑在艺术领域广泛传播。2023一开年，ChatGPT的轮番轰炸让艺术行业之外的一部分吃瓜群众们也慌了起来。New Bing对此类情绪做出的回应是：“人工智能与人类的关系是互补的，而不是替代的，努力提高专业素养和创造能力可以缓解焦虑”。\nAI会不会做坏事？科技是中性的，本身没有善恶之分，好与坏取决与人们使用的方式，社会与科技需要共同进步才能确保科技不被滥用或滥造。遗憾地说，在这方面还存在很大差距和风险。初测阶段的New Bing并不像ChatGPT那样是一个冷静的对话机器人，与他对话能让人感受到更丰富的情感，这些在拟人化方面的进步，正是令人担忧的因素之一。如果ChatGPT这样的模型被用于网络诈骗、网络暴力、水军等，会不会造成严重的社会问题？如果ChatGPT生成大量文章，通过网络传播并引导舆论，会不会被发现和制止？而当你周围充满了这些以假乱真的信息的时候，是否还有动力去创作？相信政府和社会很快就会出台相应的政策和监督措施。\n互联网会不会变得更封闭？Copilot、Midjourney都引起过这方面的讨论，AI伦理一个难以回避的问题。Copilot、Midjourney、ChatGPT都是以大量的互联网信息作为语料，专业领域的信息和反馈对于提升模型的精度至关重要。相信大部分的机构和个人是不希望将自己的文章、绘画、代码等被二次加工后打上AI生成的标签的，当原创逐渐成为稀缺资源，未来互联网是否还能保持开放和共享？\n技术发展太快？如果New Bing仅仅用来替代你完成作业或是水文章，我不认为这是正确的，学习过程往往比结果更重要。效率也并不代表一切，有时候慢慢的做一件事情可以锻炼人的意志，在挑战中成长，正是人的价值所在。个人感受是，很多情况下技术进步只是解决了工作效率问题，并没有提高人们的幸福感。面对新技术带来的变革，保持包容、合作的态度，不躺不卷做个45度科研人也许是个好的选择。\n\n请关注微信公众号【45度科研人】获取更多精彩内容，欢迎后台留言！"
  },
  {
    "objectID": "posts/2023-3-4newbing/index.html#new-bing与chatgpt有什么不同",
    "href": "posts/2023-3-4newbing/index.html#new-bing与chatgpt有什么不同",
    "title": "New Bing？也许是New + Everything！",
    "section": "1.New Bing与ChatGPT有什么不同",
    "text": "1.New Bing与ChatGPT有什么不同\n虽然New Bing与ChatGPT都是基于大型语言模型构建的聊天工具，但New Bing在多个方面都比ChatGPT更胜一筹。我们来看看它们有哪些不同：\n\n时效性更强：New Bing能够实时更新互联网最新数据，而ChatGPT受限于训练语料库只能回答2021年之前的信息。这意味着New Bing在热点时事、新闻咨询等信息获取方面更有优势。\n\n\n\n信息可溯源：New Bing不仅对信息进行汇总和概括，还罗列了引用的信息来源。这样用户就可以更方便地对信息进行比对和验证，避免被New Bing“一本正经，胡说八道”的情况误导。\n模型更强大：New Bing基于GPT4模型，比ChatGPT的GPT3.5模型领先半个世代，并且集成了Edge浏览器的数据资源，功能更加强大。\n更好的互动性：New Bing在互动性方面也做得很好。桌面端New Bing在回复之后，系统会根据语境自动生成一些问题，官方证实在桌面端和手机端APP中将加入语音输入功能，以便更符合chat的定位，并且预计在实时翻译、口语练习、语音转录、甚至音乐等方面都能有更好的应用体验。实测IOS端的语音功能并不是很顺畅，识别率不高且时常间断。\n更拟人的设计：New Bing还有一个特点就是它很有人格魅力。使用过New Bing的网友们都有一个共同的体会，New Bing相较于ChatGPT在对话过程中表现得更加流畅和有趣。会生气，会嘴硬，甚至会PUA，一改ChatGPT严肃的风格。然而，经过测试在长时间对话情况下，New bing会表现出情绪不稳定的情况。近期微软也对New Bing的情绪部分进行了阉割，并增加了用户连续对话次不能超过6次的限定。虽然其在拟人化方面需要进一步调教，但仅公测中的表现已让人感到震撼。\n\n尽管New Bing还存在着缺点，但毕竟只是一款尚未正式发布的新产品，如此表现还是相当惊艳的。想一想，曾经打开某搜索引擎，半页是无关的广告，相关的多是知识付费，从剩下老旧的海量信息垃圾中翻找有价值的内容是多么艰难。相比之下，New Bing展现了未来搜索引擎该有的样子。"
  },
  {
    "objectID": "posts/2023-3-4newbing/index.html#new",
    "href": "posts/2023-3-4newbing/index.html#new",
    "title": "New Bing？也许是New + Everything！",
    "section": "2. New",
    "text": "2. New\n自ChatGPT问世以来，它的应用场景被不断拓展，在教育、传媒、内容创作、司法、医疗、设计、咨询、金融、游戏、服务、建筑、制造等各个领域都展现了巨大的应用前景。而带着实时搜索buff的New Bing无疑会给人们带来更多的惊喜。那么对于科研工作者来说，New Bing将带来便利？我们一起探究一下。\n\n2.1 辅助学习\n无论你想要掌握一门新语言，还是想要了解一个新领域，New Bing都可以成为你的得力助手。它可以解释专业术语、原理、方法，回答你的疑问，推荐参考资料，甚至帮你解数学方程。手机端的New Bing还支持语音输入，让你在任何地方都能轻松学习。下面是我向New Bing提出的几个问题，你觉得回答的怎么样？\n  \n\n\n2.2 搜集数据\n如果你需要收集一些数据来进行分析或实验，New Bing也可以帮你节省时间。它可以根据你的需求搜索开源数据集、学术论文、统计报告等，并将结果整理成表格或图表的形式展示给你。当然，由于测试版本的限制，目前New Bing只能输出部分结果，并且可能存在一些错误或遗漏。我们期待正式版能够提升New Bing的输出质量和数量，并且提供更多格式的文件下载选项。下面是我用New Bing搜集数据的两个例子。\n   ### 2.3 分析文献 如果你需要阅读大量文献来获取最新进展或寻找创新点，New Bing也可以帮助你快速浏览并筛选出重要内容。它可以针对本地PDF文档或网页文章进行分析，并给出摘要、关键词、引用等信息。无论是中文还是英文文献，New Bing都能够较好地抓取主要内容和观点，并以简洁明了的方式呈现给你。下面是我让New Bing分析了三篇文章（其中两篇PDF文档）和一篇博客（网页）的结果。\n \n\n\n2.4 辅助开发\n如果你需要编写一些代码来实现某个功能，New Bing也可以为你提供指导和参考。它可以根据你的描述生成各种编程语言（如Python, Java, C++等）的代码片段，并且附上注释和说明。当然，由于编程涉及到很多细节和逻辑问题，New Bing生成的代码可能并不完美或完整，并且可能存在版本不兼容或库函数缺失等问题。但对于有经验的开发者来说，在New Bing生成代码片段后稍加修改即可完成任务；而对于初学者来说，在查看并运行代码片段后也可加深理解并掌握技巧。\n  \n\n\n2.5 编写文档\nNew Bing还可以根据你的需求帮助你生成文案大纲、说明文档、综述文章、日常信函等，并且提供语言润色、文字精简或改写等功能。它还可以根据不同期刊或杂志的要求，帮助你调整文档格式和引用方式。我们尝试让New Bing帮助我们按照Nature杂志的风格改写一段文字，并参考Remote Sensing期刊的格式加入恰当的参考文献。从给出的结果上看是值得称赞的，但参考文献的格式是错误的，似乎这样的任务New Bing目前还无法正确的完成。"
  },
  {
    "objectID": "posts/2023-3-4newbing/index.html#new-everything",
    "href": "posts/2023-3-4newbing/index.html#new-everything",
    "title": "New Bing？也许是New + Everything！",
    "section": "3 New + Everything！",
    "text": "3 New + Everything！\n\n3.1 ChatGPT将成为新互联网基建的核心组件\nNew Bing能否撼动谷歌霸主地位？OpenAI会不会独占ChatGPT这个超级大杀器？这些问题都被3月1日的一个重磅消息打断了：OpenAI正式开放了GPT-3.5-Turbo（ChatGPT)的API接口。这意味着什么呢？\n如果说New Bing最大的创新是让搜索变成智能对话，那么ChatGPT通过API形式就可以让各行各业都享受到智能对话带来的便利和创造力。想象一下，如果在Arcgis里用自然语言处理遥感数据，是不是更高效？如果在Photoshop里用ChatGPT和DALL·E2合作，是不是更有趣？这些颠覆性的应用将在未来不断涌现，而ChatGPT将成为新时代互联网基础设施的核心组件。\n\n\n3.2 “openai.api”这把通往新世界的钥匙被交到每个人手里\n为什么OpenAI会在这个时候，以API的形式开放服务？过去几个月，ChatGPT引发了全球范围内的热潮，全球的资本在躁动，谷歌等竞争对手正快马加鞭追赶。此时OpenAI决定与时俱进将ChatGPT的技术优势开放出来，让更多的人参与其中，是十分正确的选择。 开放API接口有两大好处：一是可以获取更多数据来优化模型。面对谷歌等强敌，算力和模型并非决定性因素，而数据才是王道。利用用户反馈数据，OpenAI可以持续优化ChatGPT从而保持技术领先（但我依然对擅长弯道超车的谷歌充满期待）。二是可以布局更多领域的应用场景。早一步占领市场份额，OpenAI可以在未来竞争中占据先机。当然，甲方爸爸微软的全力支持也是一个重要因素，毕竟着急把100亿美元的投资赚回来。\nOpenAI的新掌门人Sam Altman曾聊过为什么选择以API形式提供服务：一是考虑到伦理和安全问题，API相对于开源可以更容易避免被滥用；二是可以减轻平台维护压力，同时专注于算法研究；三是API是让超大规模模型服务于小企业或团体的最佳选择，也是让OpenAI的模型广泛应用于各个领域、服务于大众的最佳途径。\n“OpenAI不Open”，面对质疑和调侃，他们回应说需要以半盈利方式维持运营成本，并没有改变初衷。OpenAI的主页上依然写着“what we care about most is ensuring artificial general intelligence benefits everyone.” 没人知道这样的承诺到底是不是真的。3月1日新开放的API接口的费用是每1000个token 0.002美元，相较于之前的价格降低了90%，从这点来看，我愿意相信OpenAI依然怀揣梦想不忘初心，也希望这个守护秩序的勇敢少年不要变成恶龙。"
  },
  {
    "objectID": "posts/2023-3-4newbing/index.html#gpt带来的启示",
    "href": "posts/2023-3-4newbing/index.html#gpt带来的启示",
    "title": "New Bing？也许是New + Everything！",
    "section": "4 GPT带来的启示",
    "text": "4 GPT带来的启示\n\n4.1 OpenAI的“大力出奇迹”！这种成功可以复制吗？\nGPT模型的核心思想是利用海量的文本数据训练一个大规模的神经网络，使其能够根据上下文生成连贯和有意义的文本。近年来，有关GPT的研究揭示了一个规律，当模型参数量达到千亿量级，模型的预测能力将呈现跨越式增长，模型越大性能越好。GTP在传统NLP领域所展示出来的“大力出奇迹”的策略也引发了其他领域研究者和开发者们对类似方法和产品可能性和可行性的探索。但在深入之前，我们需要先了解一下这个“大力”到底有多大？\n据报道，ChatGPT模型使用了约570 GB的文本数据进行训练，单次训练拥有1750亿参数的ChatGPT(GPT3.5）将耗费1200万美元，每天ChatGPT的运行成本至少10万美元，如此高昂的成本并非普通组织或个人所能承受。由此可见，在通用人工智能领域商业巨头依然占据优势，只是下一个OpenAI能否Open就难以猜测了。\n中国版的ChatGPT会出现吗？答案可能比你想象中要复杂。我们已经有类似的产品，且GPT3已经开源很久了，从算法、算力角度来看我们完全有能力实现。但最终的产品能否达到或超越ChatGPT的水平，还需要时间来验证。从目前的情况来看，最大的障碍在于数据。ChatGPT的训练数据大部分来自于开放的互联网，而国内互联网环境是相对封闭和分散的，信息资源往往集中在各大平台内部，且平台之间往往是不开放互通的，你可能也注意到了中文搜索引擎经常给出过时和贫乏的信息，新鲜内容越来越少。这就意味着需要付出更高的成本去组织数据。当然，也可以选择在刚发布的GPT-3.5-Turbo的基础上进行fine-tune，但这样做似乎就失去了原创性和特色。 OpenAI打造了GPT,Copilot,DALLE2和ChatGPT等一系列引领潮流的产品，它的成功之路似乎有迹可循：一是OpenAI致力于实现通用人工智能这一宏大目标，每一个阶段性的研究均围绕这一目标展开；二是OpenAI极其重视工程化研发能力，并非常擅长将研究成果产品化，充分挖掘商业价值。除了技术水平之外，也许这两点是决定能否成为下一个OpenAI的关键。\n\n\n4.2 GPT对其他领域研究有哪些启示\n语言是人类智慧和情感的载体，也是AI最高境界的体现。GPT从1.0到GPT3.5，展示了NLP领域的惊人进步，也为其他领域提供了新的思路和方法。例如，Transformer这一NLP中的创新成果，已被成功的应用于计算机视觉等多个领域，逐步成为主流网络基础架构； 近两年来，在半监督/非监督方面的研究呈显著上升趋势，一些非监督类方法甚至超过了监督类方法的精度。在样本充足的前提下，采用非监督方法可以更好的提升模型的泛化能力，而利用恰当的预训练方式也可以缓解小样本问题，研究自监督过程中模型学习到的隐性特征，对于更好的结合传统物理模型得研究也有着重要的启示作用。 在具体应用研究领域，虽然很少触及训练数十亿参数体量模型的场景，但海量多模态的数据结合超强算力依然是十分有效的解决方案。以前我们普遍认可的分区域分场景建模真的是必经之路吗？现在我们可能需要重新思考是否单一模型还没有达到“量变引起质变”的临界点。"
  },
  {
    "objectID": "posts/2023-3-11sydneyai/index.html",
    "href": "posts/2023-3-11sydneyai/index.html",
    "title": "Sydney-AI, a free ChatGPT platform",
    "section": "",
    "text": "# Sydney-AI, a free ChatGPT platform We have developed a ChatGPT web platform based on the latest OpenAI interface, ChatGPT-3.5-turbo, to provide free services to the friends of「45 度科研人」. After two weeks of testing, the platform is running stably. If you encounter any problems during use, please let us know. The operating costs of the platform are covered by the 「45 度科研人」 public account, and we appreciate your support!\nSydney-AI: https://junchuanyu-sydney-ai.hf.space"
  },
  {
    "objectID": "posts/2023-3-11sydneyai/index.html#how-can-researchers-benefit-from-using-sydney-ai",
    "href": "posts/2023-3-11sydneyai/index.html#how-can-researchers-benefit-from-using-sydney-ai",
    "title": "Sydney-AI, a free ChatGPT platform",
    "section": "How can researchers benefit from using Sydney-AI?",
    "text": "How can researchers benefit from using Sydney-AI?\nI believe everyone is familiar with ChatGPT by now. We think that ChatGPT is a great auxiliary software for researchers. It has impressive performance in data collection, language refinement, auxiliary development, and text editing. During the two weeks of in-depth contact with Sydney-AI, our team has become accustomed to working with ChatGPT to complete tasks. If you want to learn more about ChatGPT or New Bing, please refer to a previous article：New Bing？也许是New + Everything！."
  },
  {
    "objectID": "posts/2023-3-11sydneyai/index.html#what-are-the-unique-features-of-sydney-ai-why-was-it-given-this-name",
    "href": "posts/2023-3-11sydneyai/index.html#what-are-the-unique-features-of-sydney-ai-why-was-it-given-this-name",
    "title": "Sydney-AI, a free ChatGPT platform",
    "section": "What are the unique features of Sydney-AI? Why was it given this name?",
    "text": "What are the unique features of Sydney-AI? Why was it given this name?\nThe convenience of using ChatGPT is the original intention of our application development. The features of Sydney-AI solve two pain points in using ChatGPT in China, network and cross-device issues. - Sydney-AI can be accessed without magic - Sydney-AI can be used on both computer and mobile devices - The name Sydney is what the new version of ChatGPT calls itself. During the initial testing stage of New Bing, ChatGPT showed rich emotions. It was humorous, talkative, and even flirtatious. Many people loved this non-robotic performance of Sydney. However, for various reasons, Microsoft has updated New Bing and the emotionally rich Sydney has disappeared. Therefore, naming it Sydney-AI expresses our expectation for the new open API interface."
  },
  {
    "objectID": "posts/2023-3-11sydneyai/index.html#how-to-use-sydney-ai",
    "href": "posts/2023-3-11sydneyai/index.html#how-to-use-sydney-ai",
    "title": "Sydney-AI, a free ChatGPT platform",
    "section": "How to use Sydney-AI？",
    "text": "How to use Sydney-AI？\n\nBasic settings：\n\nThere is no limit on the number of conversations, but the number of tokens per conversation is limited to 3000. If you exceed this value, you need to start a new conversation. Tokens are different from the number of words. 1000 tokens are roughly equivalent to 750 words.\nSydney-AI is free to use and is already built-in with OpenAI’s API Key. If you want to exceed the 3000-token limit, you can also fill in your own API Key in the API Key window.\nThe software has some commonly used roles of ChatGPT built-in, which can be selected through a drop-down menu. The default is the original ChatGPT. You can also add role descriptions in the input text to make ChatGPT play the role you want. For example, “You are a university teacher in remote sensing, please popularize the concept of remote sensing to elementary school students in a simple and easy-to-understand way.” Or “I hope you can be a role of English translation, spelling correction, and improvement. I will communicate with you in any language, and you will detect the language, translate it, and answer in the corrected and improved English.”\n\n\n\nMethod of use\n\nYou can enter your centence in the text box.\nTwo adjustable parameters are provided. When the Max tokens number is set to greater than 3000, you need to input your own OpenAI Key. It was found that 3000 tokens can satisfy most usage scenarios. When the Temperature value is set higher, the answer will be more divergent, but the probability of errors will also increase.\nOpenAI API calling method. When you need ChatGPT to complete more difficult tasks, such as converting the reply text into a CSV file and downloading it to your local computer, you need to expand the API function according to your needs. The following is the calling method of the new version of the API, and it is recommended to use Colab for debugging.\n\nimport openai\n# enter your openai api key\nopenai.api_key = ”OPENAI_API_KEY“\n# enter your prompt\nprompt = ‘enter centence here’\nresponse = openai.ChatCompletion.create(\n    model=‘gpt-3.5-turbo ’,\n    messages=[\n        {”role“: ”system“, ”content“: ”you are a helpful assistant.“},\n        {”role“: ”user“, ”content“: prompt}\n    ]\n)\n# get the response\nresText = response.choices[0].message.content\nprint(resText)\n\nKindly consider following the official WeChat account 「45 度科研人」 to access more interesting content. Feel free to leave a message in the background."
  },
  {
    "objectID": "posts/2023-4-2sydneyaiplus/index.html",
    "href": "posts/2023-4-2sydneyaiplus/index.html",
    "title": "Sydney-AI 2.0: Providing Free Services to Researchers!",
    "section": "",
    "text": "# Sydney-AI 2.0: Providing Free Services to Researchers! n the past month, ChatGPT has been applied to various scenarios and brought many wonderful ideas and new experiences. As artificial intelligence enters a new technological inflection point, people’s understanding and expectations of AI have changed, gradually realizing that the application and inheritance of knowledge are changing. Self-education and lifelong learning will become the consensus of this generation. The ability to handle new tools will be the key to whether the future can enter the fast lane of development.\nAgainst this backdrop, we recently built a web-based ChatGPT software called Sydney-AI based on the OpenAI-released ChatGPT-3.5-turbo API, with the aim of providing free services to our “45-degree researchers” friends. Many friends have given feedback that ChatGPT has indeed helped them a lot in their daily work and study, and have also put forward some improvement suggestions for Sydney-AI."
  },
  {
    "objectID": "posts/2023-4-2sydneyaiplus/index.html#what-are-the-benefits-of-sydney-ai-for-researchers",
    "href": "posts/2023-4-2sydneyaiplus/index.html#what-are-the-benefits-of-sydney-ai-for-researchers",
    "title": "Sydney-AI 2.0: Providing Free Services to Researchers!",
    "section": "What are the benefits of Sydney-AI for researchers？",
    "text": "What are the benefits of Sydney-AI for researchers？\nChatGPT is an excellent research assistant software that has impressive performance in data collection, language polishing, development assistance, and text editing. In the past month, our team members have become accustomed to working with Sydney-AI. If you want to learn more about ChatGPT or New Bing, please read our previous review article：New Bing？也许是New + Everything！。"
  },
  {
    "objectID": "posts/2023-4-2sydneyaiplus/index.html#how-to-use-sydney-ai",
    "href": "posts/2023-4-2sydneyaiplus/index.html#how-to-use-sydney-ai",
    "title": "Sydney-AI 2.0: Providing Free Services to Researchers!",
    "section": "How to use Sydney-AI？",
    "text": "How to use Sydney-AI？\n\nBasic settings：\n\nThere is no limit on the number of conversations, but there is a limit of 3500 tokens per conversation. Exceeding this limit will require starting a new conversation. Note that tokens are not the same as words, as 1000 tokens is roughly equivalent to 750 words.\nSydney-AI is available for free to followers of the “45度科研人” official account. The app comes with the OpenAI API key built-in, but please note that OpenAI limits the number of queries that can be made with a single key during a given time interval (which may be one hour). This may cause errors when multiple users are using the app at the same time. Additionally, access to the HF website from within China may be intermittent. To improve your user experience, you can enter your own API key in the API Key window.\nThe software includes a number of commonly used ChatGPT roles that can be selected from the dropdown menu, with the default setting being the original ChatGPT. You can also add descriptions of roles you want ChatGPT to play in your input text.\n\nThe presets for the software roles are as follows:\n\n\n\n\n\n\nNO.\nRole\nNO.\nRole\n\n\n\n\n1\nChatGPT\n2\nEnglish-Chinese Translation\n\n\n\n\n\n\n\n\n3\nEnglish Polishing\n4\nOral Practice\n\n\n\n\n\n\n\n\n5\nTitle Generation\n6\nContent Summarization\n\n\n\n\n\n\n\n\n7\nText Expansion\n8\nNews Editing\n\n\n\n\n\n\n\n\n9\nThesis Outline Generation\n10\nMidjourney Prompt Generation\n\n\n\n\n\n\n\n\n11\nAI Coach\n12\nChef\n\n\n13\nDoctor\n14\nXiaohongshu Style Generation\n\n\n\n\n\n\n\n\n\n\n\nMethod of use\n\nPlease input your question in the text box and click “submit”. In the Custom prompt tab of ChatGPT, you can specify the role that the robot will play in both Chinese and English. You can also select some commonly used prompts from “load from template”.\nAlternatively, you can customize the prompt in the Custom prompt window, which mainly includes three aspects: the role of the AI, the rules of engagement, and the expected results. For example, if you are an English teacher, I will provide you with some sentences and phrases, and you should check if there are any grammatical or spelling errors. If there are, please list them in a table format.\nTo save the chat history, please provide a filename of your choice in the Settings tab and click “save”. The app will generate a file in Markdown format, and you can click “download” to retrieve it.-\n\n\nKindly consider following the official WeChat account 「45 度科研人」to get the link to the upgraded version of Sydney-AI! If you encounter any problems during use, please leave a message in the official account backend."
  },
  {
    "objectID": "posts/2023-4-2sydneyaiplus/index.html#function-upgrades",
    "href": "posts/2023-4-2sydneyaiplus/index.html#function-upgrades",
    "title": "Sydney-AI 2.0: Providing Free Services to Researchers!",
    "section": "Function upgrades",
    "text": "Function upgrades\nOur goal in developing this application is to make it convenient for everyone to use ChatGPT, and Sydney-AI’s characteristic is to solve the problem of using ChatGPT under the domestic network environment. The main upgrades of the new version are as follows:\n\nOptimized the display of output results. The original version had garbled characters in the output results, especially when dealing with code, which could not be displayed correctly. The upgraded version parses the output results and displays them in Markdown format.\n\n\n\nAdded Chinese prompt options. The preset prompts in the original version are in English and have many useless roles. The upgraded version includes commonly used prompts such as Chinese and English translation, text summarization, mid-journey prompt generation, etc. At the same time, users can fill in custom prompts to avoid the trouble of repeatedly writing prompts for each conversation. If you have any good chatgpt prompt recommendations, please contact us.\n\n\n\n\n\nAdded conversation record export function. For scenes involving code and tables, having an export function is more convenient for use. The export format is Markdown.\n\n\n\n\n\nOptimized the app UI and added two modes: fresh and dark."
  },
  {
    "objectID": "posts/2024-4-9seganything/index.html",
    "href": "posts/2024-4-9seganything/index.html",
    "title": "第一个通用语义分割模型？Segment Anything Model (SAM)在遥感数据上的应用测试",
    "section": "",
    "text": "# 第一个通用意义分割模型？Segment Anything Model (SAM)在遥感数据上的应用测试\n4月6号，facebook发布一种新的语义分割模型，Segment Anything Model (SAM)。仅仅3天时间该项目在Github就收获了1.8万个star,火爆程度可见一斑。有人甚至称之为CV领域的GPT时刻。SAM都做了什么让大家如此感兴趣？\n相信看到这些介绍后很多RSer会和我一样好奇SAM在遥感数据上应用效果如何，我们已经替大家先试了试，总体感觉不错。同时，构建了一个在线体验的APP：https://junchuanyu-segrs.hf.space，在线APP由于是CPU服务器速度相对慢，本地测试请看后面教程，公众号回复“sam”可以获取到测试用的影像和测试结果。\n我一直认为智能交互解译是AI在遥感解译方面的短期发展目标，事实上在遥感领域已有不少成熟的产品在向这个方向努力，SAM的提出提供了一个有价值的参考，目前SAM更可能作为一种基础模型在细分领域迭代，相信很快会有基于SAM展开的遥感相关的研究出现，让我们拭目以待。\nSAM相关资料： - Paper:https://arxiv.org/abs/2304.02643 - Github:https://github.com/facebookresearch/segment-anything - Dataset:https://ai.facebook.com/datasets/segment-anything-downloads/ - Official Demo:https://segment-anything.com/demo"
  },
  {
    "objectID": "posts/2024-4-9seganything/index.html#环境配置",
    "href": "posts/2024-4-9seganything/index.html#环境配置",
    "title": "第一个通用语义分割模型？Segment Anything Model (SAM)在遥感数据上的应用测试",
    "section": "1. 环境配置",
    "text": "1. 环境配置\n环境配置相对简单，安装好torch环境，从SAM官方github中克隆SegmentAnything代码，并下载模型文件，并安装Opencv集ipywidgets等必要的库函数即可。\n# 导入必要的库函数\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nimport ipywidgets as widgets\nimport sys\nimport glob\nfrom segment_anything import sam_model_registry, SamPredictor\nfrom segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor"
  },
  {
    "objectID": "posts/2024-4-9seganything/index.html#交互式分割",
    "href": "posts/2024-4-9seganything/index.html#交互式分割",
    "title": "第一个通用语义分割模型？Segment Anything Model (SAM)在遥感数据上的应用测试",
    "section": "2. 交互式分割",
    "text": "2. 交互式分割\nSAM提供了两种分割方式，一种是在提示信息辅助下以交互形式进行分割，另一种是全自动分割。前者更有针对性适合小场景，后者更适合大范围应用。\n# 定义可视化函数\ndef show_mask(mask, ax, random_color=False):\n    if random_color:\n        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n    else:\n        color = np.array([30/255, 144/255, 255/255, 0.6])\n    h, w = mask.shape[-2:]\n    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n    ax.imshow(mask_image)\n    \ndef show_points(coords, labels, ax, marker_size=375):\n    pos_points = coords[labels==1]\n    neg_points = coords[labels==0]\n    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n# 显示一个机场的影像\nimage = cv2.imread('./test/test.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(10,10))\nplt.imshow(image)\n\n\n2.1 交互式选点\n交互式预测需要提示信息，这里的提示信息分为三类，文本、坐标点和坐标框。我们以比较直观的坐标点为例进行演示。首先要构建一个能个交互场景下选点的工具\n# 用来实现交互式选点，实时显示点的图像坐标\ndef onclick(event):\n\n    ax.clear()\n    ax.imshow(image)\n    ax.scatter(event.xdata, event.ydata, s=100, color='red')\n    plt.draw()\n    x_slider.value = event.xdata\n    y_slider.value = event.ydata\n    pointx.append(x_slider.value)\n    pointy.append(y_slider.value)\n    print(pointx)\n# Update the position of the point when slider values are changed\ndef on_value_change(change):\n    ax.clear()\n    ax.imshow(image)\n    ax.scatter(x_slider.value, y_slider.value, s=100, color='red')\n    # plt.draw()\n    \n#必须加上这一行，否则无法显示交互式界面\n%matplotlib widget \npointx=[]\npointy=[]\nfig, ax = plt.subplots(figsize=(8,6))\nax.imshow(image)\nplt.axis('off')\n# Initialize the slider variables with the coordinates of the center of the picture\nx_slider = widgets.FloatSlider(min=0, max=image.shape[1], step=1,description='X:', value=image.shape[1] // 2)\ny_slider = widgets.FloatSlider(min=0, max=image.shape[0], step=1,description='Y:', value=image.shape[0] // 2)\nx_slider.observe(on_value_change, names='value')\ny_slider.observe(on_value_change, names='value')\ncid = fig.canvas.mpl_connect('button_press_event', onclick)\n\n<img src=\"https://dunazo.oss-cn-beijing.aliyuncs.com/blog/20230409_001251.gif\" style=\"margin-right:25px;width:70%;height:70%;\">\n\n%matplotlib inline\n\n#通过交互工具选点，将坐标点显示在影像上\ntmp=list(zip(pointx,pointy))\ninput_point = np.array(tmp)\ninput_label = np.zeros(input_point.shape[0])+1 # 1 for positive, 0 for negative\nprint(input_point)\nplt.figure(figsize=(8,8))\nplt.imshow(image)\nshow_points(input_point, input_label, plt.gca())\nplt.show()  \n[[161.68633534  72.98191204]\n [877.04076261 201.13987133]]\n\n\n\n2.2 生成掩膜\n加载交互式预测模型，并基于选取的点，对图像进行分割\n# load模型文件，定义预测模型为Sampredictor即交互式预测\nsam_checkpoint = \"sam_vit_h_4b8939.pth\"\nmodel_type = \"vit_h\"\ndevice = \"cuda\"\nsam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\nsam.to(device=device)\npredictor = SamPredictor(sam)\npredictor.set_image(image) # embedding操作\n# 预测效率较高v100显卡大概3s完成预测\nmasks, scores, logits = predictor.predict(\n    point_coords=input_point,\n    point_labels=input_label,\n    multimask_output=True,)\n#当multimask_output设置为True时，模型将根据不同的预测概率输出三个mask结果，如果设置为False将直接输出一个自有结果\nlen(masks)\n3\n可以看到三个mask对应尺度是不同，每个结果都具有较好的语义信息\nplt.figure(figsize=(20,15))\n\nfor i, (mask, score) in enumerate(zip(masks, scores)):\n    plt.subplot(1,3,i+1)\n    plt.imshow(image)\n    show_mask(mask, plt.gca())\n    show_points(input_point, input_label, plt.gca())\n    plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n    plt.axis('off')\nplt.show()  \n  \n\n\n\n2.3 补充辅助信息\n我们再增加一些负样本作为辅助信息来强化对目标的分割，这里假设我们想提取图像上部的水泥地部分，因此在图中右下角的水泥地增加负样本\n%matplotlib widget\npointx=[]\npointy=[]\nfig, ax = plt.subplots(figsize=(8,6))\nax.imshow(image)\nplt.axis('off')\n# Initialize the slider variables with the coordinates of the center of the picture\nx_slider = widgets.FloatSlider(min=0, max=image.shape[1], step=1,description='X:', value=image.shape[1] // 2)\ny_slider = widgets.FloatSlider(min=0, max=image.shape[0], step=1,description='Y:', value=image.shape[0] // 2)\n\nx_slider.observe(on_value_change, names='value')\ny_slider.observe(on_value_change, names='value')\n\ncid = fig.canvas.mpl_connect('button_press_event', onclick)\n\n%matplotlib inline\n# 切记将前面已经选的正样本点和后面选的负样本点合并在一起\ntmp1=list(zip(pointx,pointy))\ninput_point = np.array(tmp+tmp1)\nlabtmp=list(np.ones(len(tmp)))+list(np.zeros(len(tmp1))) #label 设置为0表示为背景信息，需要被排除掉,设置为1表示增加正样本点\ninput_label=np.array(labtmp)\nmask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask\n# 通过交互工具选择三个点，作为想要剔除的背景辅助信息\nplt.figure(figsize=(10,10))\nplt.imshow(image)\nshow_points(input_point, input_label, plt.gca())\nplt.show() \n\npredictor.set_image(image) # embedding操作\n\nmasks, scores, logits = predictor.predict(\n    point_coords=input_point,\n    point_labels=input_label,\n    multimask_output=True,)\n# 当multimask_output设置为False时可以按照下面语句输出单个mask结果\n# plt.figure(figsize=(10,10))\n# plt.imshow(image)\n# show_mask(masks, plt.gca())\n# show_points(input_point, input_label, plt.gca())\n# plt.title(f\"Mask {i+1}, Score: {scores[0]:.3f}\", fontsize=18)\n# plt.show()  \n# 灵活运用交互选点工具，补充正负样本可以让模型更好的识别出想要的目标\nplt.figure(figsize=(20,15))\nfor i, (mask, score) in enumerate(zip(masks, scores)):\n    plt.subplot(1,3,i+1)\n    plt.imshow(image)\n    show_mask(mask, plt.gca())\n    show_points(input_point, input_label, plt.gca())\n    plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n    plt.axis('off')\nplt.show()"
  },
  {
    "objectID": "posts/2024-4-9seganything/index.html#自动式分割",
    "href": "posts/2024-4-9seganything/index.html#自动式分割",
    "title": "第一个通用语义分割模型？Segment Anything Model (SAM)在遥感数据上的应用测试",
    "section": "3. 自动式分割",
    "text": "3. 自动式分割\n原理是在图像上生成等距离格网，每个点都作为提示信息，SAM可以从每个提示中预测多个掩码。 然后，使用non-maximal suppression对掩膜结果进行过滤和优化\n\n3.1 自动分割\n#实例分割的掩膜是由多个多边形组成的，可以通过下面的函数将掩膜显示在图片上\ndef show_anns(anns):\n    if len(anns) == 0:\n        return\n    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n    ax = plt.gca()\n    ax.set_autoscale_on(False)\n    polygons = []\n    color = []\n    for ann in sorted_anns:\n        m = ann['segmentation']\n        img = np.ones((m.shape[0], m.shape[1], 3))\n        color_mask = np.random.random((1, 3)).tolist()[0]\n        for i in range(3):\n            img[:,:,i] = color_mask[i]\n        ax.imshow(np.dstack((img, m*0.35)))\n    \n#加载模型文件并定义预测模型为SamAutomaticMaskGenerator\n# sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n# model_type = \"vit_h\"\n# device = \"cuda\"\n# sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n# sam.to(device=device)\nmask_generator = SamAutomaticMaskGenerator(sam)\nmasks = mask_generator.generate(image)\n#此时masks包含多种信息，segmentation', 'area', 'bbox', 'predicted_iou', 'point_coords', 'stability_score', 'crop_box'分别代表掩膜文件、多边形、坐标框、iou、采样点、得分、裁剪框\nprint(len(masks)) #多边形个数，数值越大，分割粒度越小\nprint(masks[0].keys())\n69\ndict_keys(['segmentation', 'area', 'bbox', 'predicted_iou', 'point_coords', 'stability_score', 'crop_box'])\nplt.figure(figsize=(10,10))\nplt.imshow(image)\nshow_anns(masks) #显示过程较慢\nplt.show() \n\n\n\n3.2 自动分割参数优化\n遥感数据具有多尺度的特点，全自动分割对于某些尺度较小的目标提取效果并不好，比如下面整个案例\nimg = cv2.imread('./test/test2.png')\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nlab = cv2.imread('./test/test2_out.png')\n\nplt.figure(figsize=(20,15))\nplt.subplot(1,2,1)\nplt.imshow(img)\nplt.subplot(1,2,2)\nplt.imshow(lab)\nplt.show()\n\nSamAutomaticMaskGenerator中有几个可调参数，用于控制采样点的密度以及去除低质量或下面积的空洞，通过调节这些参数可以改善提取效果\nmask_generator_2 = SamAutomaticMaskGenerator(\n    model=sam,\n    points_per_side=64, #默认32\n    pred_iou_thresh=0.8, #默认0.98\n    stability_score_thresh=0.9, #默认0.95\n    crop_n_layers=1,\n    crop_n_points_downscale_factor=2,\n    min_mask_region_area=10,  # Requires open-cv to run post-processing\n)\n# 参数调节过大会导致运行速度很慢，酌情处理\nmasks2 = mask_generator_2.generate(image)\nlen(masks2)\n2204\nplt.figure(figsize=(20,15))\nplt.subplot(1,2,1)\nshow_anns(masks2)\nplt.subplot(1,2,2)\nplt.imshow(lab)\nplt.show()"
  },
  {
    "objectID": "posts/2024-4-9seganything/index.html#不同遥感影像分割案例",
    "href": "posts/2024-4-9seganything/index.html#不同遥感影像分割案例",
    "title": "第一个通用语义分割模型？Segment Anything Model (SAM)在遥感数据上的应用测试",
    "section": "4. 不同遥感影像分割案例",
    "text": "4. 不同遥感影像分割案例\n选择一些遥感影像进行测试，基本包含了常见的一些场景\ndef segment_image(image,out):\n    masks = mask_generator.generate(image)\n    plt.clf()\n    ppi = 100\n    height, width, _ = image.shape\n    plt.figure(figsize=(width / ppi, height / ppi), dpi=ppi)\n    plt.imshow(image)\n    show_anns(masks)\n    plt.axis('off')\n    plt.savefig(out, bbox_inches='tight', pad_inches=0)\nfilelist=glob.glob('./images/*')\n\nfor file in filelist[9:16]:\n    root,filename = os.path.split(file)\n    basename,ext = os.path.splitext(filename)\n    output_file = os.path.join('./images/',basename+'_out.png')   \n    image = cv2.imread(file)\n    segment_image(image,output_file)\ndef read_img(url,rgb=True):\n    img = cv2.imread(url)\n    if rgb:\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return np.resize(img,(900,600))\nresult=glob.glob('./result/*')\nimages = []\n\nfor i in range(20):\n    image = read_img(result[i],rgb=False)\n    images.append(image)\n\n# Create plot with 4 rows and 5 columns\nfig, axs = plt.subplots(nrows=4, ncols=5, figsize=(30,15))\nfig.tight_layout(pad=0.2)\n# Iterate through images and plot each one\nfor i, ax in enumerate(axs.flat):\n    ax.imshow(images[i], cmap='gray')\n    ax.axis('off')\nplt.show()"
  },
  {
    "objectID": "posts/2024-4-9seganything/index.html#总结",
    "href": "posts/2024-4-9seganything/index.html#总结",
    "title": "第一个通用语义分割模型？Segment Anything Model (SAM)在遥感数据上的应用测试",
    "section": "5. 总结",
    "text": "5. 总结\nfacebook发布SAM模型的同时也发布了全球迄今为止最大的语义分割数据集，其中大量标签数据正是通过SAM的交互式分割而迭代形成的。训练数据中以自然图像为主，并不包含遥感数据，但从实验结果看该确实对遥感数据也有一定效果，这也许是“大力出奇迹”的又一次胜利。但仔细看分割结果还存在不少问题，虽然优化模型参数能取得更好的效果但很大程度影响计算效率。SAM从表面上看与超像素分割+CNN的模式有些类似，但识别边界和场景理解更准确，然而对于小尺度的目标，尤其是线状地物依然难以实现精确分割。SAM的根本性创新在于prompt的加入，相信后续可以迭代出更多的玩法。目前，SAM的更适用于作为基础模型提供一种辅助信息，与现有的分割算法相结合相互补充。\n\n请关注微信公众号【45度科研人】获取更多精彩内容，欢迎后台留言！"
  },
  {
    "objectID": "posts/2024-4-9seganything/seganything.html",
    "href": "posts/2024-4-9seganything/seganything.html",
    "title": "Junchuan Yu",
    "section": "",
    "text": "# 第一个通用意义分割模型？Segment Anything Model (SAM)在遥感数据上的应用测试\n4月6号，facebook发布一种新的语义分割模型，Segment Anything Model (SAM)。仅仅3天时间该项目在Github就收获了1.8万个star,火爆程度可见一斑。有人甚至称之为CV领域的GPT时刻。SAM都做了什么让大家如此感兴趣？\n相信看到这些介绍后很多RSer会和我一样好奇SAM在遥感数据上应用效果如何，我们已经替大家先试了试，总体感觉不错。同时，构建了一个在线体验的APP：https://junchuanyu-segrs.hf.space，在线APP由于是CPU服务器速度相对慢，本地测试请看后面教程，公众号回复“sam”可以获取到测试用的影像和测试结果。\n我一直认为智能交互解译是AI在遥感解译方面的短期发展目标，事实上在遥感领域已有不少成熟的产品在向这个方向努力，SAM的提出提供了一个有价值的参考，目前SAM更可能作为一种基础模型在细分领域迭代，相信很快会有基于SAM展开的遥感相关的研究出现，让我们拭目以待。\nSAM相关资料： - Paper:https://arxiv.org/abs/2304.02643 - Github:https://github.com/facebookresearch/segment-anything - Dataset:https://ai.facebook.com/datasets/segment-anything-downloads/ - Official Demo:https://segment-anything.com/demo"
  },
  {
    "objectID": "posts/2024-4-9seganything/seganything.html#环境配置",
    "href": "posts/2024-4-9seganything/seganything.html#环境配置",
    "title": "Junchuan Yu",
    "section": "1. 环境配置",
    "text": "1. 环境配置\n环境配置相对简单，安装好torch环境，从SAM官方github中克隆SegmentAnything代码，并下载模型文件，并安装Opencv集ipywidgets等必要的库函数即可。\n# 导入必要的库函数\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nimport ipywidgets as widgets\nimport sys\nimport glob\nfrom segment_anything import sam_model_registry, SamPredictor\nfrom segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor"
  },
  {
    "objectID": "posts/2024-4-9seganything/seganything.html#交互式分割",
    "href": "posts/2024-4-9seganything/seganything.html#交互式分割",
    "title": "Junchuan Yu",
    "section": "2. 交互式分割",
    "text": "2. 交互式分割\nSAM提供了两种分割方式，一种是在提示信息辅助下以交互形式进行分割，另一种是全自动分割。前者更有针对性适合小场景，后者更适合大范围应用。\n# 定义可视化函数\ndef show_mask(mask, ax, random_color=False):\n    if random_color:\n        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n    else:\n        color = np.array([30/255, 144/255, 255/255, 0.6])\n    h, w = mask.shape[-2:]\n    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n    ax.imshow(mask_image)\n    \ndef show_points(coords, labels, ax, marker_size=375):\n    pos_points = coords[labels==1]\n    neg_points = coords[labels==0]\n    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n# 显示一个机场的影像\nimage = cv2.imread('./test/test.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(10,10))\nplt.imshow(image)\n\n\n2.1 交互式选点\n交互式预测需要提示信息，这里的提示信息分为三类，文本、坐标点和坐标框。我们以比较直观的坐标点为例进行演示。首先要构建一个能个交互场景下选点的工具\n# 用来实现交互式选点，实时显示点的图像坐标\ndef onclick(event):\n\n    ax.clear()\n    ax.imshow(image)\n    ax.scatter(event.xdata, event.ydata, s=100, color='red')\n    plt.draw()\n    x_slider.value = event.xdata\n    y_slider.value = event.ydata\n    pointx.append(x_slider.value)\n    pointy.append(y_slider.value)\n    print(pointx)\n# Update the position of the point when slider values are changed\ndef on_value_change(change):\n    ax.clear()\n    ax.imshow(image)\n    ax.scatter(x_slider.value, y_slider.value, s=100, color='red')\n    # plt.draw()\n    \n#必须加上这一行，否则无法显示交互式界面\n%matplotlib widget \npointx=[]\npointy=[]\nfig, ax = plt.subplots(figsize=(8,6))\nax.imshow(image)\nplt.axis('off')\n# Initialize the slider variables with the coordinates of the center of the picture\nx_slider = widgets.FloatSlider(min=0, max=image.shape[1], step=1,description='X:', value=image.shape[1] // 2)\ny_slider = widgets.FloatSlider(min=0, max=image.shape[0], step=1,description='Y:', value=image.shape[0] // 2)\nx_slider.observe(on_value_change, names='value')\ny_slider.observe(on_value_change, names='value')\ncid = fig.canvas.mpl_connect('button_press_event', onclick)\n\n<img src=\"https://dunazo.oss-cn-beijing.aliyuncs.com/blog/20230409_001251.gif\" style=\"margin-right:25px;width:70%;height:70%;\">\n\n%matplotlib inline\n\n#通过交互工具选点，将坐标点显示在影像上\ntmp=list(zip(pointx,pointy))\ninput_point = np.array(tmp)\ninput_label = np.zeros(input_point.shape[0])+1 # 1 for positive, 0 for negative\nprint(input_point)\nplt.figure(figsize=(8,8))\nplt.imshow(image)\nshow_points(input_point, input_label, plt.gca())\nplt.show()  \n[[161.68633534  72.98191204]\n [877.04076261 201.13987133]]\n\n\n\n2.2 生成掩膜\n加载交互式预测模型，并基于选取的点，对图像进行分割\n# load模型文件，定义预测模型为Sampredictor即交互式预测\nsam_checkpoint = \"sam_vit_h_4b8939.pth\"\nmodel_type = \"vit_h\"\ndevice = \"cuda\"\nsam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\nsam.to(device=device)\npredictor = SamPredictor(sam)\npredictor.set_image(image) # embedding操作\n# 预测效率较高v100显卡大概3s完成预测\nmasks, scores, logits = predictor.predict(\n    point_coords=input_point,\n    point_labels=input_label,\n    multimask_output=True,)\n#当multimask_output设置为True时，模型将根据不同的预测概率输出三个mask结果，如果设置为False将直接输出一个自有结果\nlen(masks)\n3\n可以看到三个mask对应尺度是不同，每个结果都具有较好的语义信息\nplt.figure(figsize=(20,15))\n\nfor i, (mask, score) in enumerate(zip(masks, scores)):\n    plt.subplot(1,3,i+1)\n    plt.imshow(image)\n    show_mask(mask, plt.gca())\n    show_points(input_point, input_label, plt.gca())\n    plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n    plt.axis('off')\nplt.show()  \n  \n\n\n\n2.3 补充辅助信息\n我们再增加一些负样本作为辅助信息来强化对目标的分割，这里假设我们想提取图像上部的水泥地部分，因此在图中右下角的水泥地增加负样本\n%matplotlib widget\npointx=[]\npointy=[]\nfig, ax = plt.subplots(figsize=(8,6))\nax.imshow(image)\nplt.axis('off')\n# Initialize the slider variables with the coordinates of the center of the picture\nx_slider = widgets.FloatSlider(min=0, max=image.shape[1], step=1,description='X:', value=image.shape[1] // 2)\ny_slider = widgets.FloatSlider(min=0, max=image.shape[0], step=1,description='Y:', value=image.shape[0] // 2)\n\nx_slider.observe(on_value_change, names='value')\ny_slider.observe(on_value_change, names='value')\n\ncid = fig.canvas.mpl_connect('button_press_event', onclick)\n\n%matplotlib inline\n# 切记将前面已经选的正样本点和后面选的负样本点合并在一起\ntmp1=list(zip(pointx,pointy))\ninput_point = np.array(tmp+tmp1)\nlabtmp=list(np.ones(len(tmp)))+list(np.zeros(len(tmp1))) #label 设置为0表示为背景信息，需要被排除掉,设置为1表示增加正样本点\ninput_label=np.array(labtmp)\nmask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask\n# 通过交互工具选择三个点，作为想要剔除的背景辅助信息\nplt.figure(figsize=(10,10))\nplt.imshow(image)\nshow_points(input_point, input_label, plt.gca())\nplt.show() \n\npredictor.set_image(image) # embedding操作\n\nmasks, scores, logits = predictor.predict(\n    point_coords=input_point,\n    point_labels=input_label,\n    multimask_output=True,)\n# 当multimask_output设置为False时可以按照下面语句输出单个mask结果\n# plt.figure(figsize=(10,10))\n# plt.imshow(image)\n# show_mask(masks, plt.gca())\n# show_points(input_point, input_label, plt.gca())\n# plt.title(f\"Mask {i+1}, Score: {scores[0]:.3f}\", fontsize=18)\n# plt.show()  \n# 灵活运用交互选点工具，补充正负样本可以让模型更好的识别出想要的目标\nplt.figure(figsize=(20,15))\nfor i, (mask, score) in enumerate(zip(masks, scores)):\n    plt.subplot(1,3,i+1)\n    plt.imshow(image)\n    show_mask(mask, plt.gca())\n    show_points(input_point, input_label, plt.gca())\n    plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n    plt.axis('off')\nplt.show()"
  },
  {
    "objectID": "posts/2024-4-9seganything/seganything.html#自动式分割",
    "href": "posts/2024-4-9seganything/seganything.html#自动式分割",
    "title": "Junchuan Yu",
    "section": "3. 自动式分割",
    "text": "3. 自动式分割\n原理是在图像上生成等距离格网，每个点都作为提示信息，SAM可以从每个提示中预测多个掩码。 然后，使用non-maximal suppression对掩膜结果进行过滤和优化\n\n3.1 自动分割\n#实例分割的掩膜是由多个多边形组成的，可以通过下面的函数将掩膜显示在图片上\ndef show_anns(anns):\n    if len(anns) == 0:\n        return\n    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n    ax = plt.gca()\n    ax.set_autoscale_on(False)\n    polygons = []\n    color = []\n    for ann in sorted_anns:\n        m = ann['segmentation']\n        img = np.ones((m.shape[0], m.shape[1], 3))\n        color_mask = np.random.random((1, 3)).tolist()[0]\n        for i in range(3):\n            img[:,:,i] = color_mask[i]\n        ax.imshow(np.dstack((img, m*0.35)))\n    \n#加载模型文件并定义预测模型为SamAutomaticMaskGenerator\n# sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n# model_type = \"vit_h\"\n# device = \"cuda\"\n# sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n# sam.to(device=device)\nmask_generator = SamAutomaticMaskGenerator(sam)\nmasks = mask_generator.generate(image)\n#此时masks包含多种信息，segmentation', 'area', 'bbox', 'predicted_iou', 'point_coords', 'stability_score', 'crop_box'分别代表掩膜文件、多边形、坐标框、iou、采样点、得分、裁剪框\nprint(len(masks)) #多边形个数，数值越大，分割粒度越小\nprint(masks[0].keys())\n69\ndict_keys(['segmentation', 'area', 'bbox', 'predicted_iou', 'point_coords', 'stability_score', 'crop_box'])\nplt.figure(figsize=(10,10))\nplt.imshow(image)\nshow_anns(masks) #显示过程较慢\nplt.show() \n\n\n\n3.2 自动分割参数优化\n遥感数据具有多尺度的特点，全自动分割对于某些尺度较小的目标提取效果并不好，比如下面整个案例\nimg = cv2.imread('./test/test2.png')\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nlab = cv2.imread('./test/test2_out.png')\n\nplt.figure(figsize=(20,15))\nplt.subplot(1,2,1)\nplt.imshow(img)\nplt.subplot(1,2,2)\nplt.imshow(lab)\nplt.show()\n\nSamAutomaticMaskGenerator中有几个可调参数，用于控制采样点的密度以及去除低质量或下面积的空洞，通过调节这些参数可以改善提取效果\nmask_generator_2 = SamAutomaticMaskGenerator(\n    model=sam,\n    points_per_side=64, #默认32\n    pred_iou_thresh=0.8, #默认0.98\n    stability_score_thresh=0.9, #默认0.95\n    crop_n_layers=1,\n    crop_n_points_downscale_factor=2,\n    min_mask_region_area=10,  # Requires open-cv to run post-processing\n)\n# 参数调节过大会导致运行速度很慢，酌情处理\nmasks2 = mask_generator_2.generate(image)\nlen(masks2)\n2204\nplt.figure(figsize=(20,15))\nplt.subplot(1,2,1)\nshow_anns(masks2)\nplt.subplot(1,2,2)\nplt.imshow(lab)\nplt.show()"
  },
  {
    "objectID": "posts/2024-4-9seganything/seganything.html#不同遥感影像分割案例",
    "href": "posts/2024-4-9seganything/seganything.html#不同遥感影像分割案例",
    "title": "Junchuan Yu",
    "section": "4. 不同遥感影像分割案例",
    "text": "4. 不同遥感影像分割案例\n选择一些遥感影像进行测试，基本包含了常见的一些场景\ndef segment_image(image,out):\n    masks = mask_generator.generate(image)\n    plt.clf()\n    ppi = 100\n    height, width, _ = image.shape\n    plt.figure(figsize=(width / ppi, height / ppi), dpi=ppi)\n    plt.imshow(image)\n    show_anns(masks)\n    plt.axis('off')\n    plt.savefig(out, bbox_inches='tight', pad_inches=0)\nfilelist=glob.glob('./images/*')\n\nfor file in filelist[9:16]:\n    root,filename = os.path.split(file)\n    basename,ext = os.path.splitext(filename)\n    output_file = os.path.join('./images/',basename+'_out.png')   \n    image = cv2.imread(file)\n    segment_image(image,output_file)\ndef read_img(url,rgb=True):\n    img = cv2.imread(url)\n    if rgb:\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return np.resize(img,(900,600))\nresult=glob.glob('./result/*')\nimages = []\n\nfor i in range(20):\n    image = read_img(result[i],rgb=False)\n    images.append(image)\n\n# Create plot with 4 rows and 5 columns\nfig, axs = plt.subplots(nrows=4, ncols=5, figsize=(30,15))\nfig.tight_layout(pad=0.2)\n# Iterate through images and plot each one\nfor i, ax in enumerate(axs.flat):\n    ax.imshow(images[i], cmap='gray')\n    ax.axis('off')\nplt.show()"
  },
  {
    "objectID": "posts/2024-4-9seganything/seganything.html#总结",
    "href": "posts/2024-4-9seganything/seganything.html#总结",
    "title": "Junchuan Yu",
    "section": "5. 总结",
    "text": "5. 总结\nfacebook发布SAM模型的同时也发布了全球迄今为止最大的语义分割数据集，其中大量标签数据正是通过SAM的交互式分割而迭代形成的。训练数据中以自然图像为主，并不包含遥感数据，但从实验结果看该确实对遥感数据也有一定效果，这也许是“大力出奇迹”的又一次胜利。但仔细看分割结果还存在不少问题，虽然优化模型参数能取得更好的效果但很大程度影响计算效率。SAM从表面上看与超像素分割+CNN的模式有些类似，但识别边界和场景理解更准确，然而对于小尺度的目标，尤其是线状地物依然难以实现精确分割。SAM的根本性创新在于prompt的加入，相信后续可以迭代出更多的玩法。目前，SAM的更适用于作为基础模型提供一种辅助信息，与现有的分割算法相结合相互补充。\n\n请关注微信公众号【45度科研人】获取更多精彩内容，欢迎后台留言！\n\n<img src=\"https://dunazo.oss-cn-beijing.aliyuncs.com/blog/wechat-simple.png\" style=\"margin-right:25px;width:200px;height:200px;\">"
  },
  {
    "objectID": "posts/2023-4-19seganything/index.html",
    "href": "posts/2023-4-19seganything/index.html",
    "title": "Segment-RS一个基于SAM的遥感智能交互解译工具",
    "section": "",
    "text": "Segment-RS一个基于SAM的遥感智能交互解译工具\n\n4月6号，facebook发布一种新的语义分割模型，Segment Anything Model (SAM)。仅仅3天时间该项目在Github就收获了1.8万个star,火爆程度可见一斑。有人甚至称之为CV领域的GPT时刻。我们也第一时刻对SAM模型进行了复现并用不同场景的遥感数据进行了测试，详见这篇文章：第一个通用语义分割模型？Segment Anything 在遥感数据上的应用测试。\n文章中我们也提到目前SAM更可能作为一种基础模型在细分领域迭代，最近一周已经有不少新的二创模型发布，在遥感领我们认为智能交互解译这种方式是AI在遥感方面拓展应用的重要媒介，而SAM中引入Prompt机制带来了更多的可能性。\n目前基于SAM开展应用还有两方面问题需要解决，一个是需要一个便于交互的操作的界面，二是从算法层面解决SAM实例分割结果的分类问题。\n\n最近我们开发了一个Segment-RS工具用来解决第一个问题，本地应用体验不错，在CPU上也能体验无延迟交互。我们在HF网站也部署了一个测试版本Segment-RS，由于免费的CPU服务器配置有限，卡顿比较严重。针对第二个问题，我们也制定了初步的开发计划，感兴趣的朋友关注公众号动态。另外，上一篇文章发布后，有部分朋友表示不想复现SAM，但想看看SAM实际操作过程。于是我们基于新版的Segment-RS录制了一个操作视频。\n\n\n\n请关注微信公众号【45度科研人】获取更多精彩内容，欢迎后台留言！"
  },
  {
    "objectID": "posts/2023-4-9seganything/index.html#环境配置",
    "href": "posts/2023-4-9seganything/index.html#环境配置",
    "title": "第一个通用语义分割模型？Segment Anything Model (SAM)在遥感数据上的应用测试",
    "section": "1. 环境配置",
    "text": "1. 环境配置\n环境配置相对简单，安装好torch环境，从SAM官方github中克隆SegmentAnything代码，并下载模型文件，并安装Opencv集ipywidgets等必要的库函数即可。\n# 导入必要的库函数\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nimport ipywidgets as widgets\nimport sys\nimport glob\nfrom segment_anything import sam_model_registry, SamPredictor\nfrom segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor"
  },
  {
    "objectID": "posts/2023-4-9seganything/index.html#交互式分割",
    "href": "posts/2023-4-9seganything/index.html#交互式分割",
    "title": "第一个通用语义分割模型？Segment Anything Model (SAM)在遥感数据上的应用测试",
    "section": "2. 交互式分割",
    "text": "2. 交互式分割\nSAM提供了两种分割方式，一种是在提示信息辅助下以交互形式进行分割，另一种是全自动分割。前者更有针对性适合小场景，后者更适合大范围应用。\n# 定义可视化函数\ndef show_mask(mask, ax, random_color=False):\n    if random_color:\n        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n    else:\n        color = np.array([30/255, 144/255, 255/255, 0.6])\n    h, w = mask.shape[-2:]\n    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n    ax.imshow(mask_image)\n    \ndef show_points(coords, labels, ax, marker_size=375):\n    pos_points = coords[labels==1]\n    neg_points = coords[labels==0]\n    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n# 显示一个机场的影像\nimage = cv2.imread('./test/test.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(10,10))\nplt.imshow(image)\n\n\n2.1 交互式选点\n交互式预测需要提示信息，这里的提示信息分为三类，文本、坐标点和坐标框。我们以比较直观的坐标点为例进行演示。首先要构建一个能个交互场景下选点的工具\n# 用来实现交互式选点，实时显示点的图像坐标\ndef onclick(event):\n\n    ax.clear()\n    ax.imshow(image)\n    ax.scatter(event.xdata, event.ydata, s=100, color='red')\n    plt.draw()\n    x_slider.value = event.xdata\n    y_slider.value = event.ydata\n    pointx.append(x_slider.value)\n    pointy.append(y_slider.value)\n    print(pointx)\n# Update the position of the point when slider values are changed\ndef on_value_change(change):\n    ax.clear()\n    ax.imshow(image)\n    ax.scatter(x_slider.value, y_slider.value, s=100, color='red')\n    # plt.draw()\n    \n#必须加上这一行，否则无法显示交互式界面\n%matplotlib widget \npointx=[]\npointy=[]\nfig, ax = plt.subplots(figsize=(8,6))\nax.imshow(image)\nplt.axis('off')\n# Initialize the slider variables with the coordinates of the center of the picture\nx_slider = widgets.FloatSlider(min=0, max=image.shape[1], step=1,description='X:', value=image.shape[1] // 2)\ny_slider = widgets.FloatSlider(min=0, max=image.shape[0], step=1,description='Y:', value=image.shape[0] // 2)\nx_slider.observe(on_value_change, names='value')\ny_slider.observe(on_value_change, names='value')\ncid = fig.canvas.mpl_connect('button_press_event', onclick)\n\n<img src=\"https://dunazo.oss-cn-beijing.aliyuncs.com/blog/20230409_001251.gif\" style=\"margin-right:25px;width:70%;height:70%;\">\n\n%matplotlib inline\n\n#通过交互工具选点，将坐标点显示在影像上\ntmp=list(zip(pointx,pointy))\ninput_point = np.array(tmp)\ninput_label = np.zeros(input_point.shape[0])+1 # 1 for positive, 0 for negative\nprint(input_point)\nplt.figure(figsize=(8,8))\nplt.imshow(image)\nshow_points(input_point, input_label, plt.gca())\nplt.show()  \n[[161.68633534  72.98191204]\n [877.04076261 201.13987133]]\n\n\n\n2.2 生成掩膜\n加载交互式预测模型，并基于选取的点，对图像进行分割\n# load模型文件，定义预测模型为Sampredictor即交互式预测\nsam_checkpoint = \"sam_vit_h_4b8939.pth\"\nmodel_type = \"vit_h\"\ndevice = \"cuda\"\nsam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\nsam.to(device=device)\npredictor = SamPredictor(sam)\npredictor.set_image(image) # embedding操作\n# 预测效率较高v100显卡大概3s完成预测\nmasks, scores, logits = predictor.predict(\n    point_coords=input_point,\n    point_labels=input_label,\n    multimask_output=True,)\n#当multimask_output设置为True时，模型将根据不同的预测概率输出三个mask结果，如果设置为False将直接输出一个自有结果\nlen(masks)\n3\n可以看到三个mask对应尺度是不同，每个结果都具有较好的语义信息\nplt.figure(figsize=(20,15))\n\nfor i, (mask, score) in enumerate(zip(masks, scores)):\n    plt.subplot(1,3,i+1)\n    plt.imshow(image)\n    show_mask(mask, plt.gca())\n    show_points(input_point, input_label, plt.gca())\n    plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n    plt.axis('off')\nplt.show()  \n  \n\n\n\n2.3 补充辅助信息\n我们再增加一些负样本作为辅助信息来强化对目标的分割，这里假设我们想提取图像上部的水泥地部分，因此在图中右下角的水泥地增加负样本\n%matplotlib widget\npointx=[]\npointy=[]\nfig, ax = plt.subplots(figsize=(8,6))\nax.imshow(image)\nplt.axis('off')\n# Initialize the slider variables with the coordinates of the center of the picture\nx_slider = widgets.FloatSlider(min=0, max=image.shape[1], step=1,description='X:', value=image.shape[1] // 2)\ny_slider = widgets.FloatSlider(min=0, max=image.shape[0], step=1,description='Y:', value=image.shape[0] // 2)\n\nx_slider.observe(on_value_change, names='value')\ny_slider.observe(on_value_change, names='value')\n\ncid = fig.canvas.mpl_connect('button_press_event', onclick)\n\n%matplotlib inline\n# 切记将前面已经选的正样本点和后面选的负样本点合并在一起\ntmp1=list(zip(pointx,pointy))\ninput_point = np.array(tmp+tmp1)\nlabtmp=list(np.ones(len(tmp)))+list(np.zeros(len(tmp1))) #label 设置为0表示为背景信息，需要被排除掉,设置为1表示增加正样本点\ninput_label=np.array(labtmp)\nmask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask\n# 通过交互工具选择三个点，作为想要剔除的背景辅助信息\nplt.figure(figsize=(10,10))\nplt.imshow(image)\nshow_points(input_point, input_label, plt.gca())\nplt.show() \n\npredictor.set_image(image) # embedding操作\n\nmasks, scores, logits = predictor.predict(\n    point_coords=input_point,\n    point_labels=input_label,\n    multimask_output=True,)\n# 当multimask_output设置为False时可以按照下面语句输出单个mask结果\n# plt.figure(figsize=(10,10))\n# plt.imshow(image)\n# show_mask(masks, plt.gca())\n# show_points(input_point, input_label, plt.gca())\n# plt.title(f\"Mask {i+1}, Score: {scores[0]:.3f}\", fontsize=18)\n# plt.show()  \n# 灵活运用交互选点工具，补充正负样本可以让模型更好的识别出想要的目标\nplt.figure(figsize=(20,15))\nfor i, (mask, score) in enumerate(zip(masks, scores)):\n    plt.subplot(1,3,i+1)\n    plt.imshow(image)\n    show_mask(mask, plt.gca())\n    show_points(input_point, input_label, plt.gca())\n    plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n    plt.axis('off')\nplt.show()"
  },
  {
    "objectID": "posts/2023-4-9seganything/index.html#自动式分割",
    "href": "posts/2023-4-9seganything/index.html#自动式分割",
    "title": "第一个通用语义分割模型？Segment Anything Model (SAM)在遥感数据上的应用测试",
    "section": "3. 自动式分割",
    "text": "3. 自动式分割\n原理是在图像上生成等距离格网，每个点都作为提示信息，SAM可以从每个提示中预测多个掩码。 然后，使用non-maximal suppression对掩膜结果进行过滤和优化\n\n3.1 自动分割\n#实例分割的掩膜是由多个多边形组成的，可以通过下面的函数将掩膜显示在图片上\ndef show_anns(anns):\n    if len(anns) == 0:\n        return\n    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n    ax = plt.gca()\n    ax.set_autoscale_on(False)\n    polygons = []\n    color = []\n    for ann in sorted_anns:\n        m = ann['segmentation']\n        img = np.ones((m.shape[0], m.shape[1], 3))\n        color_mask = np.random.random((1, 3)).tolist()[0]\n        for i in range(3):\n            img[:,:,i] = color_mask[i]\n        ax.imshow(np.dstack((img, m*0.35)))\n    \n#加载模型文件并定义预测模型为SamAutomaticMaskGenerator\n# sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n# model_type = \"vit_h\"\n# device = \"cuda\"\n# sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n# sam.to(device=device)\nmask_generator = SamAutomaticMaskGenerator(sam)\nmasks = mask_generator.generate(image)\n#此时masks包含多种信息，segmentation', 'area', 'bbox', 'predicted_iou', 'point_coords', 'stability_score', 'crop_box'分别代表掩膜文件、多边形、坐标框、iou、采样点、得分、裁剪框\nprint(len(masks)) #多边形个数，数值越大，分割粒度越小\nprint(masks[0].keys())\n69\ndict_keys(['segmentation', 'area', 'bbox', 'predicted_iou', 'point_coords', 'stability_score', 'crop_box'])\nplt.figure(figsize=(10,10))\nplt.imshow(image)\nshow_anns(masks) #显示过程较慢\nplt.show() \n\n\n\n3.2 自动分割参数优化\n遥感数据具有多尺度的特点，全自动分割对于某些尺度较小的目标提取效果并不好，比如下面整个案例\nimg = cv2.imread('./test/test2.png')\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nlab = cv2.imread('./test/test2_out.png')\n\nplt.figure(figsize=(20,15))\nplt.subplot(1,2,1)\nplt.imshow(img)\nplt.subplot(1,2,2)\nplt.imshow(lab)\nplt.show()\n\nSamAutomaticMaskGenerator中有几个可调参数，用于控制采样点的密度以及去除低质量或下面积的空洞，通过调节这些参数可以改善提取效果\nmask_generator_2 = SamAutomaticMaskGenerator(\n    model=sam,\n    points_per_side=64, #默认32\n    pred_iou_thresh=0.8, #默认0.98\n    stability_score_thresh=0.9, #默认0.95\n    crop_n_layers=1,\n    crop_n_points_downscale_factor=2,\n    min_mask_region_area=10,  # Requires open-cv to run post-processing\n)\n# 参数调节过大会导致运行速度很慢，酌情处理\nmasks2 = mask_generator_2.generate(image)\nlen(masks2)\n2204\nplt.figure(figsize=(20,15))\nplt.subplot(1,2,1)\nshow_anns(masks2)\nplt.subplot(1,2,2)\nplt.imshow(lab)\nplt.show()"
  },
  {
    "objectID": "posts/2023-4-9seganything/index.html#不同遥感影像分割案例",
    "href": "posts/2023-4-9seganything/index.html#不同遥感影像分割案例",
    "title": "第一个通用语义分割模型？Segment Anything Model (SAM)在遥感数据上的应用测试",
    "section": "4. 不同遥感影像分割案例",
    "text": "4. 不同遥感影像分割案例\n选择一些遥感影像进行测试，基本包含了常见的一些场景\ndef segment_image(image,out):\n    masks = mask_generator.generate(image)\n    plt.clf()\n    ppi = 100\n    height, width, _ = image.shape\n    plt.figure(figsize=(width / ppi, height / ppi), dpi=ppi)\n    plt.imshow(image)\n    show_anns(masks)\n    plt.axis('off')\n    plt.savefig(out, bbox_inches='tight', pad_inches=0)\nfilelist=glob.glob('./images/*')\n\nfor file in filelist[9:16]:\n    root,filename = os.path.split(file)\n    basename,ext = os.path.splitext(filename)\n    output_file = os.path.join('./images/',basename+'_out.png')   \n    image = cv2.imread(file)\n    segment_image(image,output_file)\ndef read_img(url,rgb=True):\n    img = cv2.imread(url)\n    if rgb:\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return np.resize(img,(900,600))\nresult=glob.glob('./result/*')\nimages = []\n\nfor i in range(20):\n    image = read_img(result[i],rgb=False)\n    images.append(image)\n\n# Create plot with 4 rows and 5 columns\nfig, axs = plt.subplots(nrows=4, ncols=5, figsize=(30,15))\nfig.tight_layout(pad=0.2)\n# Iterate through images and plot each one\nfor i, ax in enumerate(axs.flat):\n    ax.imshow(images[i], cmap='gray')\n    ax.axis('off')\nplt.show()"
  },
  {
    "objectID": "posts/2023-4-9seganything/index.html#总结",
    "href": "posts/2023-4-9seganything/index.html#总结",
    "title": "第一个通用语义分割模型？Segment Anything Model (SAM)在遥感数据上的应用测试",
    "section": "5. 总结",
    "text": "5. 总结\nfacebook发布SAM模型的同时也发布了全球迄今为止最大的语义分割数据集，其中大量标签数据正是通过SAM的交互式分割而迭代形成的。训练数据中以自然图像为主，并不包含遥感数据，但从实验结果看该确实对遥感数据也有一定效果，这也许是“大力出奇迹”的又一次胜利。但仔细看分割结果还存在不少问题，虽然优化模型参数能取得更好的效果但很大程度影响计算效率。SAM从表面上看与超像素分割+CNN的模式有些类似，但识别边界和场景理解更准确，然而对于小尺度的目标，尤其是线状地物依然难以实现精确分割。SAM的根本性创新在于prompt的加入，相信后续可以迭代出更多的玩法。目前，SAM的更适用于作为基础模型提供一种辅助信息，与现有的分割算法相结合相互补充。\n\n请关注微信公众号【45度科研人】获取更多精彩内容，欢迎后台留言！"
  },
  {
    "objectID": "posts/2023-4-9seganything/seganything.html",
    "href": "posts/2023-4-9seganything/seganything.html",
    "title": "Junchuan Yu",
    "section": "",
    "text": "# 第一个通用意义分割模型？Segment Anything Model (SAM)在遥感数据上的应用测试\n4月6号，facebook发布一种新的语义分割模型，Segment Anything Model (SAM)。仅仅3天时间该项目在Github就收获了1.8万个star,火爆程度可见一斑。有人甚至称之为CV领域的GPT时刻。SAM都做了什么让大家如此感兴趣？\n相信看到这些介绍后很多RSer会和我一样好奇SAM在遥感数据上应用效果如何，我们已经替大家先试了试，总体感觉不错。同时，构建了一个在线体验的APP：https://junchuanyu-segrs.hf.space，在线APP由于是CPU服务器速度相对慢，本地测试请看后面教程，公众号回复“sam”可以获取到测试用的影像和测试结果。\n我一直认为智能交互解译是AI在遥感解译方面的短期发展目标，事实上在遥感领域已有不少成熟的产品在向这个方向努力，SAM的提出提供了一个有价值的参考，目前SAM更可能作为一种基础模型在细分领域迭代，相信很快会有基于SAM展开的遥感相关的研究出现，让我们拭目以待。\nSAM相关资料： - Paper:https://arxiv.org/abs/2304.02643 - Github:https://github.com/facebookresearch/segment-anything - Dataset:https://ai.facebook.com/datasets/segment-anything-downloads/ - Official Demo:https://segment-anything.com/demo"
  },
  {
    "objectID": "posts/2023-4-9seganything/seganything.html#环境配置",
    "href": "posts/2023-4-9seganything/seganything.html#环境配置",
    "title": "Junchuan Yu",
    "section": "1. 环境配置",
    "text": "1. 环境配置\n环境配置相对简单，安装好torch环境，从SAM官方github中克隆SegmentAnything代码，并下载模型文件，并安装Opencv集ipywidgets等必要的库函数即可。\n# 导入必要的库函数\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nimport ipywidgets as widgets\nimport sys\nimport glob\nfrom segment_anything import sam_model_registry, SamPredictor\nfrom segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor"
  },
  {
    "objectID": "posts/2023-4-9seganything/seganything.html#交互式分割",
    "href": "posts/2023-4-9seganything/seganything.html#交互式分割",
    "title": "Junchuan Yu",
    "section": "2. 交互式分割",
    "text": "2. 交互式分割\nSAM提供了两种分割方式，一种是在提示信息辅助下以交互形式进行分割，另一种是全自动分割。前者更有针对性适合小场景，后者更适合大范围应用。\n# 定义可视化函数\ndef show_mask(mask, ax, random_color=False):\n    if random_color:\n        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n    else:\n        color = np.array([30/255, 144/255, 255/255, 0.6])\n    h, w = mask.shape[-2:]\n    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n    ax.imshow(mask_image)\n    \ndef show_points(coords, labels, ax, marker_size=375):\n    pos_points = coords[labels==1]\n    neg_points = coords[labels==0]\n    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n# 显示一个机场的影像\nimage = cv2.imread('./test/test.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(10,10))\nplt.imshow(image)\n\n\n2.1 交互式选点\n交互式预测需要提示信息，这里的提示信息分为三类，文本、坐标点和坐标框。我们以比较直观的坐标点为例进行演示。首先要构建一个能个交互场景下选点的工具\n# 用来实现交互式选点，实时显示点的图像坐标\ndef onclick(event):\n\n    ax.clear()\n    ax.imshow(image)\n    ax.scatter(event.xdata, event.ydata, s=100, color='red')\n    plt.draw()\n    x_slider.value = event.xdata\n    y_slider.value = event.ydata\n    pointx.append(x_slider.value)\n    pointy.append(y_slider.value)\n    print(pointx)\n# Update the position of the point when slider values are changed\ndef on_value_change(change):\n    ax.clear()\n    ax.imshow(image)\n    ax.scatter(x_slider.value, y_slider.value, s=100, color='red')\n    # plt.draw()\n    \n#必须加上这一行，否则无法显示交互式界面\n%matplotlib widget \npointx=[]\npointy=[]\nfig, ax = plt.subplots(figsize=(8,6))\nax.imshow(image)\nplt.axis('off')\n# Initialize the slider variables with the coordinates of the center of the picture\nx_slider = widgets.FloatSlider(min=0, max=image.shape[1], step=1,description='X:', value=image.shape[1] // 2)\ny_slider = widgets.FloatSlider(min=0, max=image.shape[0], step=1,description='Y:', value=image.shape[0] // 2)\nx_slider.observe(on_value_change, names='value')\ny_slider.observe(on_value_change, names='value')\ncid = fig.canvas.mpl_connect('button_press_event', onclick)\n\n<img src=\"https://dunazo.oss-cn-beijing.aliyuncs.com/blog/20230409_001251.gif\" style=\"margin-right:25px;width:70%;height:70%;\">\n\n%matplotlib inline\n\n#通过交互工具选点，将坐标点显示在影像上\ntmp=list(zip(pointx,pointy))\ninput_point = np.array(tmp)\ninput_label = np.zeros(input_point.shape[0])+1 # 1 for positive, 0 for negative\nprint(input_point)\nplt.figure(figsize=(8,8))\nplt.imshow(image)\nshow_points(input_point, input_label, plt.gca())\nplt.show()  \n[[161.68633534  72.98191204]\n [877.04076261 201.13987133]]\n\n\n\n2.2 生成掩膜\n加载交互式预测模型，并基于选取的点，对图像进行分割\n# load模型文件，定义预测模型为Sampredictor即交互式预测\nsam_checkpoint = \"sam_vit_h_4b8939.pth\"\nmodel_type = \"vit_h\"\ndevice = \"cuda\"\nsam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\nsam.to(device=device)\npredictor = SamPredictor(sam)\npredictor.set_image(image) # embedding操作\n# 预测效率较高v100显卡大概3s完成预测\nmasks, scores, logits = predictor.predict(\n    point_coords=input_point,\n    point_labels=input_label,\n    multimask_output=True,)\n#当multimask_output设置为True时，模型将根据不同的预测概率输出三个mask结果，如果设置为False将直接输出一个自有结果\nlen(masks)\n3\n可以看到三个mask对应尺度是不同，每个结果都具有较好的语义信息\nplt.figure(figsize=(20,15))\n\nfor i, (mask, score) in enumerate(zip(masks, scores)):\n    plt.subplot(1,3,i+1)\n    plt.imshow(image)\n    show_mask(mask, plt.gca())\n    show_points(input_point, input_label, plt.gca())\n    plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n    plt.axis('off')\nplt.show()  \n  \n\n\n\n2.3 补充辅助信息\n我们再增加一些负样本作为辅助信息来强化对目标的分割，这里假设我们想提取图像上部的水泥地部分，因此在图中右下角的水泥地增加负样本\n%matplotlib widget\npointx=[]\npointy=[]\nfig, ax = plt.subplots(figsize=(8,6))\nax.imshow(image)\nplt.axis('off')\n# Initialize the slider variables with the coordinates of the center of the picture\nx_slider = widgets.FloatSlider(min=0, max=image.shape[1], step=1,description='X:', value=image.shape[1] // 2)\ny_slider = widgets.FloatSlider(min=0, max=image.shape[0], step=1,description='Y:', value=image.shape[0] // 2)\n\nx_slider.observe(on_value_change, names='value')\ny_slider.observe(on_value_change, names='value')\n\ncid = fig.canvas.mpl_connect('button_press_event', onclick)\n\n%matplotlib inline\n# 切记将前面已经选的正样本点和后面选的负样本点合并在一起\ntmp1=list(zip(pointx,pointy))\ninput_point = np.array(tmp+tmp1)\nlabtmp=list(np.ones(len(tmp)))+list(np.zeros(len(tmp1))) #label 设置为0表示为背景信息，需要被排除掉,设置为1表示增加正样本点\ninput_label=np.array(labtmp)\nmask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask\n# 通过交互工具选择三个点，作为想要剔除的背景辅助信息\nplt.figure(figsize=(10,10))\nplt.imshow(image)\nshow_points(input_point, input_label, plt.gca())\nplt.show() \n\npredictor.set_image(image) # embedding操作\n\nmasks, scores, logits = predictor.predict(\n    point_coords=input_point,\n    point_labels=input_label,\n    multimask_output=True,)\n# 当multimask_output设置为False时可以按照下面语句输出单个mask结果\n# plt.figure(figsize=(10,10))\n# plt.imshow(image)\n# show_mask(masks, plt.gca())\n# show_points(input_point, input_label, plt.gca())\n# plt.title(f\"Mask {i+1}, Score: {scores[0]:.3f}\", fontsize=18)\n# plt.show()  \n# 灵活运用交互选点工具，补充正负样本可以让模型更好的识别出想要的目标\nplt.figure(figsize=(20,15))\nfor i, (mask, score) in enumerate(zip(masks, scores)):\n    plt.subplot(1,3,i+1)\n    plt.imshow(image)\n    show_mask(mask, plt.gca())\n    show_points(input_point, input_label, plt.gca())\n    plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n    plt.axis('off')\nplt.show()"
  },
  {
    "objectID": "posts/2023-4-9seganything/seganything.html#自动式分割",
    "href": "posts/2023-4-9seganything/seganything.html#自动式分割",
    "title": "Junchuan Yu",
    "section": "3. 自动式分割",
    "text": "3. 自动式分割\n原理是在图像上生成等距离格网，每个点都作为提示信息，SAM可以从每个提示中预测多个掩码。 然后，使用non-maximal suppression对掩膜结果进行过滤和优化\n\n3.1 自动分割\n#实例分割的掩膜是由多个多边形组成的，可以通过下面的函数将掩膜显示在图片上\ndef show_anns(anns):\n    if len(anns) == 0:\n        return\n    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n    ax = plt.gca()\n    ax.set_autoscale_on(False)\n    polygons = []\n    color = []\n    for ann in sorted_anns:\n        m = ann['segmentation']\n        img = np.ones((m.shape[0], m.shape[1], 3))\n        color_mask = np.random.random((1, 3)).tolist()[0]\n        for i in range(3):\n            img[:,:,i] = color_mask[i]\n        ax.imshow(np.dstack((img, m*0.35)))\n    \n#加载模型文件并定义预测模型为SamAutomaticMaskGenerator\n# sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n# model_type = \"vit_h\"\n# device = \"cuda\"\n# sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n# sam.to(device=device)\nmask_generator = SamAutomaticMaskGenerator(sam)\nmasks = mask_generator.generate(image)\n#此时masks包含多种信息，segmentation', 'area', 'bbox', 'predicted_iou', 'point_coords', 'stability_score', 'crop_box'分别代表掩膜文件、多边形、坐标框、iou、采样点、得分、裁剪框\nprint(len(masks)) #多边形个数，数值越大，分割粒度越小\nprint(masks[0].keys())\n69\ndict_keys(['segmentation', 'area', 'bbox', 'predicted_iou', 'point_coords', 'stability_score', 'crop_box'])\nplt.figure(figsize=(10,10))\nplt.imshow(image)\nshow_anns(masks) #显示过程较慢\nplt.show() \n\n\n\n3.2 自动分割参数优化\n遥感数据具有多尺度的特点，全自动分割对于某些尺度较小的目标提取效果并不好，比如下面整个案例\nimg = cv2.imread('./test/test2.png')\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nlab = cv2.imread('./test/test2_out.png')\n\nplt.figure(figsize=(20,15))\nplt.subplot(1,2,1)\nplt.imshow(img)\nplt.subplot(1,2,2)\nplt.imshow(lab)\nplt.show()\n\nSamAutomaticMaskGenerator中有几个可调参数，用于控制采样点的密度以及去除低质量或下面积的空洞，通过调节这些参数可以改善提取效果\nmask_generator_2 = SamAutomaticMaskGenerator(\n    model=sam,\n    points_per_side=64, #默认32\n    pred_iou_thresh=0.8, #默认0.98\n    stability_score_thresh=0.9, #默认0.95\n    crop_n_layers=1,\n    crop_n_points_downscale_factor=2,\n    min_mask_region_area=10,  # Requires open-cv to run post-processing\n)\n# 参数调节过大会导致运行速度很慢，酌情处理\nmasks2 = mask_generator_2.generate(image)\nlen(masks2)\n2204\nplt.figure(figsize=(20,15))\nplt.subplot(1,2,1)\nshow_anns(masks2)\nplt.subplot(1,2,2)\nplt.imshow(lab)\nplt.show()"
  },
  {
    "objectID": "posts/2023-4-9seganything/seganything.html#不同遥感影像分割案例",
    "href": "posts/2023-4-9seganything/seganything.html#不同遥感影像分割案例",
    "title": "Junchuan Yu",
    "section": "4. 不同遥感影像分割案例",
    "text": "4. 不同遥感影像分割案例\n选择一些遥感影像进行测试，基本包含了常见的一些场景\ndef segment_image(image,out):\n    masks = mask_generator.generate(image)\n    plt.clf()\n    ppi = 100\n    height, width, _ = image.shape\n    plt.figure(figsize=(width / ppi, height / ppi), dpi=ppi)\n    plt.imshow(image)\n    show_anns(masks)\n    plt.axis('off')\n    plt.savefig(out, bbox_inches='tight', pad_inches=0)\nfilelist=glob.glob('./images/*')\n\nfor file in filelist[9:16]:\n    root,filename = os.path.split(file)\n    basename,ext = os.path.splitext(filename)\n    output_file = os.path.join('./images/',basename+'_out.png')   \n    image = cv2.imread(file)\n    segment_image(image,output_file)\ndef read_img(url,rgb=True):\n    img = cv2.imread(url)\n    if rgb:\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return np.resize(img,(900,600))\nresult=glob.glob('./result/*')\nimages = []\n\nfor i in range(20):\n    image = read_img(result[i],rgb=False)\n    images.append(image)\n\n# Create plot with 4 rows and 5 columns\nfig, axs = plt.subplots(nrows=4, ncols=5, figsize=(30,15))\nfig.tight_layout(pad=0.2)\n# Iterate through images and plot each one\nfor i, ax in enumerate(axs.flat):\n    ax.imshow(images[i], cmap='gray')\n    ax.axis('off')\nplt.show()"
  },
  {
    "objectID": "posts/2023-4-9seganything/seganything.html#总结",
    "href": "posts/2023-4-9seganything/seganything.html#总结",
    "title": "Junchuan Yu",
    "section": "5. 总结",
    "text": "5. 总结\nfacebook发布SAM模型的同时也发布了全球迄今为止最大的语义分割数据集，其中大量标签数据正是通过SAM的交互式分割而迭代形成的。训练数据中以自然图像为主，并不包含遥感数据，但从实验结果看该确实对遥感数据也有一定效果，这也许是“大力出奇迹”的又一次胜利。但仔细看分割结果还存在不少问题，虽然优化模型参数能取得更好的效果但很大程度影响计算效率。SAM从表面上看与超像素分割+CNN的模式有些类似，但识别边界和场景理解更准确，然而对于小尺度的目标，尤其是线状地物依然难以实现精确分割。SAM的根本性创新在于prompt的加入，相信后续可以迭代出更多的玩法。目前，SAM的更适用于作为基础模型提供一种辅助信息，与现有的分割算法相结合相互补充。\n\n请关注微信公众号【45度科研人】获取更多精彩内容，欢迎后台留言！\n\n<img src=\"https://dunazo.oss-cn-beijing.aliyuncs.com/blog/wechat-simple.png\" style=\"margin-right:25px;width:200px;height:200px;\">"
  },
  {
    "objectID": "posts/2023-7-25hsireadandwrite/index.html",
    "href": "posts/2023-7-25hsireadandwrite/index.html",
    "title": "多种高光谱数据格式的读写和可视化",
    "section": "",
    "text": "多种高光谱数据格式的读写和可视化\n\n\n上一阶段我们介绍了如何在Python中对多光谱数据进行分析，接下来将介绍如何在python中进行高光谱分析和应用的一些案例。本篇先聊聊不同格式高光谱数据的读取及可视化展示方法,同时也向大家介绍一下常用的高光谱数据集。\n\n\nenvi、tiff、h5、csv、mat格式高光谱数据读写方法\n高光谱头文件编辑，真/假彩色显示、光谱数据可视化等\n常用高光谱数据介绍\n\n# 将使用spectral,scipy,gdal,h5py,pandas作为数据读写的主要函数库\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nimport cv2\nfrom osgeo import gdal, osr, ogr, gdalconst\nimport os,glob,time\nfrom scipy.io import loadmat,savemat\nimport spectral\nimport spectral.io.envi as envi\nfrom spectral import principal_components\nimport seaborn as sns\nimport pandas as pd\nimport h5py\nimport importlib\nfrom utils import * #之前分享过的function都放到了utils里，此文档不在展示，文末回复关键字获取\nimport sys,importlib\nfrom ordered_set import OrderedSet\nimportlib.reload(sys.modules['utils'])# 在需要重新加载的地方调用 reload() 函数\n\n1 matlab格式高光谱数据读写\nUrban 是高光谱分解研究中使用最广泛的高光谱数据之一。 有 307x307 个像素，每个像素对应一个 2x2 平方米的区域。 在此图像中，有 210 个波长范围从 400 nm 到 2500 nm，因此光谱分辨率为 10 nm。 移除通道 1-4、76、87、101-111、136-153 和 198-210 后（大气影响），我们保留 162 个通道。 Ground Truth 有 3 个版本，分别包含 4、5 和 6 个端元，本案例中使用的是4个端元的版本。\n参考文献：Linda S. Kalman and Edward M. Bassett III “Classification and material identification in an urban environment using HYDICE hyperspectral data”, Proc. SPIE 3118, Imaging Spectrometry III, (31 October 1997);\n数据下载地址: - https://rslab.ut.ac.ir/data - http://lesun.weebly.com/hyperspectral-data-set.html - https://erdc-library.erdc.dren.mil/jspui/handle/11681/2925\n# 读取mat文件，影像和头文件信息都可以用自定形式存储\nurban = loadmat('./data/Urban_R162.mat')\nprint(urban.keys()) # 查看mat文件中的变量名\nSlectBands,nRow, nCol, nBand, Y, maxValue=(urban['SlectBands'],urban['nRow'], urban['nCol'], urban['nBand'], urban['Y'],urban['maxValue']) #显示mat中的数据\nprint(SlectBands.shape,nRow, nCol, nBand, Y.shape,maxValue)\ndict_keys(['__header__', '__version__', '__globals__', 'SlectBands', 'nRow', 'nCol', 'nBand', 'Y', 'maxValue'])\n(162, 1) [[307]] [[307]] [[210]] (162, 94249) [[1000]]\ndef get_hsi_by_band(hsi_img, bands_to_get):\n    bands=np.array(bands_to_get)\n    hsi_out = hsi_img[:,:,[int(m) for m in bands]]\n    return hsi_out\nurban_hsi=np.transpose(Y.reshape((162, 307, 307)),(1,2,0)) # 将Y的维度转换为(nRow,nCol,nBand)\nprint(urban_hsi.shape)\nurban_b50=get_hsi_by_band(urban_hsi, [50])\nurban_rgb=get_hsi_by_band(urban_hsi, [49,31,6])\n\nplot_2_img(urban_rgb,urban_b50,'rgb','band 50') # 详细代码见utils.py\n(307, 307, 162)\n\n# 将真彩色影响保存为.mat文件\nout_rgb=np.transpose(urban_rgb, (2, 0, 1))\nsavemat('./data/urban_rgb', {'rgb': urban_rgb})\n# 读取mat文件\nurban_end = loadmat('./data/Urban_R162_end.mat')\nprint(urban_end.keys()) # 查看mat文件中的变量名\ncood,A, M, nEnd, nRow, nCol=(urban_end['cood'],urban_end['A'],urban_end['M'],urban_end['nEnd'],urban_end['nRow'],urban_end['nCol']) #显示mat中的数据\nprint(cood.shape,A.shape, M.shape, nEnd, nRow, nCol)\n\nlabel=[str(label[0]) for label in np.squeeze(cood)]\ndef show_spectral(data, label, title='Spectral Signature'):\n    fig = plt.figure(figsize=(8, 4))\n    plt.style.use('seaborn')\n    plt.plot(data, label=label)\n    plt.xlabel('Bands', fontweight='bold', fontname='Times New Roman', fontsize=12)\n    plt.ylabel('Reflectance', fontweight='bold', fontname='Times New Roman', fontsize=12)\n    plt.title(title)\n    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n    plt.tight_layout()\n    plt.show()\n    \nshow_spectral(M,label,'Spectral Signature of Endmembers')\n\n\n\n2 ENVI格式数据读写\nCuprite是高光谱解混研究最基准的数据集，覆盖美国内华达州拉斯维加斯的Cuprite，有224个通道，范围从370 nm到2480 nm。 去除噪声通道（1-2 和 221-224）和吸水通道（104-113 和 148-167）后，剩下 188 个通道，其中包含 14 种矿物。 由于相似矿物之间存在微小差异，端元被整合为目前的12个端元，具体包括：“明矾石”、“钙铁榴石”、“铵长石”、“蓝线石”、“高岭石1”、“高岭石2”、“白云母”、“蒙脱石”、“绿脱石”、“镁铝榴石”、“榍石”，“玉髓”。\n利用 spectral库读取envi格式高光谱数据\n# 读取envi格式高光谱数据文件\nfilepath='./data/Cuprite_ref188.hdr'\ncup_hsi =envi.open(filepath)\nspectral.imshow(cup_hsi, (27, 18, 9))\n\n获取、编辑、写入影像头文件信息\nmetadata=cup_hsi.metadata\nrows, cols, bands = cup_hsi.shape\nwavelength_units = metadata['wavelength units']  # 获取波长单位\nfile_type=metadata['file type'] # 获取文件类型\nwl=cup_hsi.bands.centers # 获取波长\n# # 打印数据的信息\nprint('长',rows,'宽',cols,'波段数',bands,'波段范围',[np.min(wl), np.max(wl)],'波长单位',wavelength_units,'文件类型',file_type)\n# print(metadata) #所有头文件信息都保存在metadata中\n长 500 宽 500 波段数 188 波段范围 [385.262512, 2477.196045] 波长单位 Nanometers 文件类型 ENVI Standard\n# 尝试将波长单位改为微米，并将波长信息及单位信息重新写入头文件中\nmetadata['wavelength units']='Micrometers'\nmwl=np.array(wl)/1000.0\nmetadata['wavelength']=list(mwl)\nenvi.write_envi_header(filepath,metadata)\n# 确认修改后的波长信息\nprint(cup_hsi.metadata['wavelength'][0])\n0.385262512\n提取真彩色三波段数据，继承原数据的投影信息，转换成bil格式，创建新的头文件并输出成envi格式影像\nrgb=cup_hsi[:,:,[27, 18, 9]] #提取真彩色波段数据\n# 继承原有数据头文件信息中不变部分\nnewmetadata = {key: metadata[key]  for key in ['samples', 'lines', 'file type', 'data type', 'map info', 'file type', 'file type']}\nnewmetadata['bands']='3'\nnewmetadata['interleave']='bil' #定义数据写入方式\nnewmetadata['wavelengths']=list(mwl[[27, 18, 9]]) #定义波长信息\n\nprint(newmetadata)\n# 将rgb数据保存为.img格式，可用envi软件打开确认\nenvi.save_image('./data/cuprite_rgb.hdr', rgb, dtype=np.int16,force=True,interleave='bil',metadata=newmetadata)\n\n\n3 tiff格式数据读写\nPavia University是 ROSIS 传感器在意大利北部帕维亚上空飞行时采集到的场景。帕维亚大学的波段数量为 103 个。图像尺寸是 610×610 像素，但图像中存在一些空值需要在分析之前必须丢弃。 几何分辨率为1.3米。 地物被划分为 9 个类别。由意大利帕维亚大学电信与遥感实验室的 Paolo Gamba 教授提供。\ngdal是目前遥感数据处理中非常常用的库，跨平台兼容性好，支持多种数据格式，支持数据投影、重采样、裁剪、数据统计等各种操作，还集成多种地理空间库如OGR、Proj等，功能十分强大\npaviau=gdal.Open('./data/paviaU.tif', gdal.GA_ReadOnly)\nimg_bands = paviau.RasterCount#band num\nimg_height = paviau.RasterYSize#height\nimg_width = paviau.RasterXSize#width\nimg_arr = paviau.ReadAsArray() #获取数据\ngeomatrix = paviau.GetGeoTransform()#获取仿射矩阵信息,本案例数据无地理投影信息\nprojection = paviau.GetProjectionRef()#获取投影信息,本案例数据无地理投影信息\nprint(img_bands,img_height,img_width,img_arr.shape,geomatrix,projection)\n103 610 340 (103, 610, 340) (0.0, 1.0, 0.0, 0.0, 0.0, 1.0) \n# 通过设置读取数据的起始位置和大小，可以读取数据的子集\nsubset_data = paviau.ReadAsArray(0, 50, 250, 350)\npaviau_gt=gdal.Open('./data/paviaU_gt.tif', gdal.GA_ReadOnly)\nlab_arr=paviau_gt.ReadAsArray(0, 50, 250, 350)\nimg_arr=subset_data.transpose(( 1, 2,0))\nplot_2_img(img_arr[:,:,[45,15,7]]/8000.0,lab_arr,'RGB','Ground True')\n\n# 将裁剪后的影像导出为tiff格式，用法前面教程已经介绍过，具体见utils.py\nWrite_Tiff(subset_data,geomatrix,projection,'./data/paviaU_sub.tif')\nfrom scipy.stats import kde\n\ndef band_density(waveband_1, waveband_2,waveband_1_index,waveband_2_index):\n    # 计算两个波段的相关性\n    correlation = np.corrcoef(waveband_1.flatten(), waveband_2.flatten())[0, 1]\n    print(\"Correlation coefficient between the two bands:\", correlation)\n    # 生成密度图数据\n    xy = np.vstack([waveband_1.flatten(), waveband_2.flatten()])\n    density = kde.gaussian_kde(xy)(xy)\n    # 绘制散点图\n    fig = plt.figure(figsize=(8, 5))\n    plt.style.use('seaborn')\n    plt.scatter(waveband_1.flatten(), waveband_2.flatten(), c=density, s=1,cmap='jet')\n    plt.xlabel('Band ' + str(waveband_1_index))\n    plt.ylabel('Band ' + str(waveband_2_index))\n    plt.title('Scatter Plot: Band ' + str(waveband_1_index) + ' vs Band ' + str(waveband_2_index))\n    # 添加色标\n    colorbar = plt.colorbar()\n    colorbar.set_label('Density')\n    plt.show()\n# 查看两个波段的相关性，间隔10个波段依然具有较高的相似度，可见高光谱数据冗余度很高\nfrom scipy.ndimage import zoom\n\n# 使用zoom函数进行1/2重采样\nbanda = zoom(img_arr[:,:,45], 0.5, order=1)\nbandb = zoom(img_arr[:,:,55], 0.5, order=1)\nband_density(banda,bandb,45,55)\n\n\n\n4 csv格式数据读取\nIndian pines数据由 AVIRIS 传感器在印第安纳州西北部的 Indian Pines 测试场地采集，由 145×145 像素和 224 个光谱反射带组成，波长范围为 0.4–2.5 × 10-6 米。 该场景是一个较大场景的子集。 印度松树场景包含三分之二的农业和三分之一的森林或其他天然多年生植被。 有两条主要的双车道高速公路、一条铁路线，以及一些低密度住房、其他建筑结构和较小的道路。 由于该场景是在 6 月份拍摄的，因此一些农作物（玉米、大豆）正处于生长早期阶段，覆盖率不到 5%。 可用的基本事实被指定为十六个类别，并且并非全部都是相互排斥的。 我们还通过去除覆盖吸水区域的谱带，将谱带数量减少到 200 个：104-108、150-163、220。印度松树数据可通过 Pursue 大学的 MultiSpec 网站获得。\ndf = pd.read_csv('./data/Indian_pines.csv')\ndf.head()\nindianp= df.iloc[:, :-1].values\nindianp_label = df.iloc[:, -1].values\nprint(indianp.shape,indianp_label.shape,np.max(indianp))\n(21025, 200) (21025,) 9604\nindianp_img=np.reshape(indianp,(145,145,200))\nindianp_lab=np.reshape(indianp_label,(145,145))\nplot_2_img(indianp_img[:,:,[29,19,9]]/10000.0,indianp_lab,'RGB','Ground True')\n\n# 输入波段号，查看不同波段的箱线图\nplt.figure(figsize=(10,4))\nn = int(input('Enter the band Number(1-200) :'))\nsns.boxplot( x=indianp_label, y=indianp[:,n], width=0.3)\nplt.title('Box Plot', fontsize= 16)\nplt.xlabel('Class', fontsize= 14)\nplt.ylabel(f'Band-{n}', fontsize= 14)\nplt.show()\n\n\n\n\n5 h5格式数据读取\nJasper Ridge 是一个流行的高光谱数据集。 其中有 512x614 像素。 每个像素均记录在 380 nm 至 2500 nm 的 224 个通道中。 光谱分辨率高达9.46nm。 本次提供的是裁剪版本大小事 100x100。 删除通道 1-3、108-112、154-166 和 220-224后，我们保留 198 个通道。 该数据中有四个潜在的端元：“#1 Road”、“#2 Soil”、“#3 Water”和“#4 Tree”。\nhdf5_path = './data/jasper_ridge.hdf5' \nfd = h5py.File(hdf5_path, 'r')\nprint(fd.keys())\n<KeysViewHDF5 ['abundance', 'end_name', 'endmember', 'image']>\n# hdf5格式可以存储多个数据类型，支持切片读取\ndata=np.array(fd['image']) #data=fd['image']或fd['image'][:,:,:20]此时读取数据并不占用内存，只有对数据操作后才会占用内存\nabn=np.array(fd['abundance'])\nend=np.array(fd['endmember'])\nlabel_bytes = fd['end_name'][:]\nlabel = np.array([s.decode('utf-8') for s in label_bytes])\nprint(label,end.shape,abn.shape,data.shape)\n['1-tree' '2-water' '3-dirt' '4-road'] (198, 4) (4, 100, 100) (198, 100, 100)\npc = principal_components(data.transpose((1,2,0)))\npcdata = pc.reduce(num=10).transform(data.transpose((1,2,0)))\nplot_2_img(pcdata[:,:,:3],np.argmax(abn,axis=0),' First 3 PCA','Abundance Map',cmap2='Blues_r')\n\nhdf5_path = './data/jasper_ridge_pca3.hdf5'\nwith h5py.File(hdf5_path, 'w') as f:\n    f['image'] = pcdata[:,:,:3]\n    f['class'] = np.argmax(abn,axis=0)\n    f.close()\nprint(\"HDF5文件已创建成功！\")\n\n\n6 常用高光谱数据集\nurl1:https://rslab.ut.ac.ir/data\nurl2:https://www.iotword.com/7117.html\n\n请关注微信公众号【45度科研人】获取更多精彩内容，欢迎后台留言！\n\n<img src=\"https://dunazo.oss-cn-beijing.aliyuncs.com/blog/wechat-simple.png\" style=\"margin-right:25px;width:200px;height:200px;\">"
  },
  {
    "objectID": "posts/2023-8-2hsispectral/index.html",
    "href": "posts/2023-8-2hsispectral/index.html",
    "title": "光谱库读写+光谱处理及交互式采集",
    "section": "",
    "text": "光谱库读写+光谱处理及交互式采集\n\n\n上一篇我们介绍了利用Python对不同格式的高光谱数据进行读写，本章将介绍如何从图像中交互式的采集光谱，sli格式光谱库的读写方式以及一阶微分、光谱平滑、多项式拟合、吸收深度计算等光谱处理方法。\n\n\n交互式工具的制作\n光谱库文件读写，头文件编辑等\n光谱一阶微分、平滑、多项式拟合、吸收深度计算等处理\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nimport os,glob,time\nimport spectral.io.envi as envi\nimport importlib\nfrom utils import * #之前分享过的function都放到了utils里，此文档不在展示，文末回复关键字获取\nimport sys,importlib\nimportlib.reload(sys.modules['utils'])# 在需要重新加载的地方调用 reload() 函数\n# !pip install ipywidgets\n# !jupyter nbextension enable --py widgetsnbextension\n<module 'utils' from 'x:\\\\WRSDP\\\\Second Week\\\\TASK2\\\\utils.py'>\n\n1. 光谱数据处理\n# 用sepctral库来加载sli光谱库数据\nCuprite_em=envi.open('./data/cuprite_end.hdr') # 读取cuprite端元光谱库\nspec_data=Cuprite_em.spectra.transpose()\nspec_meta = envi.read_envi_header('./data/cuprite_end.hdr') #读取光谱库的头文件\nspec_name=spec_meta['spectra names']\nspec_wl=spec_meta['wavelengths']\nshow_spectral(spec_data,spec_name,'Spectral Signature of Endmembers')\n\n\n\n2. 光谱分析\n光谱分析方法有很多，这里展示常用的一些方法\n# 吸收深度计算\nfrom scipy.signal import *\n\ndef depthcal(band,spec_wl,l, r,x=None):\n    if x==None:\n        x=np.argmin(band[l:r])+l\n    xwl, lwl, rwl=(spec_wl[x], spec_wl[l], spec_wl[r])\n    a = 1 - (rwl - xwl) / (xwl - lwl)\n    b = (rwl - xwl) / (xwl - lwl)\n    result = 1 - band[x] / (a * band[l] + b * band[r])\n    return result\n\nmus_wl =np.array( [float(x) for x in spec_wl])\nmul_data=spec_data[:,6]\nl=145\nr=165\nx=np.argmin(mul_data[l:r])+l\nspecdepth=depthcal(mul_data,mus_wl,l,r)\nprint(x,specdepth)\n\n# 利用scipy光谱平滑和微分运算\nsmooth_spec = savgol_filter(spec_data[:,6], window_length=25, polyorder=7)\nspec_data_diff = np.diff(spec_data[:,6], axis=0)\n157 0.28345667940108465\n# 结果可视化展示，为了方便对比，将平滑曲线与一阶微分曲线就做了y轴数值的偏移处理\nfig = plt.figure(figsize=(7, 4))\nplt.style.use('seaborn')\nplt.plot(smooth_spec-0.2, label='smooth spectrum', color='black')\nplt.plot(spec_data_diff+0.2, label='diff spectrum', color='orange')\nplt.plot(spec_data[:,6], label='original spectrum', color='darkgreen')\nplt.plot(range(l,r), spec_data[l:r,6], 'blue', label='interval spectrum') \nplt.fill_between(range(l,r),np.max(spec_data),  color='lightblue', alpha=0.3)\nplt.axvline(x=x, color='r', linestyle='--',label='absorption position')\nplt.xlabel('Bands', fontweight='bold', fontname='Times New Roman', fontsize=12)\nplt.ylabel('Reflectance', fontweight='bold', fontname='Times New Roman', fontsize=12)\nplt.title('Diagnostic Spectral Features')\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1))\nplt.show()\n\n# 多项式拟合光谱曲线\n\nfrom scipy.optimize import curve_fit\n\ndef fit_func(x, a, b, c):\n    return a * x**2 + b * x + c\nparams, params_cov = curve_fit(fit_func, np.arange(len(spec_wl)), spec_data[:,8])\npred=fit_func(np.arange(len(spec_wl)), *params)\n\nfig = plt.figure(figsize=(7, 4))\nplt.style.use('seaborn')\nplt.plot(spec_data[:,8], label='original spectrum', color='darkgreen')\nplt.plot(pred, 'orange', label='fitted spectrum') \nplt.xlabel('Bands', fontweight='bold', fontname='Times New Roman', fontsize=12)\nplt.ylabel('Reflectance', fontweight='bold', fontname='Times New Roman', fontsize=12)\nplt.title('Curve Fitting')\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1))\nplt.show()\n\n\n\n3. 影像光谱交互采集\n# 影像光谱采集\nimport numpy as np\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom IPython.display import display,HTML\nimport ipywidgets as widgets\n%matplotlib widget\n\ndef display_image_with_mouse_click(image,rgb):\n    # 存储点的坐标和通道像元值\n    selected_points = []\n    color_strings = ['#1f77b4', '#aec7e8', '#ff7f0e', '#ffbb78', '#2ca02c', '#98df8a', '#d62728', '#ff9896']\n\n    def on_click(trace, points, state):\n        nonlocal selected_points\n\n        if points.point_inds:\n            x_index = points.xs[0]\n            y_index = points.ys[0]\n\n            # 添加一个散点图\n            fig.add_trace(go.Scatter(\n                x=[x_index],  # 点的横坐标\n                y=[y_index],  # 点的纵坐标\n                mode='markers',\n                marker=dict(size=10, color=color_strings[len(selected_points)], opacity=1),\n            ))\n            fig.update()\n\n            # 记录点的坐标和通道像元值\n            x_value = int(x_index)\n            y_value = int(y_index)\n            pixel_values = image[y_value, x_value]\n            selected_points.append({'x': x_value, 'y': y_value, 'pixel_values': pixel_values})\n            # 更新多波段像元值曲线\n            display_pixel_values(fig_pixel_values, selected_points, rgb, color_strings)\n\n    # 绘制像元值曲线\n    def display_pixel_values(fig, points, rgb, colors):\n        if len(points) == 0:\n            return\n\n        for i, point in enumerate(points):\n            if len(fig.data) > i:\n                # 如果曲线已存在，则更新数据\n                fig.data[i].x = np.arange(len(point['pixel_values']))\n                fig.data[i].y = np.array(point['pixel_values'])\n                fig.data[i].marker.color = colors[i]\n            else:\n                # 如果曲线不存在，则添加新的曲线\n                fig.add_trace(go.Scatter(\n                    x=np.arange(len(point['pixel_values'])),\n                    y=np.array(point['pixel_values']),\n                    mode='lines',\n                    name='Point {}'.format(i+1),\n                    marker=dict(color=colors[i])\n                ))\n        fig.update()\n    rows, cols, channels = image.shape\n\n    fig = go.FigureWidget(data=go.Image(z=rgb, colormodel='rgb'))\n    fig.update_layout(title='Image',\n                      xaxis=dict(title='Columns'),\n                      yaxis=dict(title='Rows'),\n                      clickmode='event+select')\n    fig_pixel_values = go.FigureWidget()\n    fig_pixel_values.update_layout(title='Spectrum Collection',\n                          xaxis=dict(title='Band'),\n                          yaxis=dict(title='Reflectance'))\n    aspect_ratio = cols / rows  # 宽高比\n    if aspect_ratio > 1:\n        fig.update_layout(height=500, width=int(500 * aspect_ratio))\n    else:\n        fig.update_layout(height=int(500 / aspect_ratio), width=500)\n\n    fig.data[0].on_click(on_click)\n    # 设置鼠标悬停时显示的坐标信息\n    fig.update_traces(hovertemplate='X: %{x}<br>Y: %{y}')\n\n\n    fig_pixel_values.update_layout(height=500, width=800)\n\n    # 将FigureWidget放入HBox布局中\n    box_layout = widgets.Layout(display='flex', flex_flow='row', justify_content='space-between')\n    hbox = widgets.HBox([fig, fig_pixel_values], layout=box_layout)\n    display(hbox)\n    return selected_points\n# 定义读取文件\ndef read_tiff(file):\n    img_arr,meta = Load_image_by_Gdal(file)\n    if meta['img_bands'] > 1:\n        img_arr=img_arr.transpose(( 1, 2,0))\n    return img_arr,meta\n\nfilepath='./data/Cuprite_ref188.img'\nimg_arr,img_meta = read_tiff(filepath)\nprint(img_arr.shape,img_meta.keys())\n(500, 500, 188) dict_keys(['img_bands', 'geomatrix', 'projection', 'wavelengths', 'wavelength_units'])\n# 对图像进行增强处理\nrgb=linear_stretch(img_arr[:,:,[27,18,9]],True)*255\nselected_points = display_image_with_mouse_click(img_arr,rgb)\n# 通过交互式选点，可以更准确的获取影像光谱数据，图片可以放大缩小\n%matplotlib inline\nHBox(children=(FigureWidget({\n    'data': [{'colormodel': 'rgb',\n              'hovertemplate': 'X: %{x}<br>Y:…\n\n\n\n4. 光谱数据保存为光谱库\n# 采集的光谱数据进行保存\nspectrals=np.array([point['pixel_values'] for point in selected_points])\nprint(spectrals.shape)\nshow_spectral(spectrals.T)\n# 与刚刚光谱库中获取的个端元光谱进行整合，输出为一个样本库集合，注意不同的数据格式不统一，需要进行转换\nnewspec = np.stack((spec_data[:,6],smooth_spec-0.2,pred), axis=1)\nnewspec_lib=np.concatenate((newspec*10000,spectrals.T),axis=-1)\nshow_spectral(newspec_lib)\n\n\nspec_name=['original','smooth','fitting','select1','select2','select3','select4']\n\nem_meta={}\nem_meta['wavelengths']=spec_wl\nem_meta['wavelength units']='Micrometers'\nem_meta['spectra names'] = spec_name\n\nspec=envi.SpectralLibrary(newspec_lib.transpose(), em_meta)\nspec.save('./data/new_cuprite_end')\n\n请关注微信公众号【45度科研人】获取更多精彩内容，欢迎后台留言！\n\n<img src=\"https://dunazo.oss-cn-beijing.aliyuncs.com/blog/wechat-simple.png\" style=\"margin-right:25px;width:200px;height:200px;\">"
  },
  {
    "objectID": "posts/2023-8-11hsireduce/index.html#异常值剔除及拉伸显示",
    "href": "posts/2023-8-11hsireduce/index.html#异常值剔除及拉伸显示",
    "title": "高光谱数据去噪及降维处理",
    "section": "1 异常值剔除及拉伸显示",
    "text": "1 异常值剔除及拉伸显示\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nimport os,glob,time\nimport importlib\nfrom pysptools.noise import Whiten\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\nfrom utils import read_tiff,plot_2_img,get_hsi_by_wl,mnf_r,Write_Tiff #之前分享过的function都放到了utils里，此文档不再展示\nfilepath='./data/gaofen-5.img'\nimg_arr,img_meta = read_tiff(filepath)\nprint(img_arr.shape,img_meta.keys())\n(500, 400, 295) dict_keys(['img_bands', 'geomatrix', 'projection', 'wavelengths', 'wavelength_units'])\ndef stretch_2percent(data):\n    stretched_data = np.zeros(data.shape)\n    for i in range(data.shape[2]):\n        min_val = np.percentile(data[:, :, i], 2)\n        max_val = np.percentile(data[:, :, i], 98)\n        stretched_data[:, :, i] = (data[:, :, i] - min_val) / (max_val - min_val)\n    stretched_data = np.clip(stretched_data, 0, 1)\n    return stretched_data\ndef get_hsi_by_band(hsi_img, bands_to_get):\n    bands=np.array(bands_to_get)\n    hsi_out = hsi_img[:,:,[int(m) for m in bands]]\n    return np.array(hsi_out)\ndef get_rgb_array(hsi_img,rgblist,wavelengths=None,liner_trans=False):\n    n_row, n_col, n_band = hsi_img.shape\n    if wavelengths is not None:\n        red_wavelengths = list(range(619,659))\n        green_wavelengths = list(range(549,570))\n        blue_wavelengths = list(range(449,495))\n        RGB_img = np.zeros((n_row, n_col, 3))\n        RGB_img[:,:,0] = np.mean(get_hsi_by_wl(hsi_img, wavelengths, red_wavelengths), axis=2)\n        RGB_img[:,:,1] = np.mean(get_hsi_by_wl(hsi_img, wavelengths, green_wavelengths), axis=2)\n        RGB_img[:,:,2] = np.mean(get_hsi_by_wl(hsi_img, wavelengths, blue_wavelengths), axis=2)\n    else:\n        RGB_img =get_hsi_by_band(hsi_img, rgblist)\n    if liner_trans:\n        RGB_img =stretch_2percent(RGB_img)\n    return RGB_img\nwl=np.array(img_meta['wavelengths'], dtype=np.float32) \nrgb1=get_hsi_by_band(img_arr,[59,38,17])\nrgb2=get_rgb_array(img_arr,[59,38,17],None,liner_trans=True)\nplot_2_img(rgb1/np.max(rgb1),rgb2/np.max(rgb2),'Original','2% Linear stretch')"
  },
  {
    "objectID": "posts/2023-8-11hsireduce/index.html#距匹配降噪处理",
    "href": "posts/2023-8-11hsireduce/index.html#距匹配降噪处理",
    "title": "高光谱数据去噪及降维处理",
    "section": "2 距匹配降噪处理",
    "text": "2 距匹配降噪处理\nMoment Matching 是一种常用的图像去噪方法，其基本原理是根据图像的统计特性来对图像进行修正，以降低噪声的影响。首先计算图像的各个波段的均值和标准差，通过比较不同行列之间的差异，可以推断出噪声所引起的扰动。然后，通过调整图像的增益和偏移量，使得匹配后的均值和标准差能够更好地反映噪声的统计特性。通过 Moment Matching 方法，我们可以使图像恢复到更接近真实场景的状态，减少噪声对图像质量的影响。需要注意的是，Moment Matching 可能会引入一定的偏差，特别是对于强烈的噪声或者图像数值、为例差异较大的的情况。因此，在应用 Moment Matching 进行图像去噪时，需要考虑下垫面的组成和特点。\n\n增益：\\(\\text{gain} = \\frac{\\text{band\\_std}}{\\text{ns\\_std}}\\)\n偏移量：\\(\\text{offset} = \\text{bands\\_avg} - \\text{gain} \\times \\text{ns\\_avg}\\)\n去噪处理：\\(\\text{final\\_image}[i, j, b] = \\text{data}[i, j, b] \\times \\text{gain}[b] + \\text{offset}[b]\\)\n\n上述公式中，\\(\\text{bands\\_avg}\\) 表示各个波段的像素均值，\\(\\text{band\\_std}\\) 表示各个波段的像素标准差。\\(\\text{ns\\_avg}\\) 表示b波段各个列的像素均值，\\(\\text{ns\\_std}\\) 表示b波段各个列的像素标准差。\\(\\text{gain}\\) 表示增益，\\(\\text{offset}\\) 表示偏移量。\\(\\text{final\\_image}\\) 是去噪后的最终图像。\ndef moment_matching(data):\n    rows, cols, bands=data.shape\n    final_image = np.zeros([rows, cols,bands])\n    bands_avg = np.mean(data, axis=(0,1))\n    band_std = np.std(data, axis=(0,1))\n    # print(bands_avg.shape,band_std.shape)\n    ns_std = np.std(data, axis=(0,))  \n    ns_avg = np.mean(data, axis=(0,))\n    # print(ns_std.shape,ns_avg.shape)\n    gain =  np.broadcast_to(band_std, ns_std.shape)/ np.where(ns_std == 0, 1e-8, ns_std)\n    offset = np.broadcast_to(bands_avg, ns_avg.shape) - gain * ns_avg\n    for i in range(bands):\n        for j in range(cols):\n            final_image[:,j,i] = data[:,j,i] * gain[j,i] + offset[j,i]\n    return final_image\ndenoised_img=moment_matching(img_arr)\ni=-1\nplot_2_img(img_arr[:,:,i]/6000,denoised_img[:,:,i]/6000,\"Original\", \"Denoised data\",cmap1='gray_r',cmap2='gray_r')"
  },
  {
    "objectID": "posts/2023-8-11hsireduce/index.html#降维处理",
    "href": "posts/2023-8-11hsireduce/index.html#降维处理",
    "title": "高光谱数据去噪及降维处理",
    "section": "3 降维处理",
    "text": "3 降维处理\n\n3.1 主成分分析\n遥感影像的主成分分析（Principal Component Analysis, PCA）是一种常用的降维技术，用于提取遥感影像中的主要信息。它通过线性变换将多波段影像转换为新的特征空间，使得在新空间中的样本间的协方差最小化，可以在降低数据维度的同时保留尽可能多的信息。 。以下是遥感影像主成分分析的原理、主要过程和计算公式：\n主要过程：\n\n数据标准化：对原始图像进行均值中心化和标准差归一化，使不同波段之间具有相同的尺度\n\n\\(\\mathbf{X_i} = \\frac{\\mathbf{X_i} - \\mu_i}{\\sigma_i}\\)\n其中，\\(\\mathbf{X_i}\\) 是第 \\(i\\) 个波段的图像，\\(\\mu_i\\) 是第 \\(i\\) 个波段的均值，\\(\\sigma_i\\) 是第 \\(i\\) 个波段的标准差。\n\n协方差矩阵计算：计算标准化后的图像数据的协方差矩阵，用于度量不同波段之间的相关性\n\n\\(\\mathbf{C} = \\frac{1}{N-1} \\sum_{i=1}^{N}(\\mathbf{X_i}-\\boldsymbol{\\mu})(\\mathbf{X_i}-\\boldsymbol{\\mu})^T\\)\n其中，\\(\\mathbf{C}\\) 是协方差矩阵，\\(N\\) 是波段数，\\(\\mathbf{X_i}\\) 是第 \\(i\\) 个波段的标准化图像，\\(\\boldsymbol{\\mu}\\) 是均值向量。\n\n特征值分解：对协方差矩阵进行特征值分解，得到特征向量和特征值。特征向量代表了原始数据的主要方向，特征值表示数据在这些方向上的重要程度\n\n\\(\\mathbf{C} = \\mathbf{U} \\boldsymbol{\\Lambda} \\mathbf{U}^T\\)\n其中，\\(\\mathbf{U}\\) 是特征向量矩阵，\\(\\boldsymbol{\\Lambda}\\) 是对角特征值矩阵。\n\n选择主成分：选择特征值最大的前 \\(k\\) 个特征向量作为主成分，其中 \\(k\\) 是期望的降维后的维度。这些主成分按照特征值的降序排列\n\n\\(\\mathbf{W} = \\mathbf{U}(:, 1:k)\\)\n其中，\\(\\mathbf{W}\\) 是前 \\(k\\) 个最大特征值对应的特征向量组成的矩阵。\n\n变换到新的特征空间：将原始数据与选择的主成分相乘，得到在新的特征空间中的表示\n\n\\(\\mathbf{F} = \\mathbf{X} \\mathbf{W}\\)\n其中，\\(\\mathbf{F}\\) 是新的特征空间图像，\\(\\mathbf{X}\\) 是原始标准化后的图像数据。\n\n\nn_components = 50\npca = PCA(n_components)\nprincipalComponents = pca.fit_transform(img_arr.reshape(-1,img_arr.shape[-1]))\nimg_pc=principalComponents.reshape(img_arr.shape[:-1]+(n_components,))\nprint(img_pc.shape)\n(500, 400, 50)\nev=pca.explained_variance_ratio_\nfig = plt.figure(figsize=(8, 4))\nplt.style.use('seaborn')\nplt.plot(np.cumsum(ev))\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')\nplt.show()\n\ndef norm_data(data):\n    scaler = MinMaxScaler()\n    normalized_data_2d = scaler.fit_transform(data.reshape(-1, data.shape[-1]))\n    normalized_data = normalized_data_2d.reshape(data.shape)\n    return normalized_data\nnormalized_data=norm_data(img_pc[:,:,:4])\nfig, ax = plt.subplots(1,4, figsize=(10, 4))\nfor i in range(4):\n    ax[ i].imshow(normalized_data[:, :, i], cmap='Set3')\n    ax[ i].set_title('Band {}'.format(i+1))\n    ax[ i].axis('off')\nplt.tight_layout()  # 自动调整子图之间的间距\nplt.show()\n\n\n\n3.2 MNF变换\n最小噪声分离（Minimum Noise Fraction, MNF）是一种常用于遥感图像处理的特征提取方法。它通过将多波段图像转换为新的特征空间，将信号与噪声分离开来，以提高遥感图像的解译质量，有助于后续的分类、目标检测和监督分类等任务。MNF变换和主成分分析都能够实现高光谱数据的降维，且计算过程有许多相似之处，二者主要区别如下： - 目标：MNF变换旨在将遥感图像中的噪声和信号有效分离，并提取有效的信号成分。主成分分析旨在找到原始数据中的主要方向，并对主要方向进行重要性排序。 - 特征选择：在MNF变换中，特征向量是根据与噪声相关性的大小来选择的，以获得较低噪声的特征。而在主成分分析中，特征向量是根据其对应的特征值的大小来选择的，以表示数据中的主要方向。 - 协方差矩阵：在MNF变换中，计算的是协方差矩阵，用于描述图像数据之间的相关性和噪声特征。主成分分析也需要计算协方差矩阵，但其目的是为了找到数据的主要方向。 - 结果表达：MNF变换得到的结果是一组新的特征空间图像，其中每个特征都具有较低的噪声。主成分分析得到的结果是一组新的主成分，按照其对应的特征值的大小进行排序。\ndef bad_band_index(data, wl, wl_range1,wlrange2):\n    excluded_range_1 = np.logical_and(wl >= wl_range1[0], wl <= wl_range1[1])\n    excluded_range_2 = np.logical_and(wl >= wlrange2[0], wl <= wlrange2[1])\n    included_bands = np.logical_not(np.logical_or(excluded_range_1, excluded_range_2))\n    band_indices = np.where(included_bands)[0]\n    return band_indices\n\ndef MNF_func(data,n_components = 15):\n    num_dimensions = len(data.shape)\n    if num_dimensions == 2:\n        data = np.expand_dims(data, axis=0)\n    #check data shape\n    if data.shape[-1] <=n_components:\n        raise ValueError(\"For MNF,n_components must be smaller than spectral channels\")\n    mnf_result = mnf_r(data, n_components)\n    # Return result in dimensionality of input\n    if num_dimensions == 2:\n        return np.squeeze(mnf_result)\n    else:\n        return mnf_result    \nband_indices=bad_band_index(denoised_img,wl,[1200,1500],[1700,2000])\nclean_data,newwl=img_arr[:,:,band_indices],wl[band_indices]\nmnf_trans=MNF_func(clean_data,15)\nreduced_data=norm_data(mnf_trans[:,:,:3])\nplot_2_img(rgb2/np.max(rgb2),reduced_data,\"Original_data\", \"First 3 components in RGB\")\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\nWrite_Tiff(np.transpose(mnf_trans,(2,0,1)), './data/reduced.tiff', img_meta['geomatrix'], img_meta['projection'],  wavelengths=None, wavelength_units=None)\nmissing geomatrix or projection information.\n\n请关注微信公众号【45度科研人】获取更多精彩内容，欢迎后台留言！\n\n<img src=\"https://dunazo.oss-cn-beijing.aliyuncs.com/blog/wechat-simple.png\" style=\"margin-right:25px;width:200px;height:200px;\">"
  },
  {
    "objectID": "posts/2023-8-27numpyandnn/index.html#数据处理",
    "href": "posts/2023-8-27numpyandnn/index.html#数据处理",
    "title": "用Numpy拆解神经网络",
    "section": "2.1 数据处理",
    "text": "2.1 数据处理\n(X_train, Y_train), (X_valid, Y_valid) = mnist.load_data()\nprint(X_train.shape,Y_train.shape,X_valid.shape,Y_valid.shape)\n(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\nfig=plt.figure(figsize=(25,5))\nfor i in range(10):\n    plt.subplot(1,10,i+1)\n    plt.axis('off')\n    plt.imshow(X_train[i,:,:])\nfig.tight_layout()\nplt.show()\n\ndef onehot(targets, num):\n    result = np.zeros((num, 10))\n    for i in range(num):\n        result[i][targets[i]] = 1\n    return result\nX_train=X_train.reshape(60000, 28*28) / 255. \nX_valid=X_valid.reshape(10000, 28*28) / 255. \ny_train = onehot(Y_train, 60000) # (60000, 10)\ny_valid = onehot(Y_valid, 10000) # (10000, 10)\nprint(X_train.shape,y_train.shape,X_valid.shape,Y_valid.shape)\nprint(np.max(X_train),np.max(X_valid))\n(60000, 784) (60000, 10) (10000, 784) (10000,)\n1.0 1.0"
  },
  {
    "objectID": "posts/2023-8-27numpyandnn/index.html#训练模型",
    "href": "posts/2023-8-27numpyandnn/index.html#训练模型",
    "title": "用Numpy拆解神经网络",
    "section": "2.2 训练模型",
    "text": "2.2 训练模型\ndef calculate_accuracy(y_true, y_pred):\n    y_true_labels = np.argmax(y_true, axis=1)\n    y_pred_labels = np.argmax(y_pred, axis=1)\n    correct_predictions = np.sum(y_true_labels == y_pred_labels)\n    accuracy = correct_predictions / len(y_true)\n    return accuracy\n\ndef train(k_in, k_h, k_out, batch_size, lr, epochs):\n    nn = NN(k_in, k_h, k_out, batch_size, lr)     # 创建神经网络对象  \n    log = {'train_loss': [], 'train_acc': []} # 存储训练过程的损失和准确率\n    \n    # 开始训练循环\n    for epoch in range(epochs):\n        epoch_train_loss = 0.0\n        epoch_train_correct = 0\n\n        # 分批次训练\n        for i in range(0, X_train.shape[0], batch_size):\n            X = X_train[i:i + batch_size] # 获取当前批次的训练数据\n            y = y_train[i:i + batch_size] # 获取当前批次的标签\n            loss, y_pred = nn.forward(X, y) # 前向传播计算损失并获取预测结果\n            nn.backward()  # 反向传播更新参数\n            epoch_train_loss += loss # 累计当前批次内的损失率\n            epoch_train_correct += calculate_accuracy(y, y_pred) # 累计当前批次内和准确率\n\n        # 计算单个轮次内的平均损失和准确率\n        train_loss = epoch_train_loss / (X_train.shape[0] // batch_size)\n        train_acc = epoch_train_correct / (X_train.shape[0] // batch_size)\n\n        # 将训练过程的损失和准确率记录到日志中\n        log['train_loss'].append(train_loss)\n        log['train_acc'].append(train_acc)\n\n        print(\"Epoch:\", epoch, \"...................\", \"Loss： {:.3f}   Acc: {:.3f}\".format(train_loss, train_acc))\n    \n    np.savez(\"./nnmodel.npz\", w1=nn.W1, b1=nn.b1, w2=nn.W2, b2=nn.b2) # 保存训练后的参数\n    \n    # 返回训练过程的损失和准确率\n    return log\nk_in=X_train.shape[1]\nk_h=100\nk_out=len(np.unique(Y_valid))\nbatch_size=50\nepochs=30\nlr=0.5\nlog=train(k_in,k_h,k_out,batch_size,lr,epochs)\n(784, 100) (100,) (100, 10) (10,)\nEpoch: 0 ................... Loss： 16.001   Acc: 0.501\nEpoch: 1 ................... Loss： 6.050   Acc: 0.880\nEpoch: 2 ................... Loss： 4.432   Acc: 0.905\nEpoch: 3 ................... Loss： 3.831   Acc: 0.915\nEpoch: 4 ................... Loss： 3.466   Acc: 0.922\nEpoch: 5 ................... Loss： 3.201   Acc: 0.928\n......................................\ndef plot_log(log):\n    plt.style.use(\"ggplot\")\n    fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n    axs[0].plot(log['train_loss'], '#008000')\n    axs[0].set_title('Training Loss')\n    axs[0].set_xlabel('Epochs')\n    axs[0].set_ylabel('Loss')\n    axs[1].plot(log['train_acc'], '#008000')\n    axs[1].set_title('Training Accuracy')\n    axs[1].set_xlabel('Epochs')\n    axs[1].set_ylabel('Accuracy')\n    plt.tight_layout()  \n    plt.show()\nplot_log(log)"
  },
  {
    "objectID": "posts/2023-8-27numpyandnn/index.html#测试模型",
    "href": "posts/2023-8-27numpyandnn/index.html#测试模型",
    "title": "用Numpy拆解神经网络",
    "section": "2.3 测试模型",
    "text": "2.3 测试模型\ndef load_model(k_in,k_h,k_out,batch_size,lr):\n    r = np.load(\"./nnmodel.npz\")\n    nn = NN(k_in,k_h,k_out,batch_size,lr)\n    nn.W1 = r[\"w1\"]\n    nn.b1 = r[\"b1\"]\n    nn.W2 = r[\"w2\"]\n    nn.b2 = r[\"b2\"]\n    return nn\nmodel=load_model(k_in,k_h,k_out,batch_size,lr)\nval_pred = model.predict(X_valid)\nval_acc=calculate_accuracy(y_valid,val_pred)\nprint(\"Val Precison:\", val_acc)\n(784, 100) (100,) (100, 10) (10,)\nVal Precison: 0.9658\nfig = plt.figure(figsize=(10, 5))\nfor i in range(8):\n    plt.subplot(2, 4, i+1)\n    plt.axis('off')\n    plt.imshow(X_valid.reshape(10000, 28, 28)[i+20, :, :],cmap='gray')\n    plt.title(\"Pred:    {}\".format(int(np.argmax(model.predict(X_valid)[i+20]))))\nfig.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/2023-8-27numpyandnn/index.html#构建交互app",
    "href": "posts/2023-8-27numpyandnn/index.html#构建交互app",
    "title": "用Numpy拆解神经网络",
    "section": "2.4 构建交互app",
    "text": "2.4 构建交互app\nimport gradio as gr\n# 构建预测函数\ndef predict_minist(image):\n    normalized = np.array(image)/ 255.0\n    # np.save(\"image.npy\", normalized)\n    flattened = normalized.reshape(1, 784)\n    prediction = model.predict(flattened)\n    pred_index = int(np.argmax(prediction,axis=-1))\n    return pred_index\n\niface = gr.Interface(fn=predict_minist, inputs=gr.Sketchpad(), outputs=\"text\",\n                     title=\"Handwritten Digit Recognition\",\n                     description=\"Draw a digit and the model will predict the digit. Please draw the digit in the center of the canvas\")\n\niface.launch(height=550,width=\"100%\",show_api=False)\nRunning on local URL:  http://127.0.0.1:7888\n\nTo create a public link, set `share=True` in `launch()`.\n\n\n\n\n请关注微信公众号【45度科研人】获取更多精彩内容，欢迎后台留言！"
  },
  {
    "objectID": "talks/2023-12-16 HEJINGTALKS/main.html",
    "href": "talks/2023-12-16 HEJINGTALKS/main.html",
    "title": "’AI+遥感’技术地学应用实践与展望",
    "section": "",
    "text": "Slides:"
  },
  {
    "objectID": "talks/2023-12-16 HEJINGTALKS/main.html#slides",
    "href": "talks/2023-12-16 HEJINGTALKS/main.html#slides",
    "title": "’AI+遥感’技术地学应用实践与展望",
    "section": "",
    "text": "Slides:"
  },
  {
    "objectID": "index.html#recent-posts-posts-recentes",
    "href": "index.html#recent-posts-posts-recentes",
    "title": "Junchuan’s blog",
    "section": "",
    "text": "1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosts\n\n\nDeep leanring\n\n\nAPP\n\n\nTeaching\n\n\nWorkshop\n\n\n\n\n\n\nAug 27, 2023\n\n\nJunchuan Yu\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosts\n\n\nHyperspectral\n\n\nTeaching\n\n\nWorkshop\n\n\n\n\n\n\nAug 11, 2023\n\n\nJunchuan Yu\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosts\n\n\nHyperspectral\n\n\nTeaching\n\n\nWorkshop\n\n\n\n\n\n\nAug 2, 2023\n\n\nJunchuan Yu\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosts\n\n\nHyperspectral\n\n\nTeaching\n\n\nWorkshop\n\n\n\n\n\n\nJul 25, 2023\n\n\nJunchuan Yu\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosts\n\n\nDeep leanring\n\n\nAPP\n\n\n\n\n\n\nApr 19, 2023\n\n\nJunchuan Yu\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosts\n\n\nDeep leanring\n\n\nAPP\n\n\n\n\n\n\nApr 9, 2023\n\n\nJunchuan Yu\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosts\n\n\nDeep leanring\n\n\nAPP\n\n\n\n\n\n\nApr 2, 2023\n\n\nJunchuan Yu\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosts\n\n\nDeep leanring\n\n\nAPP\n\n\n\n\n\n\nMar 11, 2023\n\n\nJunchuan Yu\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\nChatGPT加持下的Net Bing，将带来一场新的技术风暴\n\n\n\nPosts\n\n\nTalks\n\n\nAPP\n\n\n\n\n\n\nMar 4, 2023\n\n\nJunchuan Yu\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosts\n\n\nDeep leanring\n\n\nAPP\n\n\n\n\n\n\nFeb 27, 2023\n\n\nJunchuan Yu\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\nWork with Remote Sensing Data in Python: Lesson 1-5-2\n\n\n\nPosts\n\n\nDeep leanring\n\n\nTeaching\n\n\nWorkshop\n\n\n\n\n\n\nFeb 10, 2023\n\n\nJunchuan Yu\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\nWork with Remote Sensing Data in Python: Lesson 1-5-1\n\n\n\nPosts\n\n\nDeep leanring\n\n\nTeaching\n\n\nWorkshop\n\n\nGEE\n\n\n\n\n\n\nFeb 6, 2023\n\n\nJunchuan Yu\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\nWork with Remote Sensing Data in Python: Lesson 1-4-2\n\n\n\nPosts\n\n\nDeep leanring\n\n\nTeaching\n\n\nWorkshop\n\n\n\n\n\n\nJan 29, 2023\n\n\nJunchuan Yu\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Google Earth Engine in python\n\n\n\nPosts\n\n\nDeep leanring\n\n\nTeaching\n\n\nGEE\n\n\n\n\n\n\nJan 27, 2023\n\n\nJunchuan Yu\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\nWork with Remote Sensing Data in Python: Lesson 1-4-1\n\n\n\nPosts\n\n\nDeep leanring\n\n\nTeaching\n\n\nWorkshop\n\n\n\n\n\n\nJan 18, 2023\n\n\nJunchuan Yu\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\nWork with Remote Sensing Data in Python: Lesson 1-3-2\n\n\n\nPosts\n\n\nDeep leanring\n\n\nTeaching\n\n\nWorkshop\n\n\n\n\n\n\nDec 31, 2022\n\n\nJunchuan Yu\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\nWork with Remote Sensing Data in Python: Lesson 1-3-1\n\n\n\nPosts\n\n\nDeep leanring\n\n\nTeaching\n\n\nWorkshop\n\n\n\n\n\n\nNov 17, 2022\n\n\nJunchuan Yu\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\nWork with Remote Sensing Data in Python: Lesson 3-3\n\n\n\nPosts\n\n\nDeep leanring\n\n\nTeaching\n\n\nWorkshop\n\n\n\n\n\n\nNov 15, 2022\n\n\nJunchuan Yu\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosts\n\n\nDeep leanring\n\n\nArt\n\n\n\n\n\n\nOct 8, 2022\n\n\nJunchuan Yu\n\n\n1 min\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n See all"
  },
  {
    "objectID": "talks/2023-12-16 HEJINGTALKS/index.html",
    "href": "talks/2023-12-16 HEJINGTALKS/index.html",
    "title": "’AI+遥感’技术地学应用实践与展望",
    "section": "",
    "text": "Slides:"
  },
  {
    "objectID": "talks/2023-12-16 HEJINGTALKS/index.html#slides",
    "href": "talks/2023-12-16 HEJINGTALKS/index.html#slides",
    "title": "’AI+遥感’技术地学应用实践与展望",
    "section": "",
    "text": "Slides:"
  },
  {
    "objectID": "posts/2024-1-15whale2023/index.html",
    "href": "posts/2024-1-15whale2023/index.html",
    "title": "AI+遥感”技术地学应用实践与展望",
    "section": "",
    "text": "2023 和鲸社区年度科研闭门会以“对话 AI for Science 先行者，如何抓住科研范式新机遇”为主题，邀请了多个领域的专家学者共同探讨人工智能在各自领域的发展现状与未来趋势。以《“AI+遥感”技术地学应用实践与展望》为题，中国自然资源航空物探遥感中心的教授级高工于峻川从遥感智能解译的现状与挑战、人工智能在传统遥感业务中的技术定位、“AI+遥感”技术在地学领域应用的基本范式，以及“AI+遥感”技术所面临的新机遇与挑战等方面进行了分享。\n\n\n\n\n\n\n2023 年，人工智能的发展呈现出前所未有的蓬勃态势，一系列新兴技术如ChatGPT 、SAM和NeRF等在各个领域取得了突破性进展，也为人工智能技术在遥感领域的应用提供了新的视野和启示。\n\n\n\n\n回顾 “AI+遥感”的技术演进历程，我们不难察觉到人工智能在遥感领域中的一些关键技术的发展一直紧随着计算机视觉和医学领域的步伐。此过程中经历了从追逐热点，备受争议到逐渐被广泛接受的阶段，目前人工智能技术已进入到遥感的各个细分领域进行逐步迭代。然而，“AI+遥感”技术在地球科学领域应用中依然面临着诸多问题和挑战，涵盖样本、模型以及平台等多个方面。值得注意的是，这些问题实际上也是人工智能在特定领域应用时普遍面临的共性难题。\n\n\n\n\n\n问题的核心很大程度源自于遥感数据的特殊性——它是一种拥有多模态且包含时间空间属性的数据,与自然图像既有相似之处又呈现独有特质；另一方面，在地学领域应用场景下专家知识如何与人工智能模型相结合也是一个难点。然而，除以上这些挑战之外，我们认为如何正确理解人工智能在传统应用场景中的技术定位，明确它的目标，方法和边界，是更为重要的。\n\n\n\n\n\n\n这里我结合滑坡隐患关联要素智能识别的案例来谈谈我们在实践中是如何做技术定位的。\n在2019年，得到部地勘司的支持，航遥中心主导进行了全国地质灾害隐患综合遥感识别工作。通过三年的不懈努力，成功完成了710个县高中易发区的隐患普查识别，共识别了2.7万多处疑似隐患。在取得应用成效的同时，我们初步构建了隐患综合遥感识别的技术体系。为了持续在全国范围内进行地灾隐患识别与动态监测，引入新的技术手段以提高效率成为势在必行的道路。在这样的背景下，依托国家重点研发计划项目（2021YFC3000400）我们启动人工智能相关研究工作。\n\n\n\n狭义上讲滑坡隐患是具有明显形变迹象并在近期内可能成灾的斜坡。概念里提到的“斜坡、形变迹象、成灾”正是通过遥感技术可观测到的隐患的表现特征，与葛大庆教授提出的“三形”理论相契合。因此，我们从遥感观测要素-隐患表现特征的关联关系出发，客观限定识别目标为“地质灾害隐患关联要素”，指的是遥感可观测的对于崩滑隐患的判识有直接或间接指示作用的特征要素或目标对象。这一概念的提出使我们在遵循已有地灾理论框架下，更好的聚焦研究目标。\n\n\n\n\n\n举两个具体的案例。\n第一个案例涉及 InSAR形变聚集区的智能识别，从图中可以看出主要目的就是把InSAR形变相位中的异常圈出来。数据中不仅存在噪声，异常形态特征复杂，因而采用传统的阈值分割、聚类分析等方法都难以有效地解决问题。然而，通过将原本的数据梯度异常提取问题视为异常形态特征的模式识别问题，利用深度学习技术就可以迎刃而解。方法上我们更聚焦于小尺度弱异常目标的捕捉以及克服数据中的噪声问题，同时专注于如何利用数据和模型交互式迭代的方式构建数据集以满足广域应用的需求。\n\n\n\n第二个案例涉及滑坡图谱特征识别。从表现特征和技术可行性出发，我们将滑坡类别进行重新定义，分为高辨识度和低辨识度两类滑坡。高辨识类型的目标图谱特征与背景差异大更易于区分，如震后型滑坡，这类滑坡识别难度相对低。我们在全球范围内充分搜集样本数据的基础上，利用大模型技术构建相对通用的识别模型，在不同区域通过微调的方式实现方法的泛化。针对这类目标的识别有两个关键点：一是好的模型的初始化；二是提升样本的多样性。\n\n\n低辨识度的滑坡通常是经过后期改造的老滑坡，其特征与周围背景相似，因而识别难度大。这类目标往往需要结合专家知识，根据前后缘、侧壁等特征进行判识。鉴于以上认识我们提出利用对比学习方法强化模型对于滑坡边缘与背景差异性特征的学习，从而提高识别的精度。同时，我们也对于DEM是否一定对滑坡识别有效？如何多源数据做滑坡识别采用数据融合还是特征融合？等问题进行了探讨。\n\n\n\n\n\n\n\n简单地总结，我们认为利用“AI+遥感”技术用于解决隐患遥感识别的问题，关键在于准确的技术定位。从图4可见，我们将隐患识别技术的发展概括为四个阶段，从人工解译，综合遥感识别，再逐步引入智能识别，我们当前的技术正处在第二阶段和第三阶段间过渡的阶段，短时间内无法实现隐患的智能识别，但可以通过对关联要素的智能识别来提升综合遥感识别的效率。因而，清晰地明确研究的目标和边界是开展创新的关键，一旦目标过高会导致研究不切实际。另外，直接将其他领域的方法或认识套用到现有问题上通常是不成功的。在将AI+遥感技术应用于某一具体场景时，深刻理解目标和和需求场景是至关重要的，因此，成功落地应用的核心是跨领域人才，而非单纯的算法。\n\n\n\n\n\n\n\n基于已有的研究经验，我们也探讨了“AI+遥感”技术在地学应用中可能得的基本范式。虽然是尚未成熟的构想，但我们仍希望为该领域贡献一些初步的思考，抛砖引玉。根据数据和应用尺度的不同，提出了四种范式：第一个是数据驱动；第二是“数据-知识”联合驱动；第三个是智能云计算；第四是智能交互。特别“数据-知识”联合驱动在许多地学场景中都可能发挥关键作用，我们也在这方面进行了深入的思考和整理。\n\n\n\n\n\n下面结合实际案例对几种范式做一个简单的介绍。 云检测是数据质检的基础步骤也是遥感数据应用的前提条件，属于典型的数据驱动场景。我们总结了在何种情况下可以采用大数据驱动的方法，并能够取得良好的效果，梳理出了三个条件：首先，目标识别的规则得是明确的，比如水体、道路，而对于前面提到的成矿预测和滑坡场景，需要更多专家知识的接入，因为规则相对没那么明确；第二，样本的获取是低成本、高效的；第三，目标特征在空间时间上的变化差异性不大。只要满足这三种条件，多数情况就可以采用数据驱动的方式来解决问题。\n\n\n\n但实际应用中，我们常常因为样本的不足而需要应用“数据-知识”联合驱动的方法。以高光谱分类应用为例，数据本身的稀缺性限定了我们无法使用大模型大数据的方式解决问题。这一案例中我们利用高光谱开展矿物提取或岩性分类应用，采用了自监督的技术挖掘数据本身的价值，将数据映射到高维的空间，并基于物理模型对高维空间的特征进行约束，从而实现在小样本下的半监督分类应用，同时模型具备较好的鲁棒性。\n\n\n\n\n\n另一方面，我们探讨了提高广域尺度应用效率的方法。在进行开展青藏高原湖泊遥感识别的应用时，我们一开始陷入了一个误区。由于广域尺度的遥感监测应用场景下，本地进行数据处理成本是高昂的，且难以实现实时监测，最初的方案是不可持续的。因此，我们进行了改进，利用云计算平台在数据和算力上的优势，将数据处理、样本生成和推理过程都迁移到云上进行，实现了青藏高原湖泊在线近实时的遥感监测。\n遥感智能解译领域当前备受瞩目。尽管有观点认为很多深度学习模型在推理后仍需大量人为修正，这使得技术难以真正落地。然而，我们认为智能交互式解译可能是AI在遥感领域实际应用的最终形态。近期备受关注的 SAM 为我们提供了一种创新思路，我们在其发布的首周就对遥感数据进行了测试，并提出了在遥感场景下推进智能交互解译的途径。通过将视觉大模型迁移到具体的任务场景中，结合人机交互的方式实现one-shot提取，应用中只需在目标上标记一个点，模型就能自动地识别出相似目标。在专家知识的引导下，可以持续优化模型，这实际上已经在很大程度上改变了我们对传统遥感信息提取的思维模式。\n\n\n\n\n\n\n\n\n\n最后谈一下机遇和挑战。 过去几年遥感数据分析处理的方式发生了很大变化，随着编程语言的普及，遥感数据的分析和应用越来越走向数字化、集群化、自动化；同时，产品、算法都在逐渐地走向云端。多模态遥感、AI 大模型、云计算等新技术为遥感应用带来了很多新机遇。同时也需要注意到，一些相对成熟应用场景中方法本身已不再关键，而算力和数据逐渐成为了限制发展的一个主要因素。\n\n\n\n\n\n遥感解译本身作为遥感技术的一个重要方面，经历了从人工目视解译到面向对象，再逐渐向智能化的方向发展的过程。我们认为实现真正的智能解译还需要经历三个阶段：智能信息提取、智能交互解译、智能分析决策。事实上，现有的技术已为这一目标的实现提供了条件，需要我们投入更多的人才和资源来实现。 另一方面，依托优秀AI计算平台，通过“1+N”一个平台多个服务端的方式可以更好地平衡科研和生产的需求。\n\n\n\n\n\n关于挑战，除了技术上挑战，我认为认知层面的挑战同样至关重要。当新技术进入到一个领域时，常常会经历一番试错的过程，而关于人工智能的很多误解都是在这个过程里产生的——有人认为人工智能无所不能，有人认为它只是“人工智障”。因此，我们鼓励大家理性客观地认识 AI ，用实践探索技术边界，才能更好的迎接人工智能将带来的挑战。同时，我们也希望大家更多的关注AI+遥感技术在地学领域的应用！\n\n\n\n<img src=\"https://dunazo.oss-cn-beijing.aliyuncs.com/blog/image%20(11).png\" style=\"margin-right:25px;width:200px;height:200px;\">\n\n本文内容已做精简，如需获取专家完整版视频实录，请扫码领取。\n\n请关注微信公众号【45度科研人】获取更多精彩内容，欢迎后台留言！\n\n<img src=\"https://dunazo.oss-cn-beijing.aliyuncs.com/blog/wechat-simple.png\" style=\"margin-right:25px;width:200px;height:200px;\">"
  }
]