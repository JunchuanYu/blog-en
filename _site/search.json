[
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n基于遥感指数提取水体\n\n\nWork with Remote Sensing Data in Python: Lesson 1-3-2\n\n\n\nDec 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n哨兵2号数据获取及处理\n\n\nWork with Remote Sensing Data in Python: Lesson 1-3-1\n\n\n\nNov 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n利用多光谱遥感数据进行地物分类\n\n\nWork with Remote Sensing Data in Python: Lesson 3-3\n\n\n\nNov 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAI眼中的诗意——AI绘画\n\n\n\nOct 8, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Junchuan’s blog",
    "section": "",
    "text": "0 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPosts\n\n\nDeep leanring\n\n\nArt\n\n\n \n\n\n\n\nOct 8, 2022\n\n\nJunchuan Yu\n\n\n0 min\n\n\n\n\n\n\nNo matching items\n\n\n See all/button>"
  },
  {
    "objectID": "index.html#recent-talks-palestras-recentes",
    "href": "index.html#recent-talks-palestras-recentes",
    "title": "Junchuan’s blog",
    "section": "Recent Talks / Palestras recentes",
    "text": "Recent Talks / Palestras recentes\n Next activities/Atividades futuras\n\n\n\n\n\n\n \n\n\n\nTalks\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentification of Correlation Factors of Hidden Geohazard via Deep Learning\n\n\nNational Key R&D Program (2022)\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n\n\nNo matching items\n\n\n See all"
  },
  {
    "objectID": "index.html#posts",
    "href": "index.html#posts",
    "title": "Junchuan’s blog",
    "section": "Posts",
    "text": "Posts\n\n\n\n See all posts"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Junchuan Yu",
    "section": "",
    "text": "Hi!\nMy name is Junchuan Yu, and I am a remote sensing researcher.\nI graduated from CUGB, and my research involves hyperspectral remote sensing, deep learning, system development and geological applications, etc.\nI am very happy to share the knowledge about data science and remote sensing with more people here."
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nIdentification of Correlation Factors of Hidden Geohazard via Deep Learning\n\n\nNational Key R&D Program (2022)\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publication/index.html",
    "href": "publication/index.html",
    "title": "Junchuan Yu",
    "section": "",
    "text": "Publication\n\n\n\nTotal: 40; First-author: 13; Citations:Google Scholar | Research Gate; Source code: GitHub\n\n \n\nJunchuan Yu, Liang Zhang, Qiang Li, et al., 3D autoencoder algorithm for lithological mapping using ZY-1 02D hyperspectral imagery: a case study of Liuyuan region.Journal of Applied Remote Sensing. 2021, 15(4): 42610. [link] [code] \nJunchuan Yu, Yichuan Li, Xiangxiang Zheng, et al., An effective cloud detection method for Gaofen-5 images via deep learning.Remote Sensing. 2020, 12(13): 2106. [link] [code] \nJunchuan Yu, Yichuan Li, Bokun Yan, et al., Comparison of GaoFen-5 hyperspectral and airborne hyperspectral imagery: case study of Gossan identification in Subei area.Seventh Symposium on Novel Photoelectronic Detection Technology and Applications. 2021, 11763: 1479-1483. [link] \nJunchuan Yu, Yichuan Li, Siqun Zheng, et al., Knowledge guided classification of airborne hyperspectral images with deep convolutional neural network.AOPC 2020: Optical Spectroscopy and Imaging, and Biomedical Optics. 2020, 11566: 67-71. [link] \nJunchuan Yu, Yichuan Li, Xiangxiang Xiangxiang, et al., Radiometric optimization of airborne hyperspectral imagery for large-scale geological applications.Sixth Symposium on Novel Optoelectronic Detection Technology and Applications. 2020, 11455: 905-910. [link] \nJunchuan Yu, Bokun Yan, Wenliang Liu, et al., Seamless Mosaicking of Multi-strip Airborne Hyperspectral Images Based on Hapke Model.International Conference on Sensing and Imaging. 2019,506:285-292. [link] \nJunchuan Yu and Bokun Yan, Efficient solution of large-scale domestic hyperspectral data processing and geological application.2017 International Workshop on Remote Sensing with Intelligent Processing (RSIP). 2017: 44565. [link] \nJunchuan Yu, Wenliang Liu, Bokun Yan, et al., Inversion of geochemical compositions of basalts based on field measured spectra.Remote Sensing for Land & Resources.2017,29(1): 158-163. [link] \nJunchuan Yu, Xuanxue Mo, Xuehui Yu, et al., Petrogenesis and geological implications of the Late Triassic potassic-ultrapotassic rocks in Changdu Block, northern segment of the Sanjiang area.Acta Petrologica Sinica. 2014, 30(11): 3334-3344. [link] \nJunchuan Yu, Xuanxue Mo, Yichuan Li, et al., The Cenozoic Basalts From Simao Microcontinent, Eastern Tibet Plateau: The Geochemical Characteristics and Tectonic Significance.Acta Geologica Sinica. 2013, 87:88-89. [link] \nJunchuan Yu, Xuanxue Mo, Xuehui Yu, et al., Petrogenesis of Late Triassic Volcanic Rocks from Changdu Microcontinent, NE Tibet (West China): Constraints from Geochemistry and Sr-Nd-Pb Isotopes.Acta Geologica Sinica. 2013, 87: 123-137. [link] \nJunchuan Yu, Xuanxue Mo, Xuehui Yu, et al., Geochemical characteristics and petrogenesis of Permian basaltic rocks in Keping area, Western Tarim basin: A record of plume-lithosphere interaction.Journal of Earth Science.2012, 23(4): 442-454. [link] \nJunchuan, Yu, Xuanxue, Mo, Guochen, Dong, et al., Felsic volcanic rocks from northern Tarim, NW China: Zircon U-Pb dating and geochemical characteristics.Acta Petrologica Sinica. 2011, 27(7): 2184-2194. [link]"
  },
  {
    "objectID": "publication/temp/index.html",
    "href": "publication/temp/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/temp/index.html",
    "href": "teaching/temp/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "talks/2022-rstudio-conf/index.html",
    "href": "talks/2022-rstudio-conf/index.html",
    "title": "Making awesome automations with GitHub Actions",
    "section": "",
    "text": "Event: rstudio::conf(2022)\nTalk video\nSlides"
  },
  {
    "objectID": "talks/2022-rstudio-conf/index.html#complementary-materials",
    "href": "talks/2022-rstudio-conf/index.html#complementary-materials",
    "title": "Making awesome automations with GitHub Actions",
    "section": "Complementary materials",
    "text": "Complementary materials\n\nBlog post: How important is GitHub Actions to RStudio/Posit? Code for a plot in my talk!\nBlog post: Introduction to GitHub Actions to R users\nBlog post: Monitoring quarto-dev repositories: Creating a workflow with GitHub Actions for R users\nGitHub repository: awesome-gha - Awesome GitHub Actions for R\nPublic workflow examples:\n\nsigbm - This repository has a workflow that downloads the excel file with public data related to mining dams in Brazil, every day, and stores it in the repository. There is no R code!\nnoticiasgov - This repository has a workflow that scrapes news from the official government of Sao Paulo page, and stores it in a CSV file."
  },
  {
    "objectID": "talks/2022-rstudio-conf/index.html#slides",
    "href": "talks/2022-rstudio-conf/index.html#slides",
    "title": "Making awesome automations with GitHub Actions",
    "section": "Slides",
    "text": "Slides\n\nSlides:"
  },
  {
    "objectID": "posts/lesson/main.html",
    "href": "posts/lesson/main.html",
    "title": "利用多光谱遥感数据进行地物分类",
    "section": "",
    "text": "利用多光谱遥感数据进行地物分类\n\n本课程目的是利用神经网络开展多光谱地物分类，学习从整幅卫星影像制作样本数据集，训练多分类神经网络，并实现对整景卫星数据的预测。\n运行该代码提前需要安装以下几个必要的库 1. numpy 2. tensorflow = 2.5 3. h5py = 3.1 4. Pillow = 8.4\n\nimport h5py\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.optimizers import RMSprop,Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau,CSVLogger\nfrom tensorflow.keras.models import load_model\nimport matplotlib.pyplot as plt\nimport time,glob\nfrom model import all_model\nfrom utils import *\n\n\n\n1. 加载并查看数据\n\n## 加载数据集，数据集维度为3840×3840×4，最后一个波段是真值标签，标签一共有六类，“”\nhdf5_path = \"./data/kaggle_14_3b_5c.hdf5\"\nfd = h5py.File(hdf5_path, 'r')\nfd.keys()\nimages=fd['image']\nlabels=fd['label']\nn_label=len(np.unique(labels)) #{0:'buiding',1:'Road', 2:'Tree',3: 'Crops',4:'Water'}\n## 该影像是反射率数据（通常数值在0-1之间），为了节省存储空间常将数值放大10000倍，保存为无符号整型数据\nprint(np.max(images),np.min(images),np.max(labels),np.min(labels))\nimages=np.array(images)\nlabels=np.array(labels)\nprint(images.shape,labels.shape)\n\n\n## 将整幅影像及标签数据打印出来,为了提升原始影像的显示效果，对原有数据进行拉伸处理 \n\ndef stretch_n(band, lower_percent=5, higher_percent=95): #5和95表示分位数\n    band=np.array(band,dtype=np.float32)\n    c = np.percentile(band, lower_percent)*1.0\n    d = np.percentile(band, higher_percent)*1.0       \n    band[band<c] = c\n    band[band>d] = d\n    out =  (band - c)  / (d - c)  \n    return out.astype(np.float32)\n\ndef adjust_contrast(data,n_band=3):    #通过循环对各个波段进行拉伸\n    data=np.array(data,dtype=np.float32)\n    for img in data:\n        for k in range(n_band):\n            img[:,:,k] = stretch_n(img[:,:,k])\n    return data\n\nnewimg=adjust_contrast(images.copy()) #该操作讲改变原始数据，因此用.copy，不对原始数据进行更改\nprint(np.max(images),np.max(newimg))\nshow_5_images(images/10000,labels)#plot函数要求数据为0-1之间的浮点型或0-255的8位整型数据\nshow_5_images(newimg,labels)\n\n\n\n2. 数据切片\n\n## 定义随机裁剪和顺序裁剪两种方式，顺序裁剪是按照固定步长沿行列循环裁剪，切片数是有限的，随机裁剪是以随机点为起始点裁剪，切片数可以是无限的且可自定义，后者的好处是可以通过增加算法约束label中某一类的数量来实现精准的样本获取。\ndef random_crop(image,crop_sz):\n    img_sz=image.shape[:2]\n    random_x = np.random.randint(0,img_sz[0]-crop_sz+1) ##生成随机点\n    random_y = np.random.randint(0,img_sz[1]-crop_sz+1)\n    s_img = image[random_x:random_x+crop_sz,random_y:random_y+crop_sz,:] ##以随机点为起始点生成样本框，进行切片\n    return s_img\n\ndef data_crop_random(img_arr,crop_sz,n_patch):   \n    c = img_arr.shape[-1]\n    data = np.zeros([n_patch, crop_sz, crop_sz, c])\n    for j in np.arange(n_patch):\n        image = random_crop(img_arr,crop_sz)\n        data[ j,:,:,:] = image\n    return data\ndef sequential_crop(imagearray,crop_sz,step=256):\n    data = []\n    x=0\n    row_num = ((imagearray.shape)[0] - step) // step  ##最多能裁剪几行 几列\n    col_num=((imagearray.shape)[1] - step) // step\n    x_start=0\n    y_start=0\n    for h in range(row_num):\n        for w in range(col_num):\n            crop_img = imagearray[crop_sz*h+y_start:crop_sz*(h+1)+y_start, crop_sz*w+x_start:crop_sz*(w+1)+x_start,:] ##行列循环，滑动窗口移动              \n            data.append(crop_img)\n            x=x+1\n    data=np.array(data)\n    return data\ndef data_crop(imagearray,crop_sz,stride,random=None,n_patch=250):   #设置random选项，用来切换是否采用随机裁切\n    data = []\n    for i in range(imagearray.shape[0]):\n        if random:\n            image=data_crop_random(imagearray[i,:,:,:],crop_sz,n_patch)\n        else:\n            image =sequential_crop(imagearray[i,:,:,:],crop_sz,stride)\n        if i == 0:\n            data  = image ##注意当i=0的时候需要将image赋给data，否则data依然是空，不可以进行concatnate\n        else:\n            data  = np.concatenate((data, image), axis = 0) \n        print(\"patch processing....:\"+str(i))\n    data=np.array(data,dtype=np.float32)\n    print(\"final processed:\"+str(i)+\"...No.:\"+str(data.shape[0]))    \n    return data\n\n\n## 合并images和labels方便切片\ndata_all = np.concatenate((newimg, np.expand_dims(labels,axis=-1)), axis = -1)\nstride=256\ncropsize=256\nall_patches=data_crop(data_all,cropsize,stride,random=False)##保留2景数据做测试\nprint(data_all.shape,all_patches.shape)\ni=0\n\n\n##调用utils中的plot_func查看数据与label是否对应,反复运行这个cell进行数据浏览i表示每次浏览跨越数据的个数\nplot_func(all_patches[i:i+20,:,:,:3],all_patches[i:i+20:,:,:,-1])\ni+=500\n\n\nall_patches=suffle_data(all_patches) #对数据进行打乱处理\n\n\n##可以选择将数据保存为h5文件，方便后续使用\nhdf5_path = './data/patches_rgb_4b_5c.hdf5' \nf = h5py.File(hdf5_path, mode='w')\nf['image'] = all_patches[:,:,:,0:-1]\nf['label'] = all_patches[:,:,:,-1]\nf.close()\n\n\n\n3. 模型训练\n\n# hdf5_path = \"./data/patches_rgb_4b_6c.hdf5\"\n# fd = h5py.File(hdf5_path, 'r')\n# fd.keys()\n# images=np.array(fd['image'])\n# labels=np.array(fd['label'])#3:tree,4:road,5:crop\n\n\n## 对数据进行归一化处理，并将label转成one-hot标签形式\n# n_label=5\ndef post_normalize_image(images,labels,n_label=n_label):\n    msk = label_hot(labels,n_label)\n    img = images\n    return img,msk\n# img,msk=post_normalize_image(images,labels,n_label)\nimg,msk=post_normalize_image(all_patches[:,:,:,0:-1],all_patches[:,:,:,-1],n_label)\n## 将数据集按照7:3进行划分\nxtrain,xtest,ytrain,ytest=train_test_split(img,msk,test_size=0.2,random_state=42)\ndel img,msk #如果数据较大可以在此删除降低内存\nprint(xtrain.shape,xtest.shape,ytrain.shape,ytest.shape)\ni=0\n\n\nplot_func(xtrain[i:i+20,:,:,:3],np.argmax(ytrain,axis=-1)[i:i+20:,:,:])\ni+=500\n\n\n## 设定必要的参数\nloss='categorical_crossentropy'\noptimizer=Adam()\nloss_weights=''\nmetrics=['accuracy']\nbatch_size=20\nepoch= 50\ninput_height=xtrain.shape[1]\ninput_width=xtrain.shape[2]\nnchannel=xtrain.shape[-1]\nnum_train=xtrain.shape[0]\nnum_val=xtest.shape[0]\nATM=all_model(loss,loss_weights,optimizer,metrics,input_height,input_width,n_label,nchannel) #向all_model传参返回模型\nmodelname='unet'\n\n\n## 加载UNET模型\nmodel=ATM.UNET()#deeplabv3p,UNET,dlinknet,convattunet\n# model.summary()\n\n\n## 开始训练\nbegin_time = time.time()          \nout_dir = \"./checkpoint/\"+modelname+'/'\nif not os.path.exists(out_dir):\n    os.makedirs(out_dir) \n## 设置model-checkpoint用来存储模型参数文件\nmodel_checkpoint = ModelCheckpoint(filepath=out_dir+\"-{epoch:02d}e-val_loss{val_loss:2f}.hdf5\",monitor=\"val_loss\",save_best_only=True,mode='auto')\n## 设置csvlogger用来记录训练记录\ncsvlogger =CSVLogger(filename=out_dir+modelname+'-'+str(epoch)+'-log.csv', separator=',', append=False)\n## model_checkpoint和csvlogger要想发挥作用必须放入callback中\nresult=model.fit(xtrain, ytrain, batch_size=batch_size, epochs=epoch, verbose=1, shuffle=True,validation_data=(xtest, ytest),callbacks=[model_checkpoint,csvlogger])\nend_time = time.time()\nprint('time lapsing {0} s \\n'.format(end_time - begin_time))\n\n\n## 打印训练曲线，确认训练效果，精度不够，loss不收敛，模型学习能力不足且容易过拟合\ndef plot_fig(H,outdir):\n    N=len(H.history['loss'])\n    plt.style.use(\"ggplot\")\n    plt.figure(figsize=(10,6))\n    plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n    plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n    plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n    plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n    plt.ylim(0,1)\n    plt.title(\"Training Loss and Accuracy\")\n    plt.xlabel(\"Epoch #\")\n    plt.ylabel(\"Loss/Accuracy\")\n    plt.legend(loc=\"lower left\")\n    plt.savefig(outdir)\nplot_fig(result,modelname+\"_Loss_Acc_epoch.png\")\n\n\n\n4. 评价测试\n\n## 训练过程只保留最有性能参数文件，因此从训练记录里选择最后一个即可\nh5=glob.glob(\"./checkpoint/\"+modelname+\"/*.hdf5\")\nmodel_list=np.sort(h5)\nmodel=load_model(model_list[-1])\n# model=load_model(model_list[-1],custom_objects={'interpolation':interpolation})#keras导入模型需要判断是会否有自定义函数或层，有的话需要在custom_objects中定义，并编译\nprint(model_list[-1])\n\n\n## 首先对切片数据进行预测，查看效果\ni=100\npred=model.predict(xtest,batch_size=64)\npred=np.argmax(pred,axis=-1)\ngt=np.argmax(ytest,axis=-1)\n\n\n##逐批次查看预测效果\nval_plot_func(xtest[i:i+20],gt[i:i+20],pred[i:i+20])\ni+=100\n\n\n## 用测试集评价模型精度\ndf = call_matric(pred,gt, [0,1,2,3,4, 'all'])\nprint(df)\n\n\n\n4. 优化改进\n\n4.1 数据优化\n\nbuild_num = np.sum(labels ==0)\nroad_num = np.sum(labels == 1)\ntree_num = np.sum(labels == 2)\ncrop_num = np.sum(labels == 3)\nwater_num = np.sum(labels == 4)\n# 这两行代码解决 plt 中文显示的问题\nplt.rcParams['font.sans-serif'] = ['SimHei']\nplt.rcParams['axes.unicode_minus'] = False\nplt.style.use(\"ggplot\")\n\nclasses = ('0-Building', '1-Road', '2-Tree', '3-Crop', '4-Riveer')\nnumbers = [build_num,road_num, tree_num,crop_num, water_num]\nprint(numbers)\nplt.barh(classes, numbers,color='lightblue')\nplt.title('Number of pixels in each category')\n# plt.savefig(\"Number-category.png\", dpi = 600, bbox_inches=\"tight\")\nplt.show()\n\n\n## 定义随机裁剪增加对于label中1和4样本的采集,\"num_count(image[:,:,-1],1)\"表示切片中数值为1的像元个数\ndef data_crop_random2(img_arr,crop_sz,n_patch):   \n    data =[]\n    k=0\n    for j in np.arange(1000):\n        image = random_crop(img_arr,crop_sz)\n        if num_count(image[:,:,-1],1) +num_count(image[:,:,-1],4) >8000:\n            data.append(image)\n            k+=1\n            if k==n_patch:\n                break                 \n    if k == 0:\n        data  = np.expand_dims(image,axis=0) ##注意如果k=0，即没有符合条件的数据将最后一个image赋给data，避免data为空\n    else:\n        data  = np.array(data,dtype=np.float32)\n\n    print(data.shape)\n    return data.astype(np.float32)\ndef data_crop2(imagearray,crop_sz,stride,random=None,n_patch=250):   \n    data = []\n    for i in range(imagearray.shape[0]):\n        if random:\n            image=data_crop_random2(imagearray[i,:,:,:],crop_sz,n_patch)\n        else:\n            image =sequential_crop(imagearray[i,:,:,:],crop_sz,stride)\n        if i == 0:\n            data  = image ##注意当i=0的时候需要将image赋给data，否则data依然是空，不可以进行concatnate\n        else:\n            data  = np.concatenate((data, image), axis = 0) \n        print(\"patch processing....:\"+str(i))\n    data=np.array(data,dtype=np.float32)\n    print(\"final processed:\"+str(i)+\"...No.:\"+str(data.shape[0]))    \n    return data\n\n\n## 同样使用前面14幅影像进行切片，增加不平衡样本数据的采集\n# data_all = np.concatenate((newimg, np.expand_dims(labels,axis=-1)), axis = -1)\n# stride=256\n# cropsize=256\nall_patches2=data_crop2(data_all,cropsize,stride,random=True)\nprint(data_all.shape,all_patches2.shape)\ni=0\n\n\nall_patches2=suffle_data(all_patches2)# 对新的数据集进行随机打乱\n\n\n# plot_func(all_patches2[i:i+20,:,:,:3],all_patches2[i:i+20:,:,:,-1])\n# i+=500\n\n\n## 加载前面生成的切片数据\n# hdf5_path = \"./data/patches_rgb_4b_5c.hdf5\"\n# fd = h5py.File(hdf5_path, 'r')\n# fd.keys()\n# images2=np.array(fd['image'])\n# labels2=np.array(fd['label'])\n\n\n## 对两次切片数据进行合并，得到新的数据集\nnewimages=np.concatenate((images2,all_patches2[:,:,:,0:-1]),axis=0)\nnewlabels=np.concatenate((labels2,all_patches2[:,:,:,-1]),axis=0)\nprint(newimages.shape,newlabels.shape)\n\n\n##可以选择将数据保存为h5文件，方便后续使用\nhdf5_path = './data/patches2_rgb_4b_5c.hdf5' \nf = h5py.File(hdf5_path, mode='w')\nf['image'] = newimages\nf['label'] = newlabels\nf.close()\n\n\n# hdf5_path = './data/patches2_rgb_4b_5c.hdf5' \n# fd = h5py.File(hdf5_path, 'r')\n# fd.keys()\n# newimages=np.array(fd['image'])\n# newlabels=np.array(fd['label'])#3:tree,4:road,5:crop\n\n\n## 对数据进行归一化处理，并将label转成one-hot标签形式\nn_label=5\ndef post_normalize_image(images,labels,n_label=n_label):\n    msk = label_hot(labels,n_label)\n    img = images\n    return img,msk\nimg,msk=post_normalize_image(newimages,newlabels,n_label)\n# 将数据集按照7:3进行划分\nxtrain,xtest,ytrain,ytest=train_test_split(img,msk,test_size=0.2,random_state=42)\ndel img,msk #如果数据较大可以在此删除降低内存\nprint(xtrain.shape,xtest.shape,ytrain.shape,ytest.shape)\ni=0\n\n\nplot_func(xtrain[i:i+20,:,:,:3],np.argmax(ytrain,axis=-1)[i:i+20:,:,:])\ni+=500\n\n\n\n4.2 模型优化\n\n## 计算真值标签中各个类别的占比，作为损失函数的权重，权重值越大模型识别错误代价越大一定程度缓解数据不平衡问题。\n# from sklearn.utils.class_weight import compute_class_weight\n# classes = np.unique(labels)  \n# class_weight = compute_class_weight(class_weight='balanced', classes=classes, y=labels.reshape(-1))\nclass_weight=np.array([0.35,4.48,2.07,0.68,28.55])\nprint(class_weight)\n\n\n## 采用带有权重的交叉熵损失函数\nfrom keras import backend as K\nimport tensorflow as tf\ndef weighted_categorical_crossentropy(weights):\n    \"\"\"\n    Usage:\n        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n        loss = weighted_categorical_crossentropy(weights)\n    \"\"\"\n    weights = K.variable(weights)\n    def loss(y_true, y_pred):\n        # scale predictions so that the class probas of each sample sum to 1\n        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n        # clip to prevent NaN's and Inf's\n        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n        # calc\n        loss = y_true * K.log(y_pred) * weights\n        loss = -K.sum(loss, -1)\n        return loss\n    return loss\n\n\n## 设定必要的参数\nloss=weighted_categorical_crossentropy(class_weight)\noptimizer=Adam()\nloss_weights=''\nmetrics=['accuracy']\nbatch_size=10\nepoch= 50\ninput_height=xtrain.shape[1]\ninput_width=xtrain.shape[2]\nnchannel=xtrain.shape[-1]\nnum_train=xtrain.shape[0]\nnum_val=xtest.shape[0]\nATM=all_model(loss,loss_weights,optimizer,metrics,input_height,input_width,n_label,nchannel)\nmodelname='convattunet'\n\n\n## unet下采样操作较多导致细小线状地物信息丢失，新的网络减少下采样，且在decoder部分采用注意力机制提升浅层特征的权重\nmodel=ATM.convattunet()#deeplabv3p,UNET,dlinknet,convattunet\n# model.summary()\n\n\n## 开始训练\nbegin_time = time.time()          \nout_dir = \"./checkpoint/\"+modelname+'/'\nif not os.path.exists(out_dir):\n    os.makedirs(out_dir) \nmodel_checkpoint = ModelCheckpoint(filepath=out_dir+\"-{epoch:02d}e-val_loss{val_loss:2f}.hdf5\",monitor=\"val_loss\",save_best_only=True,mode='auto')\ncsvlogger =CSVLogger(filename=out_dir+modelname+'-'+str(epoch)+'-log.csv', separator=',', append=False)\nresult=model.fit(xtrain, ytrain, batch_size=batch_size, epochs=epoch, verbose=1, shuffle=True,validation_data=(xtest, ytest),callbacks=[model_checkpoint,csvlogger])\nend_time = time.time()\nprint('time lapsing {0} s \\n'.format(end_time - begin_time))\n\n\n## 训练时长较短，模型为达到收敛因此最高精度不是很高，但训练曲线和验证曲线趋势十分吻合，且loss有明显的降低，表明模型性能有提升\nplot_fig(result,modelname+\"_Loss_Acc_epoch.png\")\n\n\nh5=glob.glob(\"./checkpoint/\"+modelname+\"/*.hdf5\")\nmodel_list=np.sort(h5)\n# model=load_model(model_list[-1])\nmodel=load_model(model_list[-1],custom_objects={'loss':weighted_categorical_crossentropy}) #loss作为自定义层需要指出\nprint(model_list[-1])\n\n\n## 首先对切片数据进行预测，查看效果\ni=100\npred=model.predict(xtest,batch_size=64)\npred=np.argmax(pred,axis=-1)\ngt=np.argmax(ytest,axis=-1)\n\n\nval_plot_func(xtest[i:i+20],gt[i:i+20],pred[i:i+20])\ni+=20\n\n\n## 评估结果发现1类和4类地物较之前都有精度的提升，整体miou提升16%\ndf = call_matric(pred,gt, [0,1,2,3,4,'all'])\nprint(df)\n\n\n\n4.3 整景影像的预测\n\n## 加载整景的影像进行测试\ntest_data  = h5py.File('./data/kaggle_test1_3b_5c.hdf5', 'r')\ntestimg = np.array(test_data['image'])\ntestlabel=np.array(test_data['label'])\nprint(testimg.shape,testlabel.shape)\n\n\n## 与训练数据采用相同的预处理方式\nimage=adjust_contrast(testimg)\nnp.max(image),np.max(testimg)\n\n\n## 首先对影像做padding，保证其能够被crop_size整除，先沿着行列分别裁切样本，再统一进行预测，预测后数据按照原来的顺序再排列组合复原。需要注意的是这里采用的是膨胀预测的方法，喂给模型用来预测的切片大小是256，但放的时候只保留了中间的128×128，四周数据可靠度低，直接废弃\ndef center_predict(img,model,batch_size,n_label,strides=128,img_size=256):\n    corner_size=int(0.25*img_size)\n    h,w,c = img.shape\n    padding_h = (h//strides + 1) * strides+corner_size+corner_size\n    padding_w = (w//strides + 1) * strides+corner_size+corner_size\n    \n    padding_img = np.zeros((padding_h,padding_w,c),dtype=np.float16)\n    padding_img[corner_size:corner_size+h,corner_size:corner_size+w,:] = img[:,:,:]\n    mask_whole = np.zeros((padding_h,padding_w,n_label),dtype=np.float16)\n    crop_batch=[]\n    for i in range(h//strides+1):\n        for j in range(w//strides+1):\n            crop_img = padding_img[i*strides:i*strides+img_size,j*strides:j*strides+img_size,:]\n            ch,cw,c = crop_img.shape\n            \n            if ch != img_size or cw != img_size:\n                continue\n            crop_batch.append(crop_img)\n            \n    crop_batch=np.array(crop_batch)\n    start_time=time.time()\n    pred=model.predict(crop_batch,batch_size=batch_size)\n\n    for i in range(h//strides+1):\n        for j in range(w//strides+1):\n            mask_whole[i*strides+corner_size:i*strides+img_size-corner_size,j*strides+corner_size:j*strides+img_size-corner_size] = pred[(i+1-1)*(w//strides+1)+(j+1)-1,corner_size:img_size-corner_size,corner_size:img_size-corner_size]\n    score = mask_whole[corner_size:corner_size+h,corner_size:corner_size+w]\n    end_time=time.time()\n    print('pred_time:',end_time-start_time)\n    return score\n\n\nh_pred = center_predict(image[0],model,32,n_label)\nh_pred_mask=np.argmax(h_pred, axis = -1)\nprint(h_pred.shape,testlabel[0].shape)\n\n\nfig=plt.figure(figsize=(20,20)) \nplt.subplot(1,2,1)\nplt.imshow(testlabel[0,:,:])\nplt.subplot(1,2,2)\nplt.imshow(h_pred_mask)\nplt.show()"
  },
  {
    "objectID": "talks/2022-NKRDP-DL/index.html",
    "href": "talks/2022-NKRDP-DL/index.html",
    "title": "Identification of Correlation Factors of Hidden Geohazard via Deep Learning",
    "section": "",
    "text": "Slides:"
  },
  {
    "objectID": "talks/2022-NKRDP-DL/index.html#complementary-materials",
    "href": "talks/2022-NKRDP-DL/index.html#complementary-materials",
    "title": "Identification of Correlation Factors of Hidden Geohazard via Deep Learning",
    "section": "Complementary materials",
    "text": "Complementary materials\n\nBlog post: How important is GitHub Actions to RStudio/Posit? Code for a plot in my talk!\nBlog post: Introduction to GitHub Actions to R users\nBlog post: Monitoring quarto-dev repositories: Creating a workflow with GitHub Actions for R users\nGitHub repository: awesome-gha - Awesome GitHub Actions for R"
  },
  {
    "objectID": "talks/2022-NKRDP-DL/index.html#slides",
    "href": "talks/2022-NKRDP-DL/index.html#slides",
    "title": "Identification of Correlation Factors of Hidden Geohazard via Deep Learning",
    "section": "Slides",
    "text": "Slides\n\nSlides:"
  },
  {
    "objectID": "talks/2022-NKRDP-DL/index.html#section",
    "href": "talks/2022-NKRDP-DL/index.html#section",
    "title": "Identification of Correlation Factors of Hidden Geohazard via Deep Learning",
    "section": "```",
    "text": "```"
  },
  {
    "objectID": "posts/2022-01/article.html",
    "href": "posts/2022-01/article.html",
    "title": "Junchuan Yu",
    "section": "",
    "text": "AI眼中的诗意——AI绘画\n\n1. AI绘画大火\n\n2016 年，Memo Akten 和 Mike Tyka 等人工智能艺术家在在旧金山展出了由谷歌的 Deep Dream 算法生成的图像。短短两年时间，人工智能艺术作品登上了世界艺术舞台。\n2018 年，一幅由人工智能生成的图像在佳士得拍卖，这是一幅由巴黎的 Obvious 集体制作的渲染版画。埃德蒙·贝拉米肖像拍卖是第一次被广泛报道的人工智能艺术品拍卖。\n\n\n\n2022年被称为AI绘画元年。美国科罗拉多州博览会的一项美术比赛上，一等奖竟然颁给了AI自动生成的画作。作品全名《太空歌剧院》，由一位名叫Jason Allen的游戏设计师提交，作品利用AI软件 MidJourney，经过了近900次的迭代，数周的挑选与调整，才最终被打印到画布上，参与了这次比赛。 \n无独有偶，2022年戛纳电影短片节的最佳短片，也颁给了AI人工智能生成的作品《THE CROW》（乌鸦）\nDALL-E 2、Midjourney和StableDiffusion等AI绘画工具，让业余人士只要简单打几个关键字，就能够创作出复杂、或抽象、或写实的艺术作品。下面是一幅有AI生成的中国山水画。 \n\n\n\n2. AI眼中的诗意\n\nAI能理解我们的古诗吗？以滕王阁序为例，看看AI眼中“落霞与孤鹜齐飞，秋水共长天一色”这两句优美的诗句是怎样的。下面是由AI给出的部分答案：\n\n\n\n\n\n\n\n\n\n3.AI绘画原理\n\nDiffusionModel已取代GAN称为图像生成及AI绘画领域的新宠儿，引发了扩散模型的研究热潮，目前采用的扩散模型大都是来自于2020年的的工作DDPM: Denoising Diffusion Probabilistic Models。简单来说，扩散模型包含两个过程：前向扩散过程和反向生成过程，前向扩散过程是对一张图像逐渐添加高斯噪音直至慢慢抹去图像中所有可辨别的细节，直到变成纯粹的“噪点”，而反向生成过程是去噪音过程，逐渐对图像进行去像素化来学习逆转噪声并恢复图像。  \n正如谷歌解释的那样：\n\n\n“Running this reversed corruption process synthesizes data from pure noise by gradually denoising it until a clean sample is produced.”\n\n\n训练模型的核心数据集则是 LAION-high-resolution 和 LAION-Aesthetics。使用 AWS 提供的 4000 块 A100 显卡组成的强力计算集群，花费约 15 万小时的训练完成了第一个版本。具体技术细节请移步：Stable_diffusion, diffusion_model\n如何深入学习扩散模型的原理？fast.ai将和 Huggingface, Stability.ai等各方一起创作一门新课程，叫做 From Deep Learning Foundations to Stable Diffusion。这门课将会用新的方式让普通人从原理上理解 Stable Diffusion 模型。课程详细介绍请移步：传送门\n\n\n\n4. 人们对于AI绘画的看法？\n\n不少艺术家认为AI绘画这是一种作弊：“我们正在目睹艺术的消亡。如果创造性的工作在机器面前都不安全，那么即使你有高技能，也有被淘汰的危险。到那时，我们将拥有什么？”\n网友更是直言：“从AI目前的进度来看，行业被挤压是迟早的事情了。那些嘲讽人类艺术家的人，当你被替代感到危机的时候，你一样会发出无能的怒吼”\n懂得灵活运用AI绘画作为创作工具的艺术家们认为：“这项技术具有改变我们交流方式的巨大潜力，我们期待与大家一起建立一个更快乐、更具交流性和创造性的未来”\n除了图像生成之外，复杂的 AI 生成视频模型也已出现。不久前，Meta推出了AI系统“Make-a-Video”，可以由文本、图像生成短视频，也可以改变现有视频的细节来生成新的视频。现在，谷歌推出了新的视频生成模型“Imagen Video”，可实现每秒24帧的速度下达到1280 x 768像素的高清视频，这又会对当下火爆的对短视频领域带来哪些机遇和挑战呢？  ### 5. 总结\nAI绘画取得的效果是令人惊叹的；\n模型本身还是依然于海量的数据集，而这些数据库中所包含的图像和文字还是不全面的，因而无法解析数据库之外的词句以及绘画风格，比如“齐天大圣”，“岩彩画”。长远来看其对于日常实物基本可以做到准确的模拟，但对于场景和复杂语义的理解需要更多的努力，其中一个必要的途径是构建更为广泛的且细粒化的数据集，也许不久能够实现AI利用互联网资源进行自学习；\nAi绘画成功的背后是扩散模型，而扩散模型的思路是可以用到其他领域的，比如同样拥有海量数据及视觉属性的遥感领域，试想一下利用遥感数据对扩散模型进行迁移学习能否实现遥感场景的变换，这种新的数据生成技术能否为小样本识别问题提供解决方案呢？\nAI绘制“落霞与孤鹜齐飞，秋水共长天一色”图片下载链接：百度云, 提取码：1234 #### References\n《THE CROW》: https://www.bilibili.com/video/BV16P411V7Ah?share_source=copy_web\nStable_diffusion: https://huggingface.co/blog/stable_diffusion\ndiffusion_model: https://theaisummer.com/diffusion-models/\n传送门: https://www.fast.ai/posts/part2-2022.html"
  },
  {
    "objectID": "posts/lesson/index.html",
    "href": "posts/lesson/index.html",
    "title": "利用多光谱遥感数据进行地物分类",
    "section": "",
    "text": "利用多光谱遥感数据进行地物分类\n\n本课程目的是利用神经网络开展多光谱地物分类，学习从整幅卫星影像制作样本数据集，训练多分类神经网络，并实现对整景卫星数据的预测。\n运行该代码提前需要安装以下几个必要的库 1. numpy 2. tensorflow = 2.5 3. h5py = 3.1 4. Pillow = 8.4\n\nimport h5py\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.optimizers import RMSprop,Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau,CSVLogger\nfrom tensorflow.keras.models import load_model\nimport matplotlib.pyplot as plt\nimport time,glob\nfrom model import all_model\nfrom utils import *\n\n\n\n1. 加载并查看数据\n\n## 加载数据集，数据集维度为3840×3840×4，最后一个波段是真值标签，标签一共有六类，“”\nhdf5_path = \"./data/kaggle_14_3b_5c.hdf5\"\nfd = h5py.File(hdf5_path, 'r')\nfd.keys()\nimages=fd['image']\nlabels=fd['label']\nn_label=len(np.unique(labels)) #{0:'buiding',1:'Road', 2:'Tree',3: 'Crops',4:'Water'}\n## 该影像是反射率数据（通常数值在0-1之间），为了节省存储空间常将数值放大10000倍，保存为无符号整型数据\nprint(np.max(images),np.min(images),np.max(labels),np.min(labels))\nimages=np.array(images)\nlabels=np.array(labels)\nprint(images.shape,labels.shape)\n\n\n## 将整幅影像及标签数据打印出来,为了提升原始影像的显示效果，对原有数据进行拉伸处理 \n\ndef stretch_n(band, lower_percent=5, higher_percent=95): #5和95表示分位数\n    band=np.array(band,dtype=np.float32)\n    c = np.percentile(band, lower_percent)*1.0\n    d = np.percentile(band, higher_percent)*1.0       \n    band[band<c] = c\n    band[band>d] = d\n    out =  (band - c)  / (d - c)  \n    return out.astype(np.float32)\n\ndef adjust_contrast(data,n_band=3):    #通过循环对各个波段进行拉伸\n    data=np.array(data,dtype=np.float32)\n    for img in data:\n        for k in range(n_band):\n            img[:,:,k] = stretch_n(img[:,:,k])\n    return data\n\nnewimg=adjust_contrast(images.copy()) #该操作讲改变原始数据，因此用.copy，不对原始数据进行更改\nprint(np.max(images),np.max(newimg))\nshow_5_images(images/10000,labels)#plot函数要求数据为0-1之间的浮点型或0-255的8位整型数据\nshow_5_images(newimg,labels)\n\n\n\n2. 数据切片\n\n## 定义随机裁剪和顺序裁剪两种方式，顺序裁剪是按照固定步长沿行列循环裁剪，切片数是有限的，随机裁剪是以随机点为起始点裁剪，切片数可以是无限的且可自定义，后者的好处是可以通过增加算法约束label中某一类的数量来实现精准的样本获取。\ndef random_crop(image,crop_sz):\n    img_sz=image.shape[:2]\n    random_x = np.random.randint(0,img_sz[0]-crop_sz+1) ##生成随机点\n    random_y = np.random.randint(0,img_sz[1]-crop_sz+1)\n    s_img = image[random_x:random_x+crop_sz,random_y:random_y+crop_sz,:] ##以随机点为起始点生成样本框，进行切片\n    return s_img\n\ndef data_crop_random(img_arr,crop_sz,n_patch):   \n    c = img_arr.shape[-1]\n    data = np.zeros([n_patch, crop_sz, crop_sz, c])\n    for j in np.arange(n_patch):\n        image = random_crop(img_arr,crop_sz)\n        data[ j,:,:,:] = image\n    return data\ndef sequential_crop(imagearray,crop_sz,step=256):\n    data = []\n    x=0\n    row_num = ((imagearray.shape)[0] - step) // step  ##最多能裁剪几行 几列\n    col_num=((imagearray.shape)[1] - step) // step\n    x_start=0\n    y_start=0\n    for h in range(row_num):\n        for w in range(col_num):\n            crop_img = imagearray[crop_sz*h+y_start:crop_sz*(h+1)+y_start, crop_sz*w+x_start:crop_sz*(w+1)+x_start,:] ##行列循环，滑动窗口移动              \n            data.append(crop_img)\n            x=x+1\n    data=np.array(data)\n    return data\ndef data_crop(imagearray,crop_sz,stride,random=None,n_patch=250):   #设置random选项，用来切换是否采用随机裁切\n    data = []\n    for i in range(imagearray.shape[0]):\n        if random:\n            image=data_crop_random(imagearray[i,:,:,:],crop_sz,n_patch)\n        else:\n            image =sequential_crop(imagearray[i,:,:,:],crop_sz,stride)\n        if i == 0:\n            data  = image ##注意当i=0的时候需要将image赋给data，否则data依然是空，不可以进行concatnate\n        else:\n            data  = np.concatenate((data, image), axis = 0) \n        print(\"patch processing....:\"+str(i))\n    data=np.array(data,dtype=np.float32)\n    print(\"final processed:\"+str(i)+\"...No.:\"+str(data.shape[0]))    \n    return data\n\n\n## 合并images和labels方便切片\ndata_all = np.concatenate((newimg, np.expand_dims(labels,axis=-1)), axis = -1)\nstride=256\ncropsize=256\nall_patches=data_crop(data_all,cropsize,stride,random=False)##保留2景数据做测试\nprint(data_all.shape,all_patches.shape)\ni=0\n\n\n##调用utils中的plot_func查看数据与label是否对应,反复运行这个cell进行数据浏览i表示每次浏览跨越数据的个数\nplot_func(all_patches[i:i+20,:,:,:3],all_patches[i:i+20:,:,:,-1])\ni+=500\n\n\nall_patches=suffle_data(all_patches) #对数据进行打乱处理\n\n\n##可以选择将数据保存为h5文件，方便后续使用\nhdf5_path = './data/patches_rgb_4b_5c.hdf5' \nf = h5py.File(hdf5_path, mode='w')\nf['image'] = all_patches[:,:,:,0:-1]\nf['label'] = all_patches[:,:,:,-1]\nf.close()\n\n\n\n3. 模型训练\n\n# hdf5_path = \"./data/patches_rgb_4b_6c.hdf5\"\n# fd = h5py.File(hdf5_path, 'r')\n# fd.keys()\n# images=np.array(fd['image'])\n# labels=np.array(fd['label'])#3:tree,4:road,5:crop\n\n\n## 对数据进行归一化处理，并将label转成one-hot标签形式\n# n_label=5\ndef post_normalize_image(images,labels,n_label=n_label):\n    msk = label_hot(labels,n_label)\n    img = images\n    return img,msk\n# img,msk=post_normalize_image(images,labels,n_label)\nimg,msk=post_normalize_image(all_patches[:,:,:,0:-1],all_patches[:,:,:,-1],n_label)\n## 将数据集按照7:3进行划分\nxtrain,xtest,ytrain,ytest=train_test_split(img,msk,test_size=0.2,random_state=42)\ndel img,msk #如果数据较大可以在此删除降低内存\nprint(xtrain.shape,xtest.shape,ytrain.shape,ytest.shape)\ni=0\n\n\nplot_func(xtrain[i:i+20,:,:,:3],np.argmax(ytrain,axis=-1)[i:i+20:,:,:])\ni+=500\n\n\n## 设定必要的参数\nloss='categorical_crossentropy'\noptimizer=Adam()\nloss_weights=''\nmetrics=['accuracy']\nbatch_size=20\nepoch= 50\ninput_height=xtrain.shape[1]\ninput_width=xtrain.shape[2]\nnchannel=xtrain.shape[-1]\nnum_train=xtrain.shape[0]\nnum_val=xtest.shape[0]\nATM=all_model(loss,loss_weights,optimizer,metrics,input_height,input_width,n_label,nchannel) #向all_model传参返回模型\nmodelname='unet'\n\n\n## 加载UNET模型\nmodel=ATM.UNET()#deeplabv3p,UNET,dlinknet,convattunet\n# model.summary()\n\n\n## 开始训练\nbegin_time = time.time()          \nout_dir = \"./checkpoint/\"+modelname+'/'\nif not os.path.exists(out_dir):\n    os.makedirs(out_dir) \n## 设置model-checkpoint用来存储模型参数文件\nmodel_checkpoint = ModelCheckpoint(filepath=out_dir+\"-{epoch:02d}e-val_loss{val_loss:2f}.hdf5\",monitor=\"val_loss\",save_best_only=True,mode='auto')\n## 设置csvlogger用来记录训练记录\ncsvlogger =CSVLogger(filename=out_dir+modelname+'-'+str(epoch)+'-log.csv', separator=',', append=False)\n## model_checkpoint和csvlogger要想发挥作用必须放入callback中\nresult=model.fit(xtrain, ytrain, batch_size=batch_size, epochs=epoch, verbose=1, shuffle=True,validation_data=(xtest, ytest),callbacks=[model_checkpoint,csvlogger])\nend_time = time.time()\nprint('time lapsing {0} s \\n'.format(end_time - begin_time))\n\n\n## 打印训练曲线，确认训练效果，精度不够，loss不收敛，模型学习能力不足且容易过拟合\ndef plot_fig(H,outdir):\n    N=len(H.history['loss'])\n    plt.style.use(\"ggplot\")\n    plt.figure(figsize=(10,6))\n    plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n    plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n    plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n    plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n    plt.ylim(0,1)\n    plt.title(\"Training Loss and Accuracy\")\n    plt.xlabel(\"Epoch #\")\n    plt.ylabel(\"Loss/Accuracy\")\n    plt.legend(loc=\"lower left\")\n    plt.savefig(outdir)\nplot_fig(result,modelname+\"_Loss_Acc_epoch.png\")\n\n\n\n4. 评价测试\n\n## 训练过程只保留最有性能参数文件，因此从训练记录里选择最后一个即可\nh5=glob.glob(\"./checkpoint/\"+modelname+\"/*.hdf5\")\nmodel_list=np.sort(h5)\nmodel=load_model(model_list[-1])\n# model=load_model(model_list[-1],custom_objects={'interpolation':interpolation})#keras导入模型需要判断是会否有自定义函数或层，有的话需要在custom_objects中定义，并编译\nprint(model_list[-1])\n\n\n## 首先对切片数据进行预测，查看效果\ni=100\npred=model.predict(xtest,batch_size=64)\npred=np.argmax(pred,axis=-1)\ngt=np.argmax(ytest,axis=-1)\n\n\n##逐批次查看预测效果\nval_plot_func(xtest[i:i+20],gt[i:i+20],pred[i:i+20])\ni+=100\n\n\n## 用测试集评价模型精度\ndf = call_matric(pred,gt, [0,1,2,3,4, 'all'])\nprint(df)\n\n\n\n4. 优化改进\n\n4.1 数据优化\n\nbuild_num = np.sum(labels ==0)\nroad_num = np.sum(labels == 1)\ntree_num = np.sum(labels == 2)\ncrop_num = np.sum(labels == 3)\nwater_num = np.sum(labels == 4)\n# 这两行代码解决 plt 中文显示的问题\nplt.rcParams['font.sans-serif'] = ['SimHei']\nplt.rcParams['axes.unicode_minus'] = False\nplt.style.use(\"ggplot\")\n\nclasses = ('0-Building', '1-Road', '2-Tree', '3-Crop', '4-Riveer')\nnumbers = [build_num,road_num, tree_num,crop_num, water_num]\nprint(numbers)\nplt.barh(classes, numbers,color='lightblue')\nplt.title('Number of pixels in each category')\n# plt.savefig(\"Number-category.png\", dpi = 600, bbox_inches=\"tight\")\nplt.show()\n\n\n## 定义随机裁剪增加对于label中1和4样本的采集,\"num_count(image[:,:,-1],1)\"表示切片中数值为1的像元个数\ndef data_crop_random2(img_arr,crop_sz,n_patch):   \n    data =[]\n    k=0\n    for j in np.arange(1000):\n        image = random_crop(img_arr,crop_sz)\n        if num_count(image[:,:,-1],1) +num_count(image[:,:,-1],4) >8000:\n            data.append(image)\n            k+=1\n            if k==n_patch:\n                break                 \n    if k == 0:\n        data  = np.expand_dims(image,axis=0) ##注意如果k=0，即没有符合条件的数据将最后一个image赋给data，避免data为空\n    else:\n        data  = np.array(data,dtype=np.float32)\n\n    print(data.shape)\n    return data.astype(np.float32)\ndef data_crop2(imagearray,crop_sz,stride,random=None,n_patch=250):   \n    data = []\n    for i in range(imagearray.shape[0]):\n        if random:\n            image=data_crop_random2(imagearray[i,:,:,:],crop_sz,n_patch)\n        else:\n            image =sequential_crop(imagearray[i,:,:,:],crop_sz,stride)\n        if i == 0:\n            data  = image ##注意当i=0的时候需要将image赋给data，否则data依然是空，不可以进行concatnate\n        else:\n            data  = np.concatenate((data, image), axis = 0) \n        print(\"patch processing....:\"+str(i))\n    data=np.array(data,dtype=np.float32)\n    print(\"final processed:\"+str(i)+\"...No.:\"+str(data.shape[0]))    \n    return data\n\n\n## 同样使用前面14幅影像进行切片，增加不平衡样本数据的采集\n# data_all = np.concatenate((newimg, np.expand_dims(labels,axis=-1)), axis = -1)\n# stride=256\n# cropsize=256\nall_patches2=data_crop2(data_all,cropsize,stride,random=True)\nprint(data_all.shape,all_patches2.shape)\ni=0\n\n\nall_patches2=suffle_data(all_patches2)# 对新的数据集进行随机打乱\n\n\n# plot_func(all_patches2[i:i+20,:,:,:3],all_patches2[i:i+20:,:,:,-1])\n# i+=500\n\n\n## 加载前面生成的切片数据\n# hdf5_path = \"./data/patches_rgb_4b_5c.hdf5\"\n# fd = h5py.File(hdf5_path, 'r')\n# fd.keys()\n# images2=np.array(fd['image'])\n# labels2=np.array(fd['label'])\n\n\n## 对两次切片数据进行合并，得到新的数据集\nnewimages=np.concatenate((images2,all_patches2[:,:,:,0:-1]),axis=0)\nnewlabels=np.concatenate((labels2,all_patches2[:,:,:,-1]),axis=0)\nprint(newimages.shape,newlabels.shape)\n\n\n##可以选择将数据保存为h5文件，方便后续使用\nhdf5_path = './data/patches2_rgb_4b_5c.hdf5' \nf = h5py.File(hdf5_path, mode='w')\nf['image'] = newimages\nf['label'] = newlabels\nf.close()\n\n\n# hdf5_path = './data/patches2_rgb_4b_5c.hdf5' \n# fd = h5py.File(hdf5_path, 'r')\n# fd.keys()\n# newimages=np.array(fd['image'])\n# newlabels=np.array(fd['label'])#3:tree,4:road,5:crop\n\n\n## 对数据进行归一化处理，并将label转成one-hot标签形式\nn_label=5\ndef post_normalize_image(images,labels,n_label=n_label):\n    msk = label_hot(labels,n_label)\n    img = images\n    return img,msk\nimg,msk=post_normalize_image(newimages,newlabels,n_label)\n# 将数据集按照7:3进行划分\nxtrain,xtest,ytrain,ytest=train_test_split(img,msk,test_size=0.2,random_state=42)\ndel img,msk #如果数据较大可以在此删除降低内存\nprint(xtrain.shape,xtest.shape,ytrain.shape,ytest.shape)\ni=0\n\n\nplot_func(xtrain[i:i+20,:,:,:3],np.argmax(ytrain,axis=-1)[i:i+20:,:,:])\ni+=500\n\n\n\n4.2 模型优化\n\n## 计算真值标签中各个类别的占比，作为损失函数的权重，权重值越大模型识别错误代价越大一定程度缓解数据不平衡问题。\n# from sklearn.utils.class_weight import compute_class_weight\n# classes = np.unique(labels)  \n# class_weight = compute_class_weight(class_weight='balanced', classes=classes, y=labels.reshape(-1))\nclass_weight=np.array([0.35,4.48,2.07,0.68,28.55])\nprint(class_weight)\n\n\n## 采用带有权重的交叉熵损失函数\nfrom keras import backend as K\nimport tensorflow as tf\ndef weighted_categorical_crossentropy(weights):\n    \"\"\"\n    Usage:\n        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n        loss = weighted_categorical_crossentropy(weights)\n    \"\"\"\n    weights = K.variable(weights)\n    def loss(y_true, y_pred):\n        # scale predictions so that the class probas of each sample sum to 1\n        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n        # clip to prevent NaN's and Inf's\n        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n        # calc\n        loss = y_true * K.log(y_pred) * weights\n        loss = -K.sum(loss, -1)\n        return loss\n    return loss\n\n\n## 设定必要的参数\nloss=weighted_categorical_crossentropy(class_weight)\noptimizer=Adam()\nloss_weights=''\nmetrics=['accuracy']\nbatch_size=10\nepoch= 50\ninput_height=xtrain.shape[1]\ninput_width=xtrain.shape[2]\nnchannel=xtrain.shape[-1]\nnum_train=xtrain.shape[0]\nnum_val=xtest.shape[0]\nATM=all_model(loss,loss_weights,optimizer,metrics,input_height,input_width,n_label,nchannel)\nmodelname='convattunet'\n\n\n## unet下采样操作较多导致细小线状地物信息丢失，新的网络减少下采样，且在decoder部分采用注意力机制提升浅层特征的权重\nmodel=ATM.convattunet()#deeplabv3p,UNET,dlinknet,convattunet\n# model.summary()\n\n\n## 开始训练\nbegin_time = time.time()          \nout_dir = \"./checkpoint/\"+modelname+'/'\nif not os.path.exists(out_dir):\n    os.makedirs(out_dir) \nmodel_checkpoint = ModelCheckpoint(filepath=out_dir+\"-{epoch:02d}e-val_loss{val_loss:2f}.hdf5\",monitor=\"val_loss\",save_best_only=True,mode='auto')\ncsvlogger =CSVLogger(filename=out_dir+modelname+'-'+str(epoch)+'-log.csv', separator=',', append=False)\nresult=model.fit(xtrain, ytrain, batch_size=batch_size, epochs=epoch, verbose=1, shuffle=True,validation_data=(xtest, ytest),callbacks=[model_checkpoint,csvlogger])\nend_time = time.time()\nprint('time lapsing {0} s \\n'.format(end_time - begin_time))\n\n\n## 训练时长较短，模型为达到收敛因此最高精度不是很高，但训练曲线和验证曲线趋势十分吻合，且loss有明显的降低，表明模型性能有提升\nplot_fig(result,modelname+\"_Loss_Acc_epoch.png\")\n\n\nh5=glob.glob(\"./checkpoint/\"+modelname+\"/*.hdf5\")\nmodel_list=np.sort(h5)\n# model=load_model(model_list[-1])\nmodel=load_model(model_list[-1],custom_objects={'loss':weighted_categorical_crossentropy}) #loss作为自定义层需要指出\nprint(model_list[-1])\n\n\n## 首先对切片数据进行预测，查看效果\ni=100\npred=model.predict(xtest,batch_size=64)\npred=np.argmax(pred,axis=-1)\ngt=np.argmax(ytest,axis=-1)\n\n\nval_plot_func(xtest[i:i+20],gt[i:i+20],pred[i:i+20])\ni+=20\n\n\n## 评估结果发现1类和4类地物较之前都有精度的提升，整体miou提升16%\ndf = call_matric(pred,gt, [0,1,2,3,4,'all'])\nprint(df)\n\n\n\n4.3 整景影像的预测\n\n## 加载整景的影像进行测试\ntest_data  = h5py.File('./data/kaggle_test1_3b_5c.hdf5', 'r')\ntestimg = np.array(test_data['image'])\ntestlabel=np.array(test_data['label'])\nprint(testimg.shape,testlabel.shape)\n\n\n## 与训练数据采用相同的预处理方式\nimage=adjust_contrast(testimg)\nnp.max(image),np.max(testimg)\n\n\n## 首先对影像做padding，保证其能够被crop_size整除，先沿着行列分别裁切样本，再统一进行预测，预测后数据按照原来的顺序再排列组合复原。需要注意的是这里采用的是膨胀预测的方法，喂给模型用来预测的切片大小是256，但放的时候只保留了中间的128×128，四周数据可靠度低，直接废弃\ndef center_predict(img,model,batch_size,n_label,strides=128,img_size=256):\n    corner_size=int(0.25*img_size)\n    h,w,c = img.shape\n    padding_h = (h//strides + 1) * strides+corner_size+corner_size\n    padding_w = (w//strides + 1) * strides+corner_size+corner_size\n    \n    padding_img = np.zeros((padding_h,padding_w,c),dtype=np.float16)\n    padding_img[corner_size:corner_size+h,corner_size:corner_size+w,:] = img[:,:,:]\n    mask_whole = np.zeros((padding_h,padding_w,n_label),dtype=np.float16)\n    crop_batch=[]\n    for i in range(h//strides+1):\n        for j in range(w//strides+1):\n            crop_img = padding_img[i*strides:i*strides+img_size,j*strides:j*strides+img_size,:]\n            ch,cw,c = crop_img.shape\n            \n            if ch != img_size or cw != img_size:\n                continue\n            crop_batch.append(crop_img)\n            \n    crop_batch=np.array(crop_batch)\n    start_time=time.time()\n    pred=model.predict(crop_batch,batch_size=batch_size)\n\n    for i in range(h//strides+1):\n        for j in range(w//strides+1):\n            mask_whole[i*strides+corner_size:i*strides+img_size-corner_size,j*strides+corner_size:j*strides+img_size-corner_size] = pred[(i+1-1)*(w//strides+1)+(j+1)-1,corner_size:img_size-corner_size,corner_size:img_size-corner_size]\n    score = mask_whole[corner_size:corner_size+h,corner_size:corner_size+w]\n    end_time=time.time()\n    print('pred_time:',end_time-start_time)\n    return score\n\n\nh_pred = center_predict(image[0],model,32,n_label)\nh_pred_mask=np.argmax(h_pred, axis = -1)\nprint(h_pred.shape,testlabel[0].shape)\n\n\nfig=plt.figure(figsize=(20,20)) \nplt.subplot(1,2,1)\nplt.imshow(testlabel[0,:,:])\nplt.subplot(1,2,2)\nplt.imshow(h_pred_mask)\nplt.show()"
  },
  {
    "objectID": "posts/2022-01/index.html",
    "href": "posts/2022-01/index.html",
    "title": "AI眼中的诗意——AI绘画",
    "section": "",
    "text": "1. AI绘画大火\n\n\n2016 年，Memo Akten 和 Mike Tyka 等人工智能艺术家在在旧金山展出了由谷歌的 Deep Dream 算法生成的图像。短短两年时间，人工智能艺术作品登上了世界艺术舞台。\n2018 年，一幅由人工智能生成的图像在佳士得拍卖，这是一幅由巴黎的 Obvious 集体制作的渲染版画。埃德蒙·贝拉米肖像拍卖是第一次被广泛报道的人工智能艺术品拍卖。\n\n\n\n\n\n\n\n\n2022年被称为AI绘画元年。美国科罗拉多州博览会的一项美术比赛上，一等奖竟然颁给了AI自动生成的画作。作品全名《太空歌剧院》，由一位名叫Jason Allen的游戏设计师提交，作品利用AI软件 MidJourney，经过了近900次的迭代，数周的挑选与调整，才最终被打印到画布上，参与了这次比赛。 \n无独有偶，2022年戛纳电影短片节的最佳短片，也颁给了AI人工智能生成的作品《THE CROW》（乌鸦）\nDALL-E 2、Midjourney和StableDiffusion等AI绘画工具，让业余人士只要简单打几个关键字，就能够创作出复杂、或抽象、或写实的艺术作品。下面是一幅有AI生成的中国山水画。 \n\n\n\n2. AI眼中的诗意\n\n\nAI能理解我们的古诗吗？以滕王阁序为例，看看AI眼中“落霞与孤鹜齐飞，秋水共长天一色”这两句优美的诗句是怎样的。下面是由AI给出的部分答案：\n\n\n\n\n\n\n\n\n\n3.AI绘画原理\n\n\nDiffusionModel已取代GAN称为图像生成及AI绘画领域的新宠儿，引发了扩散模型的研究热潮，目前采用的扩散模型大都是来自于2020年的的工作DDPM: Denoising Diffusion Probabilistic Models。简单来说，扩散模型包含两个过程：前向扩散过程和反向生成过程，前向扩散过程是对一张图像逐渐添加高斯噪音直至慢慢抹去图像中所有可辨别的细节，直到变成纯粹的“噪点”，而反向生成过程是去噪音过程，逐渐对图像进行去像素化来学习逆转噪声并恢复图像。  \n正如谷歌解释的那样：\n\n\n“Running this reversed corruption process synthesizes data from pure noise by gradually denoising it until a clean sample is produced.”\n\n\n训练模型的核心数据集则是 LAION-high-resolution 和 LAION-Aesthetics。使用 AWS 提供的 4000 块 A100 显卡组成的强力计算集群，花费约 15 万小时的训练完成了第一个版本。具体技术细节请移步：Stable_diffusion, diffusion_model\n如何深入学习扩散模型的原理？fast.ai将和 Huggingface, Stability.ai等各方一起创作一门新课程，叫做 From Deep Learning Foundations to Stable Diffusion。这门课将会用新的方式让普通人从原理上理解 Stable Diffusion 模型。课程详细介绍请移步：传送门\n\n\n\n4. 人们对于AI绘画的看法？\n\n\n不少艺术家认为AI绘画这是一种作弊：“我们正在目睹艺术的消亡。如果创造性的工作在机器面前都不安全，那么即使你有高技能，也有被淘汰的危险。到那时，我们将拥有什么？”\n网友更是直言：“从AI目前的进度来看，行业被挤压是迟早的事情了。那些嘲讽人类艺术家的人，当你被替代感到危机的时候，你一样会发出无能的怒吼”\n懂得灵活运用AI绘画作为创作工具的艺术家们认为：“这项技术具有改变我们交流方式的巨大潜力，我们期待与大家一起建立一个更快乐、更具交流性和创造性的未来”\n除了图像生成之外，复杂的 AI 生成视频模型也已出现。不久前，Meta推出了AI系统“Make-a-Video”，可以由文本、图像生成短视频，也可以改变现有视频的细节来生成新的视频。现在，谷歌推出了新的视频生成模型“Imagen Video”，可实现每秒24帧的速度下达到1280 x 768像素的高清视频，这又会对当下火爆的对短视频领域带来哪些机遇和挑战呢？ \n\n\n\n5. 总结\n\n\nAI绘画取得的效果是令人惊叹的；\n模型本身还是依然于海量的数据集，而这些数据库中所包含的图像和文字还是不全面的，因而无法解析数据库之外的词句以及绘画风格，比如“齐天大圣”，“岩彩画”。长远来看其对于日常实物基本可以做到准确的模拟，但对于场景和复杂语义的理解需要更多的努力，其中一个必要的途径是构建更为广泛的且细粒化的数据集，也许不久能够实现AI利用互联网资源进行自学习；\nAi绘画成功的背后是扩散模型，而扩散模型的思路是可以用到其他领域的，比如同样拥有海量数据及视觉属性的遥感领域，试想一下利用遥感数据对扩散模型进行迁移学习能否实现遥感场景的变换，这种新的数据生成技术能否为小样本识别问题提供解决方案呢？\nAI绘制“落霞与孤鹜齐飞，秋水共长天一色”图片下载链接：百度云, 提取码：1234\n\n\nReferences\n\n\n《THE CROW》: https://www.bilibili.com/video/BV16P411V7Ah?share_source=copy_web\nStable_diffusion: https://huggingface.co/blog/stable_diffusion\ndiffusion_model: https://theaisummer.com/diffusion-models/\n传送门: https://www.fast.ai/posts/part2-2022.html"
  },
  {
    "objectID": "posts/lesson/main - 副本.html",
    "href": "posts/lesson/main - 副本.html",
    "title": "Junchuan Yu",
    "section": "",
    "text": "利用多光谱遥感数据进行地物分类\n\n本课程目的是利用神经网络开展多光谱地物分类，学习从整幅卫星影像制作样本数据集，训练多分类神经网络，并实现对整景卫星数据的预测。\n运行该代码提前需要安装以下几个必要的库 1. numpy 2. tensorflow = 2.5 3. h5py = 3.1 4. Pillow = 8.4\n\nimport h5py\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.optimizers import RMSprop,Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau,CSVLogger\nfrom tensorflow.keras.models import load_model\nimport matplotlib.pyplot as plt\nimport time,glob\nfrom model import all_model\nfrom utils import *\n\n\n\n1. 加载并查看数据\n\n## 加载数据集，数据集维度为3840×3840×4，最后一个波段是真值标签，标签一共有六类，“”\nhdf5_path = \"./data/kaggle_14_3b_5c.hdf5\"\nfd = h5py.File(hdf5_path, 'r')\nfd.keys()\nimages=fd['image']\nlabels=fd['label']\nn_label=len(np.unique(labels)) #{0:'buiding',1:'Road', 2:'Tree',3: 'Crops',4:'Water'}\n## 该影像是反射率数据（通常数值在0-1之间），为了节省存储空间常将数值放大10000倍，保存为无符号整型数据\nprint(np.max(images),np.min(images),np.max(labels),np.min(labels))\nimages=np.array(images)\nlabels=np.array(labels)\nprint(images.shape,labels.shape)\n\n\n## 将整幅影像及标签数据打印出来,为了提升原始影像的显示效果，对原有数据进行拉伸处理 \n\ndef stretch_n(band, lower_percent=5, higher_percent=95): #5和95表示分位数\n    band=np.array(band,dtype=np.float32)\n    c = np.percentile(band, lower_percent)*1.0\n    d = np.percentile(band, higher_percent)*1.0       \n    band[band<c] = c\n    band[band>d] = d\n    out =  (band - c)  / (d - c)  \n    return out.astype(np.float32)\n\ndef adjust_contrast(data,n_band=3):    #通过循环对各个波段进行拉伸\n    data=np.array(data,dtype=np.float32)\n    for img in data:\n        for k in range(n_band):\n            img[:,:,k] = stretch_n(img[:,:,k])\n    return data\n\nnewimg=adjust_contrast(images.copy()) #该操作讲改变原始数据，因此用.copy，不对原始数据进行更改\nprint(np.max(images),np.max(newimg))\nshow_5_images(images/10000,labels)#plot函数要求数据为0-1之间的浮点型或0-255的8位整型数据\nshow_5_images(newimg,labels)\n\n\n\n2. 数据切片\n\n## 定义随机裁剪和顺序裁剪两种方式，顺序裁剪是按照固定步长沿行列循环裁剪，切片数是有限的，随机裁剪是以随机点为起始点裁剪，切片数可以是无限的且可自定义，后者的好处是可以通过增加算法约束label中某一类的数量来实现精准的样本获取。\ndef random_crop(image,crop_sz):\n    img_sz=image.shape[:2]\n    random_x = np.random.randint(0,img_sz[0]-crop_sz+1) ##生成随机点\n    random_y = np.random.randint(0,img_sz[1]-crop_sz+1)\n    s_img = image[random_x:random_x+crop_sz,random_y:random_y+crop_sz,:] ##以随机点为起始点生成样本框，进行切片\n    return s_img\n\ndef data_crop_random(img_arr,crop_sz,n_patch):   \n    c = img_arr.shape[-1]\n    data = np.zeros([n_patch, crop_sz, crop_sz, c])\n    for j in np.arange(n_patch):\n        image = random_crop(img_arr,crop_sz)\n        data[ j,:,:,:] = image\n    return data\ndef sequential_crop(imagearray,crop_sz,step=256):\n    data = []\n    x=0\n    row_num = ((imagearray.shape)[0] - step) // step  ##最多能裁剪几行 几列\n    col_num=((imagearray.shape)[1] - step) // step\n    x_start=0\n    y_start=0\n    for h in range(row_num):\n        for w in range(col_num):\n            crop_img = imagearray[crop_sz*h+y_start:crop_sz*(h+1)+y_start, crop_sz*w+x_start:crop_sz*(w+1)+x_start,:] ##行列循环，滑动窗口移动              \n            data.append(crop_img)\n            x=x+1\n    data=np.array(data)\n    return data\ndef data_crop(imagearray,crop_sz,stride,random=None,n_patch=250):   #设置random选项，用来切换是否采用随机裁切\n    data = []\n    for i in range(imagearray.shape[0]):\n        if random:\n            image=data_crop_random(imagearray[i,:,:,:],crop_sz,n_patch)\n        else:\n            image =sequential_crop(imagearray[i,:,:,:],crop_sz,stride)\n        if i == 0:\n            data  = image ##注意当i=0的时候需要将image赋给data，否则data依然是空，不可以进行concatnate\n        else:\n            data  = np.concatenate((data, image), axis = 0) \n        print(\"patch processing....:\"+str(i))\n    data=np.array(data,dtype=np.float32)\n    print(\"final processed:\"+str(i)+\"...No.:\"+str(data.shape[0]))    \n    return data\n\n\n## 合并images和labels方便切片\ndata_all = np.concatenate((newimg, np.expand_dims(labels,axis=-1)), axis = -1)\nstride=256\ncropsize=256\nall_patches=data_crop(data_all,cropsize,stride,random=False)##保留2景数据做测试\nprint(data_all.shape,all_patches.shape)\ni=0\n\n\n##调用utils中的plot_func查看数据与label是否对应,反复运行这个cell进行数据浏览i表示每次浏览跨越数据的个数\nplot_func(all_patches[i:i+20,:,:,:3],all_patches[i:i+20:,:,:,-1])\ni+=500\n\n\nall_patches=suffle_data(all_patches) #对数据进行打乱处理\n\n\n##可以选择将数据保存为h5文件，方便后续使用\nhdf5_path = './data/patches_rgb_4b_5c.hdf5' \nf = h5py.File(hdf5_path, mode='w')\nf['image'] = all_patches[:,:,:,0:-1]\nf['label'] = all_patches[:,:,:,-1]\nf.close()\n\n\n\n3. 模型训练\n\n# hdf5_path = \"./data/patches_rgb_4b_6c.hdf5\"\n# fd = h5py.File(hdf5_path, 'r')\n# fd.keys()\n# images=np.array(fd['image'])\n# labels=np.array(fd['label'])#3:tree,4:road,5:crop\n\n\n## 对数据进行归一化处理，并将label转成one-hot标签形式\n# n_label=5\ndef post_normalize_image(images,labels,n_label=n_label):\n    msk = label_hot(labels,n_label)\n    img = images\n    return img,msk\n# img,msk=post_normalize_image(images,labels,n_label)\nimg,msk=post_normalize_image(all_patches[:,:,:,0:-1],all_patches[:,:,:,-1],n_label)\n## 将数据集按照7:3进行划分\nxtrain,xtest,ytrain,ytest=train_test_split(img,msk,test_size=0.2,random_state=42)\ndel img,msk #如果数据较大可以在此删除降低内存\nprint(xtrain.shape,xtest.shape,ytrain.shape,ytest.shape)\ni=0\n\n\nplot_func(xtrain[i:i+20,:,:,:3],np.argmax(ytrain,axis=-1)[i:i+20:,:,:])\ni+=500\n\n\n## 设定必要的参数\nloss='categorical_crossentropy'\noptimizer=Adam()\nloss_weights=''\nmetrics=['accuracy']\nbatch_size=20\nepoch= 50\ninput_height=xtrain.shape[1]\ninput_width=xtrain.shape[2]\nnchannel=xtrain.shape[-1]\nnum_train=xtrain.shape[0]\nnum_val=xtest.shape[0]\nATM=all_model(loss,loss_weights,optimizer,metrics,input_height,input_width,n_label,nchannel) #向all_model传参返回模型\nmodelname='unet'\n\n\n## 加载UNET模型\nmodel=ATM.UNET()#deeplabv3p,UNET,dlinknet,convattunet\n# model.summary()\n\n\n## 开始训练\nbegin_time = time.time()          \nout_dir = \"./checkpoint/\"+modelname+'/'\nif not os.path.exists(out_dir):\n    os.makedirs(out_dir) \n## 设置model-checkpoint用来存储模型参数文件\nmodel_checkpoint = ModelCheckpoint(filepath=out_dir+\"-{epoch:02d}e-val_loss{val_loss:2f}.hdf5\",monitor=\"val_loss\",save_best_only=True,mode='auto')\n## 设置csvlogger用来记录训练记录\ncsvlogger =CSVLogger(filename=out_dir+modelname+'-'+str(epoch)+'-log.csv', separator=',', append=False)\n## model_checkpoint和csvlogger要想发挥作用必须放入callback中\nresult=model.fit(xtrain, ytrain, batch_size=batch_size, epochs=epoch, verbose=1, shuffle=True,validation_data=(xtest, ytest),callbacks=[model_checkpoint,csvlogger])\nend_time = time.time()\nprint('time lapsing {0} s \\n'.format(end_time - begin_time))\n\n\n## 打印训练曲线，确认训练效果，精度不够，loss不收敛，模型学习能力不足且容易过拟合\ndef plot_fig(H,outdir):\n    N=len(H.history['loss'])\n    plt.style.use(\"ggplot\")\n    plt.figure(figsize=(10,6))\n    plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n    plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n    plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n    plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n    plt.ylim(0,1)\n    plt.title(\"Training Loss and Accuracy\")\n    plt.xlabel(\"Epoch #\")\n    plt.ylabel(\"Loss/Accuracy\")\n    plt.legend(loc=\"lower left\")\n    plt.savefig(outdir)\nplot_fig(result,modelname+\"_Loss_Acc_epoch.png\")\n\n\n\n4. 评价测试\n\n## 训练过程只保留最有性能参数文件，因此从训练记录里选择最后一个即可\nh5=glob.glob(\"./checkpoint/\"+modelname+\"/*.hdf5\")\nmodel_list=np.sort(h5)\nmodel=load_model(model_list[-1])\n# model=load_model(model_list[-1],custom_objects={'interpolation':interpolation})#keras导入模型需要判断是会否有自定义函数或层，有的话需要在custom_objects中定义，并编译\nprint(model_list[-1])\n\n\n## 首先对切片数据进行预测，查看效果\ni=100\npred=model.predict(xtest,batch_size=64)\npred=np.argmax(pred,axis=-1)\ngt=np.argmax(ytest,axis=-1)\n\n\n##逐批次查看预测效果\nval_plot_func(xtest[i:i+20],gt[i:i+20],pred[i:i+20])\ni+=100\n\n\n## 用测试集评价模型精度\ndf = call_matric(pred,gt, [0,1,2,3,4, 'all'])\nprint(df)\n\n\n\n4. 优化改进\n\n4.1 数据优化\n\nbuild_num = np.sum(labels ==0)\nroad_num = np.sum(labels == 1)\ntree_num = np.sum(labels == 2)\ncrop_num = np.sum(labels == 3)\nwater_num = np.sum(labels == 4)\n# 这两行代码解决 plt 中文显示的问题\nplt.rcParams['font.sans-serif'] = ['SimHei']\nplt.rcParams['axes.unicode_minus'] = False\nplt.style.use(\"ggplot\")\n\nclasses = ('0-Building', '1-Road', '2-Tree', '3-Crop', '4-Riveer')\nnumbers = [build_num,road_num, tree_num,crop_num, water_num]\nprint(numbers)\nplt.barh(classes, numbers,color='lightblue')\nplt.title('Number of pixels in each category')\n# plt.savefig(\"Number-category.png\", dpi = 600, bbox_inches=\"tight\")\nplt.show()\n\n\n## 定义随机裁剪增加对于label中1和4样本的采集,\"num_count(image[:,:,-1],1)\"表示切片中数值为1的像元个数\ndef data_crop_random2(img_arr,crop_sz,n_patch):   \n    data =[]\n    k=0\n    for j in np.arange(1000):\n        image = random_crop(img_arr,crop_sz)\n        if num_count(image[:,:,-1],1) +num_count(image[:,:,-1],4) >8000:\n            data.append(image)\n            k+=1\n            if k==n_patch:\n                break                 \n    if k == 0:\n        data  = np.expand_dims(image,axis=0) ##注意如果k=0，即没有符合条件的数据将最后一个image赋给data，避免data为空\n    else:\n        data  = np.array(data,dtype=np.float32)\n\n    print(data.shape)\n    return data.astype(np.float32)\ndef data_crop2(imagearray,crop_sz,stride,random=None,n_patch=250):   \n    data = []\n    for i in range(imagearray.shape[0]):\n        if random:\n            image=data_crop_random2(imagearray[i,:,:,:],crop_sz,n_patch)\n        else:\n            image =sequential_crop(imagearray[i,:,:,:],crop_sz,stride)\n        if i == 0:\n            data  = image ##注意当i=0的时候需要将image赋给data，否则data依然是空，不可以进行concatnate\n        else:\n            data  = np.concatenate((data, image), axis = 0) \n        print(\"patch processing....:\"+str(i))\n    data=np.array(data,dtype=np.float32)\n    print(\"final processed:\"+str(i)+\"...No.:\"+str(data.shape[0]))    \n    return data\n\n\n## 同样使用前面14幅影像进行切片，增加不平衡样本数据的采集\n# data_all = np.concatenate((newimg, np.expand_dims(labels,axis=-1)), axis = -1)\n# stride=256\n# cropsize=256\nall_patches2=data_crop2(data_all,cropsize,stride,random=True)\nprint(data_all.shape,all_patches2.shape)\ni=0\n\n\nall_patches2=suffle_data(all_patches2)# 对新的数据集进行随机打乱\n\n\n# plot_func(all_patches2[i:i+20,:,:,:3],all_patches2[i:i+20:,:,:,-1])\n# i+=500\n\n\n## 加载前面生成的切片数据\n# hdf5_path = \"./data/patches_rgb_4b_5c.hdf5\"\n# fd = h5py.File(hdf5_path, 'r')\n# fd.keys()\n# images2=np.array(fd['image'])\n# labels2=np.array(fd['label'])\n\n\n## 对两次切片数据进行合并，得到新的数据集\nnewimages=np.concatenate((images2,all_patches2[:,:,:,0:-1]),axis=0)\nnewlabels=np.concatenate((labels2,all_patches2[:,:,:,-1]),axis=0)\nprint(newimages.shape,newlabels.shape)\n\n\n##可以选择将数据保存为h5文件，方便后续使用\nhdf5_path = './data/patches2_rgb_4b_5c.hdf5' \nf = h5py.File(hdf5_path, mode='w')\nf['image'] = newimages\nf['label'] = newlabels\nf.close()\n\n\n# hdf5_path = './data/patches2_rgb_4b_5c.hdf5' \n# fd = h5py.File(hdf5_path, 'r')\n# fd.keys()\n# newimages=np.array(fd['image'])\n# newlabels=np.array(fd['label'])#3:tree,4:road,5:crop\n\n\n## 对数据进行归一化处理，并将label转成one-hot标签形式\nn_label=5\ndef post_normalize_image(images,labels,n_label=n_label):\n    msk = label_hot(labels,n_label)\n    img = images\n    return img,msk\nimg,msk=post_normalize_image(newimages,newlabels,n_label)\n# 将数据集按照7:3进行划分\nxtrain,xtest,ytrain,ytest=train_test_split(img,msk,test_size=0.2,random_state=42)\ndel img,msk #如果数据较大可以在此删除降低内存\nprint(xtrain.shape,xtest.shape,ytrain.shape,ytest.shape)\ni=0\n\n\nplot_func(xtrain[i:i+20,:,:,:3],np.argmax(ytrain,axis=-1)[i:i+20:,:,:])\ni+=500\n\n\n\n4.2 模型优化\n\n## 计算真值标签中各个类别的占比，作为损失函数的权重，权重值越大模型识别错误代价越大一定程度缓解数据不平衡问题。\n# from sklearn.utils.class_weight import compute_class_weight\n# classes = np.unique(labels)  \n# class_weight = compute_class_weight(class_weight='balanced', classes=classes, y=labels.reshape(-1))\nclass_weight=np.array([0.35,4.48,2.07,0.68,28.55])\nprint(class_weight)\n\n\n## 采用带有权重的交叉熵损失函数\nfrom keras import backend as K\nimport tensorflow as tf\ndef weighted_categorical_crossentropy(weights):\n    \"\"\"\n    Usage:\n        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n        loss = weighted_categorical_crossentropy(weights)\n    \"\"\"\n    weights = K.variable(weights)\n    def loss(y_true, y_pred):\n        # scale predictions so that the class probas of each sample sum to 1\n        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n        # clip to prevent NaN's and Inf's\n        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n        # calc\n        loss = y_true * K.log(y_pred) * weights\n        loss = -K.sum(loss, -1)\n        return loss\n    return loss\n\n\n## 设定必要的参数\nloss=weighted_categorical_crossentropy(class_weight)\noptimizer=Adam()\nloss_weights=''\nmetrics=['accuracy']\nbatch_size=10\nepoch= 50\ninput_height=xtrain.shape[1]\ninput_width=xtrain.shape[2]\nnchannel=xtrain.shape[-1]\nnum_train=xtrain.shape[0]\nnum_val=xtest.shape[0]\nATM=all_model(loss,loss_weights,optimizer,metrics,input_height,input_width,n_label,nchannel)\nmodelname='convattunet'\n\n\n## unet下采样操作较多导致细小线状地物信息丢失，新的网络减少下采样，且在decoder部分采用注意力机制提升浅层特征的权重\nmodel=ATM.convattunet()#deeplabv3p,UNET,dlinknet,convattunet\n# model.summary()\n\n\n## 开始训练\nbegin_time = time.time()          \nout_dir = \"./checkpoint/\"+modelname+'/'\nif not os.path.exists(out_dir):\n    os.makedirs(out_dir) \nmodel_checkpoint = ModelCheckpoint(filepath=out_dir+\"-{epoch:02d}e-val_loss{val_loss:2f}.hdf5\",monitor=\"val_loss\",save_best_only=True,mode='auto')\ncsvlogger =CSVLogger(filename=out_dir+modelname+'-'+str(epoch)+'-log.csv', separator=',', append=False)\nresult=model.fit(xtrain, ytrain, batch_size=batch_size, epochs=epoch, verbose=1, shuffle=True,validation_data=(xtest, ytest),callbacks=[model_checkpoint,csvlogger])\nend_time = time.time()\nprint('time lapsing {0} s \\n'.format(end_time - begin_time))\n\n\n## 训练时长较短，模型为达到收敛因此最高精度不是很高，但训练曲线和验证曲线趋势十分吻合，且loss有明显的降低，表明模型性能有提升\nplot_fig(result,modelname+\"_Loss_Acc_epoch.png\")\n\n\nh5=glob.glob(\"./checkpoint/\"+modelname+\"/*.hdf5\")\nmodel_list=np.sort(h5)\n# model=load_model(model_list[-1])\nmodel=load_model(model_list[-1],custom_objects={'loss':weighted_categorical_crossentropy}) #loss作为自定义层需要指出\nprint(model_list[-1])\n\n\n## 首先对切片数据进行预测，查看效果\ni=100\npred=model.predict(xtest,batch_size=64)\npred=np.argmax(pred,axis=-1)\ngt=np.argmax(ytest,axis=-1)\n\n\nval_plot_func(xtest[i:i+20],gt[i:i+20],pred[i:i+20])\ni+=20\n\n\n## 评估结果发现1类和4类地物较之前都有精度的提升，整体miou提升16%\ndf = call_matric(pred,gt, [0,1,2,3,4,'all'])\nprint(df)\n\n\n\n4.3 整景影像的预测\n\n## 加载整景的影像进行测试\ntest_data  = h5py.File('./data/kaggle_test1_3b_5c.hdf5', 'r')\ntestimg = np.array(test_data['image'])\ntestlabel=np.array(test_data['label'])\nprint(testimg.shape,testlabel.shape)\n\n\n## 与训练数据采用相同的预处理方式\nimage=adjust_contrast(testimg)\nnp.max(image),np.max(testimg)\n\n\n## 首先对影像做padding，保证其能够被crop_size整除，先沿着行列分别裁切样本，再统一进行预测，预测后数据按照原来的顺序再排列组合复原。需要注意的是这里采用的是膨胀预测的方法，喂给模型用来预测的切片大小是256，但放的时候只保留了中间的128×128，四周数据可靠度低，直接废弃\ndef center_predict(img,model,batch_size,n_label,strides=128,img_size=256):\n    corner_size=int(0.25*img_size)\n    h,w,c = img.shape\n    padding_h = (h//strides + 1) * strides+corner_size+corner_size\n    padding_w = (w//strides + 1) * strides+corner_size+corner_size\n    \n    padding_img = np.zeros((padding_h,padding_w,c),dtype=np.float16)\n    padding_img[corner_size:corner_size+h,corner_size:corner_size+w,:] = img[:,:,:]\n    mask_whole = np.zeros((padding_h,padding_w,n_label),dtype=np.float16)\n    crop_batch=[]\n    for i in range(h//strides+1):\n        for j in range(w//strides+1):\n            crop_img = padding_img[i*strides:i*strides+img_size,j*strides:j*strides+img_size,:]\n            ch,cw,c = crop_img.shape\n            \n            if ch != img_size or cw != img_size:\n                continue\n            crop_batch.append(crop_img)\n            \n    crop_batch=np.array(crop_batch)\n    start_time=time.time()\n    pred=model.predict(crop_batch,batch_size=batch_size)\n\n    for i in range(h//strides+1):\n        for j in range(w//strides+1):\n            mask_whole[i*strides+corner_size:i*strides+img_size-corner_size,j*strides+corner_size:j*strides+img_size-corner_size] = pred[(i+1-1)*(w//strides+1)+(j+1)-1,corner_size:img_size-corner_size,corner_size:img_size-corner_size]\n    score = mask_whole[corner_size:corner_size+h,corner_size:corner_size+w]\n    end_time=time.time()\n    print('pred_time:',end_time-start_time)\n    return score\n\n\nh_pred = center_predict(image[0],model,32,n_label)\nh_pred_mask=np.argmax(h_pred, axis = -1)\nprint(h_pred.shape,testlabel[0].shape)\n\n\nfig=plt.figure(figsize=(20,20)) \nplt.subplot(1,2,1)\nplt.imshow(testlabel[0,:,:])\nplt.subplot(1,2,2)\nplt.imshow(h_pred_mask)\nplt.show()"
  },
  {
    "objectID": "posts/2022-10-8_ai_poem/index.html",
    "href": "posts/2022-10-8_ai_poem/index.html",
    "title": "AI眼中的诗意——AI绘画",
    "section": "",
    "text": "1. AI绘画大火\n\n\n2016 年，Memo Akten 和 Mike Tyka 等人工智能艺术家在在旧金山展出了由谷歌的 Deep Dream 算法生成的图像。短短两年时间，人工智能艺术作品登上了世界艺术舞台。\n2018 年，一幅由人工智能生成的图像在佳士得拍卖，这是一幅由巴黎的 Obvious 集体制作的渲染版画。埃德蒙·贝拉米肖像拍卖是第一次被广泛报道的人工智能艺术品拍卖。\n\n\n\n\n\n\n\n\n2022年被称为AI绘画元年。美国科罗拉多州博览会的一项美术比赛上，一等奖竟然颁给了AI自动生成的画作。作品全名《太空歌剧院》，由一位名叫Jason Allen的游戏设计师提交，作品利用AI软件 MidJourney，经过了近900次的迭代，数周的挑选与调整，才最终被打印到画布上，参与了这次比赛。 \n无独有偶，2022年戛纳电影短片节的最佳短片，也颁给了AI人工智能生成的作品《THE CROW》（乌鸦）\nDALL-E 2、Midjourney和StableDiffusion等AI绘画工具，让业余人士只要简单打几个关键字，就能够创作出复杂、或抽象、或写实的艺术作品。下面是一幅有AI生成的中国山水画。 \n\n\n\n2. AI眼中的诗意\n\n\nAI能理解我们的古诗吗？以滕王阁序为例，看看AI眼中“落霞与孤鹜齐飞，秋水共长天一色”这两句优美的诗句是怎样的。下面是由AI给出的部分答案：\n\n\n\n\n\n\n\n\n\n3.AI绘画原理\n\n\nDiffusionModel已取代GAN称为图像生成及AI绘画领域的新宠儿，引发了扩散模型的研究热潮，目前采用的扩散模型大都是来自于2020年的的工作DDPM: Denoising Diffusion Probabilistic Models。简单来说，扩散模型包含两个过程：前向扩散过程和反向生成过程，前向扩散过程是对一张图像逐渐添加高斯噪音直至慢慢抹去图像中所有可辨别的细节，直到变成纯粹的“噪点”，而反向生成过程是去噪音过程，逐渐对图像进行去像素化来学习逆转噪声并恢复图像。  \n正如谷歌解释的那样：\n\n\n“Running this reversed corruption process synthesizes data from pure noise by gradually denoising it until a clean sample is produced.”\n\n\n训练模型的核心数据集则是 LAION-high-resolution 和 LAION-Aesthetics。使用 AWS 提供的 4000 块 A100 显卡组成的强力计算集群，花费约 15 万小时的训练完成了第一个版本。具体技术细节请移步：Stable_diffusion, diffusion_model\n如何深入学习扩散模型的原理？fast.ai将和 Huggingface, Stability.ai等各方一起创作一门新课程，叫做 From Deep Learning Foundations to Stable Diffusion。这门课将会用新的方式让普通人从原理上理解 Stable Diffusion 模型。课程详细介绍请移步：传送门\n\n\n\n4. 人们对于AI绘画的看法？\n\n\n不少艺术家认为AI绘画这是一种作弊：“我们正在目睹艺术的消亡。如果创造性的工作在机器面前都不安全，那么即使你有高技能，也有被淘汰的危险。到那时，我们将拥有什么？”\n网友更是直言：“从AI目前的进度来看，行业被挤压是迟早的事情了。那些嘲讽人类艺术家的人，当你被替代感到危机的时候，你一样会发出无能的怒吼”\n懂得灵活运用AI绘画作为创作工具的艺术家们认为：“这项技术具有改变我们交流方式的巨大潜力，我们期待与大家一起建立一个更快乐、更具交流性和创造性的未来”\n除了图像生成之外，复杂的 AI 生成视频模型也已出现。不久前，Meta推出了AI系统“Make-a-Video”，可以由文本、图像生成短视频，也可以改变现有视频的细节来生成新的视频。现在，谷歌推出了新的视频生成模型“Imagen Video”，可实现每秒24帧的速度下达到1280 x 768像素的高清视频，这又会对当下火爆的对短视频领域带来哪些机遇和挑战呢？ \n\n\n\n5. 总结\n\n\nAI绘画取得的效果是令人惊叹的；\n模型本身还是依然于海量的数据集，而这些数据库中所包含的图像和文字还是不全面的，因而无法解析数据库之外的词句以及绘画风格，比如“齐天大圣”，“岩彩画”。长远来看其对于日常实物基本可以做到准确的模拟，但对于场景和复杂语义的理解需要更多的努力，其中一个必要的途径是构建更为广泛的且细粒化的数据集，也许不久能够实现AI利用互联网资源进行自学习；\nAi绘画成功的背后是扩散模型，而扩散模型的思路是可以用到其他领域的，比如同样拥有海量数据及视觉属性的遥感领域，试想一下利用遥感数据对扩散模型进行迁移学习能否实现遥感场景的变换，这种新的数据生成技术能否为小样本识别问题提供解决方案呢？\nAI绘制“落霞与孤鹜齐飞，秋水共长天一色”图片下载链接：百度云, 提取码：1234\n\n\nReferences\n\n\n《THE CROW》: https://www.bilibili.com/video/BV16P411V7Ah?share_source=copy_web\nStable_diffusion: https://huggingface.co/blog/stable_diffusion\ndiffusion_model: https://theaisummer.com/diffusion-models/\n传送门: https://www.fast.ai/posts/part2-2022.html"
  },
  {
    "objectID": "posts/2022-11-15_wrsdp_3-3/index.html",
    "href": "posts/2022-11-15_wrsdp_3-3/index.html",
    "title": "利用多光谱遥感数据进行地物分类",
    "section": "",
    "text": "利用多光谱遥感数据进行地物分类\n本课程目的是利用神经网络开展多光谱地物分类，学习从整幅卫星影像制作样本数据集，训练多分类神经网络，并实现对整景卫星数据的预测。 运行该代码提前需要安装以下几个必要的库：\n\nnumpy\ntensorflow = 2.5\nh5py = 3.1\nPillow = 8.4\n\n\nimport h5py\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.optimizers import RMSprop,Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau,CSVLogger\nfrom tensorflow.keras.models import load_model\nimport matplotlib.pyplot as plt\nimport time,glob\nfrom model import all_model\nfrom utils import *\n\n\n1. 加载并查看数据\n\n## 加载数据集，数据集维度为3840×3840×4，最后一个波段是真值标签，标签一共有六类，“”\nhdf5_path = \"./data/kaggle_14_3b_5c.hdf5\"\nfd = h5py.File(hdf5_path, 'r')\nfd.keys()\nimages=fd['image']\nlabels=fd['label']\nn_label=len(np.unique(labels)) #{0:'buiding',1:'Road', 2:'Tree',3: 'Crops',4:'Water'}\n## 该影像是反射率数据（通常数值在0-1之间），为了节省存储空间常将数值放大10000倍，保存为无符号整型数据\nprint(np.max(images),np.min(images),np.max(labels),np.min(labels))\nimages=np.array(images)\nlabels=np.array(labels)\nprint(images.shape,labels.shape)\n\n9995 213 4 0\n(14, 3840, 3840, 3) (14, 3840, 3840)\n\n\n\n## 将整幅影像及标签数据打印出来,为了提升原始影像的显示效果，对原有数据进行拉伸处理 \n\ndef stretch_n(band, lower_percent=5, higher_percent=95): #5和95表示分位数\n    band=np.array(band,dtype=np.float32)\n    c = np.percentile(band, lower_percent)*1.0\n    d = np.percentile(band, higher_percent)*1.0       \n    band[band<c] = c\n    band[band>d] = d\n    out =  (band - c)  / (d - c)  \n    return out.astype(np.float32)\n\ndef adjust_contrast(data,n_band=3):    #通过循环对各个波段进行拉伸\n    data=np.array(data,dtype=np.float32)\n    for img in data:\n        for k in range(n_band):\n            img[:,:,k] = stretch_n(img[:,:,k])\n    return data\n\nnewimg=adjust_contrast(images.copy()) #该操作讲改变原始数据，因此用.copy，不对原始数据进行更改\nprint(np.max(images),np.max(newimg))\nshow_5_images(images/10000,labels)#plot函数要求数据为0-1之间的浮点型或0-255的8位整型数据\nshow_5_images(newimg,labels)\n\n\n9995 1.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2. 数据切片\n\n## 定义随机裁剪和顺序裁剪两种方式，顺序裁剪是按照固定步长沿行列循环裁剪，切片数是有限的，随机裁剪是以随机点为起始点裁剪，切片数可以是无限的且可自定义，后者的好处是可以通过增加算法约束label中某一类的数量来实现精准的样本获取。\ndef random_crop(image,crop_sz):\n    img_sz=image.shape[:2]\n    random_x = np.random.randint(0,img_sz[0]-crop_sz+1) ##生成随机点\n    random_y = np.random.randint(0,img_sz[1]-crop_sz+1)\n    s_img = image[random_x:random_x+crop_sz,random_y:random_y+crop_sz,:] ##以随机点为起始点生成样本框，进行切片\n    return s_img\n\ndef data_crop_random(img_arr,crop_sz,n_patch):   \n    c = img_arr.shape[-1]\n    data = np.zeros([n_patch, crop_sz, crop_sz, c])\n    for j in np.arange(n_patch):\n        image = random_crop(img_arr,crop_sz)\n        data[ j,:,:,:] = image\n    return data\ndef sequential_crop(imagearray,crop_sz,step=256):\n    data = []\n    x=0\n    row_num = ((imagearray.shape)[0] - step) // step  ##最多能裁剪几行 几列\n    col_num=((imagearray.shape)[1] - step) // step\n    x_start=0\n    y_start=0\n    for h in range(row_num):\n        for w in range(col_num):\n            crop_img = imagearray[crop_sz*h+y_start:crop_sz*(h+1)+y_start, crop_sz*w+x_start:crop_sz*(w+1)+x_start,:] ##行列循环，滑动窗口移动              \n            data.append(crop_img)\n            x=x+1\n    data=np.array(data)\n    return data\ndef data_crop(imagearray,crop_sz,stride,random=None,n_patch=250):   #设置random选项，用来切换是否采用随机裁切\n    data = []\n    for i in range(imagearray.shape[0]):\n        if random:\n            image=data_crop_random(imagearray[i,:,:,:],crop_sz,n_patch)\n        else:\n            image =sequential_crop(imagearray[i,:,:,:],crop_sz,stride)\n        if i == 0:\n            data  = image ##注意当i=0的时候需要将image赋给data，否则data依然是空，不可以进行concatnate\n        else:\n            data  = np.concatenate((data, image), axis = 0) \n        print(\"patch processing....:\"+str(i))\n    data=np.array(data,dtype=np.float32)\n    print(\"final processed:\"+str(i)+\"...No.:\"+str(data.shape[0]))    \n    return data\n\n\n## 合并images和labels方便切片\ndata_all = np.concatenate((newimg, np.expand_dims(labels,axis=-1)), axis = -1)\nstride=256\ncropsize=256\nall_patches=data_crop(data_all,cropsize,stride,random=False)##保留2景数据做测试\nprint(data_all.shape,all_patches.shape)\ni=0\n\npatch processing....:0\npatch processing....:1\npatch processing....:2\npatch processing....:3\npatch processing....:4\npatch processing....:5\npatch processing....:6\npatch processing....:7\npatch processing....:8\npatch processing....:9\npatch processing....:10\npatch processing....:11\npatch processing....:12\npatch processing....:13\nfinal processed:13...No.:2744\n(14, 3840, 3840, 4) (2744, 256, 256, 4)\n\n\n\n##调用utils中的plot_func查看数据与label是否对应,反复运行这个cell进行数据浏览i表示每次浏览跨越数据的个数\nplot_func(all_patches[i:i+20,:,:,:3],all_patches[i:i+20:,:,:,-1])\ni+=500\n\n\n\n\n\n\n\n\nall_patches=suffle_data(all_patches) #对数据进行打乱处理\n\n(2744, 256, 256, 4)\n\n\n\n##可以选择将数据保存为h5文件，方便后续使用\nhdf5_path = './data/patches_rgb_4b_5c.hdf5' \nf = h5py.File(hdf5_path, mode='w')\nf['image'] = all_patches[:,:,:,0:-1]\nf['label'] = all_patches[:,:,:,-1]\nf.close()\n\n\n\n3. 模型训练\n\n# hdf5_path = \"./data/patches_rgb_4b_6c.hdf5\"\n# fd = h5py.File(hdf5_path, 'r')\n# fd.keys()\n# images=np.array(fd['image'])\n# labels=np.array(fd['label'])#3:tree,4:road,5:crop\n\n\n## 对数据进行归一化处理，并将label转成one-hot标签形式\n# n_label=5\ndef post_normalize_image(images,labels,n_label=n_label):\n    msk = label_hot(labels,n_label)\n    img = images\n    return img,msk\n# img,msk=post_normalize_image(images,labels,n_label)\nimg,msk=post_normalize_image(all_patches[:,:,:,0:-1],all_patches[:,:,:,-1],n_label)\n## 将数据集按照7:3进行划分\nxtrain,xtest,ytrain,ytest=train_test_split(img,msk,test_size=0.2,random_state=42)\ndel img,msk #如果数据较大可以在此删除降低内存\nprint(xtrain.shape,xtest.shape,ytrain.shape,ytest.shape)\ni=0\n\n(2195, 256, 256, 3) (549, 256, 256, 3) (2195, 256, 256, 5) (549, 256, 256, 5)\n\n\n\nplot_func(xtrain[i:i+20,:,:,:3],np.argmax(ytrain,axis=-1)[i:i+20:,:,:])\ni+=500\n\n\n\n\n\n\n\n\n## 设定必要的参数\nloss='categorical_crossentropy'\noptimizer=Adam()\nloss_weights=''\nmetrics=['accuracy']\nbatch_size=20\nepoch= 50\ninput_height=xtrain.shape[1]\ninput_width=xtrain.shape[2]\nnchannel=xtrain.shape[-1]\nnum_train=xtrain.shape[0]\nnum_val=xtest.shape[0]\nATM=all_model(loss,loss_weights,optimizer,metrics,input_height,input_width,n_label,nchannel) #向all_model传参返回模型\nmodelname='unet'\n\n\n## 加载UNET模型\nmodel=ATM.UNET()#deeplabv3p,UNET,dlinknet,convattunet\n# model.summary()\n\n\n## 开始训练\nbegin_time = time.time()          \nout_dir = \"./checkpoint/\"+modelname+'/'\nif not os.path.exists(out_dir):\n    os.makedirs(out_dir) \n## 设置model-checkpoint用来存储模型参数文件\nmodel_checkpoint = ModelCheckpoint(filepath=out_dir+\"-{epoch:02d}e-val_loss{val_loss:2f}.hdf5\",monitor=\"val_loss\",save_best_only=True,mode='auto')\n## 设置csvlogger用来记录训练记录\ncsvlogger =CSVLogger(filename=out_dir+modelname+'-'+str(epoch)+'-log.csv', separator=',', append=False)\n## model_checkpoint和csvlogger要想发挥作用必须放入callback中\nresult=model.fit(xtrain, ytrain, batch_size=batch_size, epochs=epoch, verbose=1, shuffle=True,validation_data=(xtest, ytest),callbacks=[model_checkpoint,csvlogger])\nend_time = time.time()\nprint('time lapsing {0} s \\n'.format(end_time - begin_time))\n\nEpoch 1/50\n110/110 [==============================] - 42s 275ms/step - loss: 1.1495 - accuracy: 0.6211 - val_loss: 0.8305 - val_accuracy: 0.7096\nEpoch 2/50\n110/110 [==============================] - 22s 198ms/step - loss: 0.8297 - accuracy: 0.7023 - val_loss: 0.8479 - val_accuracy: 0.7174\nEpoch 3/50\n110/110 [==============================] - 23s 213ms/step - loss: 0.7564 - accuracy: 0.7295 - val_loss: 0.7701 - val_accuracy: 0.7465\nEpoch 4/50\n110/110 [==============================] - 23s 209ms/step - loss: 0.7241 - accuracy: 0.7353 - val_loss: 0.7346 - val_accuracy: 0.7350\nEpoch 5/50\n110/110 [==============================] - 23s 209ms/step - loss: 0.6612 - accuracy: 0.7560 - val_loss: 0.6612 - val_accuracy: 0.7651\nEpoch 6/50\n110/110 [==============================] - 22s 199ms/step - loss: 0.6603 - accuracy: 0.7573 - val_loss: 0.6624 - val_accuracy: 0.7571\nEpoch 7/50\n110/110 [==============================] - 23s 211ms/step - loss: 0.6069 - accuracy: 0.7803 - val_loss: 0.6109 - val_accuracy: 0.7743\nEpoch 8/50\n110/110 [==============================] - 23s 209ms/step - loss: 0.5959 - accuracy: 0.7837 - val_loss: 0.5707 - val_accuracy: 0.7895\nEpoch 9/50\n110/110 [==============================] - 23s 211ms/step - loss: 0.5670 - accuracy: 0.7965 - val_loss: 0.5354 - val_accuracy: 0.8031\nEpoch 10/50\n110/110 [==============================] - 22s 196ms/step - loss: 0.5354 - accuracy: 0.8077 - val_loss: 0.5420 - val_accuracy: 0.8020\nEpoch 11/50\n110/110 [==============================] - 22s 197ms/step - loss: 0.5269 - accuracy: 0.8084 - val_loss: 0.5449 - val_accuracy: 0.8104\nEpoch 12/50\n110/110 [==============================] - 23s 208ms/step - loss: 0.5232 - accuracy: 0.8107 - val_loss: 0.5206 - val_accuracy: 0.8109\nEpoch 13/50\n110/110 [==============================] - 23s 210ms/step - loss: 0.4868 - accuracy: 0.8268 - val_loss: 0.4856 - val_accuracy: 0.8252\nEpoch 14/50\n110/110 [==============================] - 22s 198ms/step - loss: 0.4827 - accuracy: 0.8273 - val_loss: 0.6254 - val_accuracy: 0.7814\nEpoch 15/50\n110/110 [==============================] - 22s 198ms/step - loss: 0.4961 - accuracy: 0.8224 - val_loss: 0.4901 - val_accuracy: 0.8262\nEpoch 16/50\n110/110 [==============================] - 23s 209ms/step - loss: 0.4700 - accuracy: 0.8329 - val_loss: 0.4735 - val_accuracy: 0.8256\nEpoch 17/50\n110/110 [==============================] - 22s 197ms/step - loss: 0.4603 - accuracy: 0.8339 - val_loss: 0.4862 - val_accuracy: 0.8220\nEpoch 18/50\n110/110 [==============================] - 22s 196ms/step - loss: 0.4476 - accuracy: 0.8389 - val_loss: 0.4996 - val_accuracy: 0.8220\nEpoch 19/50\n110/110 [==============================] - 23s 210ms/step - loss: 0.4218 - accuracy: 0.8471 - val_loss: 0.4579 - val_accuracy: 0.8304\nEpoch 20/50\n110/110 [==============================] - 22s 197ms/step - loss: 0.3968 - accuracy: 0.8580 - val_loss: 0.4725 - val_accuracy: 0.8203\nEpoch 21/50\n110/110 [==============================] - 22s 199ms/step - loss: 0.4013 - accuracy: 0.8550 - val_loss: 0.4729 - val_accuracy: 0.8256\nEpoch 22/50\n110/110 [==============================] - 23s 210ms/step - loss: 0.3659 - accuracy: 0.8679 - val_loss: 0.4542 - val_accuracy: 0.8362\nEpoch 23/50\n110/110 [==============================] - 22s 197ms/step - loss: 0.3730 - accuracy: 0.8648 - val_loss: 0.4884 - val_accuracy: 0.8149\nEpoch 24/50\n110/110 [==============================] - 23s 210ms/step - loss: 0.3559 - accuracy: 0.8716 - val_loss: 0.4249 - val_accuracy: 0.8431\nEpoch 25/50\n110/110 [==============================] - 22s 198ms/step - loss: 0.3223 - accuracy: 0.8838 - val_loss: 0.4684 - val_accuracy: 0.8340\nEpoch 26/50\n110/110 [==============================] - 22s 198ms/step - loss: 0.3408 - accuracy: 0.8776 - val_loss: 0.4931 - val_accuracy: 0.8341\nEpoch 27/50\n110/110 [==============================] - 22s 197ms/step - loss: 0.2869 - accuracy: 0.8964 - val_loss: 0.4596 - val_accuracy: 0.8429\nEpoch 28/50\n110/110 [==============================] - 22s 197ms/step - loss: 0.2717 - accuracy: 0.9020 - val_loss: 0.4739 - val_accuracy: 0.8430\nEpoch 29/50\n110/110 [==============================] - 22s 199ms/step - loss: 0.2566 - accuracy: 0.9070 - val_loss: 0.4645 - val_accuracy: 0.8507\nEpoch 30/50\n110/110 [==============================] - 22s 199ms/step - loss: 0.2469 - accuracy: 0.9102 - val_loss: 0.5344 - val_accuracy: 0.8413\nEpoch 31/50\n110/110 [==============================] - 22s 198ms/step - loss: 0.2762 - accuracy: 0.9023 - val_loss: 0.5081 - val_accuracy: 0.8386\nEpoch 32/50\n110/110 [==============================] - 22s 198ms/step - loss: 0.2831 - accuracy: 0.8983 - val_loss: 0.4640 - val_accuracy: 0.8485\nEpoch 33/50\n110/110 [==============================] - 22s 198ms/step - loss: 0.2424 - accuracy: 0.9125 - val_loss: 0.4504 - val_accuracy: 0.8548\nEpoch 34/50\n110/110 [==============================] - 22s 197ms/step - loss: 0.2218 - accuracy: 0.9193 - val_loss: 0.4752 - val_accuracy: 0.8492\nEpoch 35/50\n110/110 [==============================] - 22s 198ms/step - loss: 0.2124 - accuracy: 0.9227 - val_loss: 0.4808 - val_accuracy: 0.8529\nEpoch 36/50\n110/110 [==============================] - 22s 198ms/step - loss: 0.2016 - accuracy: 0.9262 - val_loss: 0.5387 - val_accuracy: 0.8507\nEpoch 37/50\n110/110 [==============================] - 22s 196ms/step - loss: 0.1973 - accuracy: 0.9278 - val_loss: 0.5704 - val_accuracy: 0.8502\nEpoch 38/50\n110/110 [==============================] - 22s 197ms/step - loss: 0.1797 - accuracy: 0.9334 - val_loss: 0.5250 - val_accuracy: 0.8552\nEpoch 39/50\n110/110 [==============================] - 22s 198ms/step - loss: 0.1729 - accuracy: 0.9356 - val_loss: 0.5235 - val_accuracy: 0.8561\nEpoch 40/50\n110/110 [==============================] - 22s 197ms/step - loss: 0.1718 - accuracy: 0.9359 - val_loss: 0.5590 - val_accuracy: 0.8421\nEpoch 41/50\n110/110 [==============================] - 22s 197ms/step - loss: 0.1823 - accuracy: 0.9320 - val_loss: 0.5435 - val_accuracy: 0.8470\nEpoch 42/50\n110/110 [==============================] - 22s 198ms/step - loss: 0.1711 - accuracy: 0.9358 - val_loss: 0.5568 - val_accuracy: 0.8570\nEpoch 43/50\n110/110 [==============================] - 22s 197ms/step - loss: 0.1513 - accuracy: 0.9426 - val_loss: 0.6066 - val_accuracy: 0.8564\nEpoch 44/50\n110/110 [==============================] - 22s 198ms/step - loss: 0.1386 - accuracy: 0.9466 - val_loss: 0.5596 - val_accuracy: 0.8534\nEpoch 45/50\n110/110 [==============================] - 22s 197ms/step - loss: 0.1423 - accuracy: 0.9451 - val_loss: 0.6868 - val_accuracy: 0.8247\nEpoch 46/50\n110/110 [==============================] - 22s 196ms/step - loss: 0.2242 - accuracy: 0.9180 - val_loss: 0.5130 - val_accuracy: 0.8211\nEpoch 47/50\n110/110 [==============================] - 22s 197ms/step - loss: 0.2567 - accuracy: 0.9064 - val_loss: 0.4996 - val_accuracy: 0.8435\nEpoch 48/50\n110/110 [==============================] - 22s 198ms/step - loss: 0.1763 - accuracy: 0.9341 - val_loss: 0.5540 - val_accuracy: 0.8556\nEpoch 49/50\n110/110 [==============================] - 22s 200ms/step - loss: 0.1357 - accuracy: 0.9472 - val_loss: 0.5536 - val_accuracy: 0.8574\nEpoch 50/50\n110/110 [==============================] - 22s 198ms/step - loss: 0.1193 - accuracy: 0.9528 - val_loss: 0.5919 - val_accuracy: 0.8576\ntime lapsing 1127.027404308319 s \n\n\n\n\n## 打印训练曲线，确认训练效果，精度不够，loss不收敛，模型学习能力不足且容易过拟合\ndef plot_fig(H,outdir):\n    N=len(H.history['loss'])\n    plt.style.use(\"ggplot\")\n    plt.figure(figsize=(10,6))\n    plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n    plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n    plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n    plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n    plt.ylim(0,1)\n    plt.title(\"Training Loss and Accuracy\")\n    plt.xlabel(\"Epoch #\")\n    plt.ylabel(\"Loss/Accuracy\")\n    plt.legend(loc=\"lower left\")\n    plt.savefig(outdir)\nplot_fig(result,modelname+\"_Loss_Acc_epoch.png\")\n\n\n\n\n\n\n4. 评价测试\n\n## 训练过程只保留最有性能参数文件，因此从训练记录里选择最后一个即可\nh5=glob.glob(\"./checkpoint/\"+modelname+\"/*.hdf5\")\nmodel_list=np.sort(h5)\nmodel=load_model(model_list[-1])\n# model=load_model(model_list[-1],custom_objects={'interpolation':interpolation})#keras导入模型需要判断是会否有自定义函数或层，有的话需要在custom_objects中定义，并编译\nprint(model_list[-1])\n\n./checkpoint/unet\\-24e-val_loss0.424916.hdf5\n\n\n\n## 首先对切片数据进行预测，查看效果\ni=100\npred=model.predict(xtest,batch_size=64)\npred=np.argmax(pred,axis=-1)\ngt=np.argmax(ytest,axis=-1)\n\n9/9 [==============================] - 9s 530ms/step\n\n\n\n##逐批次查看预测效果\nval_plot_func(xtest[i:i+20],gt[i:i+20],pred[i:i+20])\ni+=100\n\n\n\n\n\n\n\n\n\n\n\n## 用测试集评价模型精度\ndf = call_matric(pred,gt, [0,1,2,3,4, 'all'])\nprint(df)\n\n     precision    recall  F1-score       iou        oa      miou\n0     0.850384  0.895335  0.872280  0.773491  0.843104  0.519765\n1     0.716035  0.285363  0.408089  0.256352  0.843104  0.519765\n2     0.754666  0.684739  0.718004  0.560067  0.843104  0.519765\n3     0.862850  0.904145  0.883015  0.790534  0.843104  0.519765\n4     0.466484  0.291084  0.358479  0.218382  0.843104  0.519765\nall   0.730084  0.612133  0.647973  0.519765  0.843104  0.519765\n\n\n\n\n4. 优化改进\n\n4.1 数据优化\n\nbuild_num = np.sum(labels ==0)\nroad_num = np.sum(labels == 1)\ntree_num = np.sum(labels == 2)\ncrop_num = np.sum(labels == 3)\nwater_num = np.sum(labels == 4)\n# 这两行代码解决 plt 中文显示的问题\nplt.rcParams['font.sans-serif'] = ['SimHei']\nplt.rcParams['axes.unicode_minus'] = False\nplt.style.use(\"ggplot\")\n\nclasses = ('0-Building', '1-Road', '2-Tree', '3-Crop', '4-Riveer')\nnumbers = [build_num,road_num, tree_num,crop_num, water_num]\nprint(numbers)\nplt.barh(classes, numbers,color='lightblue')\nplt.title('Number of pixels in each category')\n# plt.savefig(\"Number-category.png\", dpi = 600, bbox_inches=\"tight\")\nplt.show()\n\n[99016105, 11047398, 17424954, 76998497, 1951446]\n\n\n\n\n\n\n## 定义随机裁剪增加对于label中1和4样本的采集,\"num_count(image[:,:,-1],1)\"表示切片中数值为1的像元个数\ndef data_crop_random2(img_arr,crop_sz,n_patch):   \n    data =[]\n    k=0\n    for j in np.arange(1000):\n        image = random_crop(img_arr,crop_sz)\n        if num_count(image[:,:,-1],1) +num_count(image[:,:,-1],4) >8000:\n            data.append(image)\n            k+=1\n            if k==n_patch:\n                break                 \n    if k == 0:\n        data  = np.expand_dims(image,axis=0) ##注意如果k=0，即没有符合条件的数据将最后一个image赋给data，避免data为空\n    else:\n        data  = np.array(data,dtype=np.float32)\n\n    print(data.shape)\n    return data.astype(np.float32)\ndef data_crop2(imagearray,crop_sz,stride,random=None,n_patch=250):   \n    data = []\n    for i in range(imagearray.shape[0]):\n        if random:\n            image=data_crop_random2(imagearray[i,:,:,:],crop_sz,n_patch)\n        else:\n            image =sequential_crop(imagearray[i,:,:,:],crop_sz,stride)\n        if i == 0:\n            data  = image ##注意当i=0的时候需要将image赋给data，否则data依然是空，不可以进行concatnate\n        else:\n            data  = np.concatenate((data, image), axis = 0) \n        print(\"patch processing....:\"+str(i))\n    data=np.array(data,dtype=np.float32)\n    print(\"final processed:\"+str(i)+\"...No.:\"+str(data.shape[0]))    \n    return data\n\n\n## 同样使用前面14幅影像进行切片，增加不平衡样本数据的采集\n# data_all = np.concatenate((newimg, np.expand_dims(labels,axis=-1)), axis = -1)\n# stride=256\n# cropsize=256\nall_patches2=data_crop2(data_all,cropsize,stride,random=True)\nprint(data_all.shape,all_patches2.shape)\ni=0\n\n(1, 256, 256, 4)\npatch processing....:0\n(1, 256, 256, 4)\npatch processing....:1\n(207, 256, 256, 4)\npatch processing....:2\n(1, 256, 256, 4)\npatch processing....:3\n(250, 256, 256, 4)\npatch processing....:4\n(250, 256, 256, 4)\npatch processing....:5\n(63, 256, 256, 4)\npatch processing....:6\n(197, 256, 256, 4)\npatch processing....:7\n(151, 256, 256, 4)\npatch processing....:8\n(250, 256, 256, 4)\npatch processing....:9\n(68, 256, 256, 4)\npatch processing....:10\n(171, 256, 256, 4)\npatch processing....:11\n(24, 256, 256, 4)\npatch processing....:12\n(81, 256, 256, 4)\npatch processing....:13\nfinal processed:13...No.:1715\n(14, 3840, 3840, 4) (1715, 256, 256, 4)\n\n\n\nall_patches2=suffle_data(all_patches2)# 对新的数据集进行随机打乱\n\n(1715, 256, 256, 4)\n\n\n\n# plot_func(all_patches2[i:i+20,:,:,:3],all_patches2[i:i+20:,:,:,-1])\n# i+=500\n\n\n\n\n\n\n\n\n## 加载前面生成的切片数据\n# hdf5_path = \"./data/patches_rgb_4b_5c.hdf5\"\n# fd = h5py.File(hdf5_path, 'r')\n# fd.keys()\n# images2=np.array(fd['image'])\n# labels2=np.array(fd['label'])\n\n\n## 对两次切片数据进行合并，得到新的数据集\nnewimages=np.concatenate((images2,all_patches2[:,:,:,0:-1]),axis=0)\nnewlabels=np.concatenate((labels2,all_patches2[:,:,:,-1]),axis=0)\nprint(newimages.shape,newlabels.shape)\n\n(4459, 256, 256, 3) (4459, 256, 256)\n\n\n\n##可以选择将数据保存为h5文件，方便后续使用\nhdf5_path = './data/patches2_rgb_4b_5c.hdf5' \nf = h5py.File(hdf5_path, mode='w')\nf['image'] = newimages\nf['label'] = newlabels\nf.close()\n\n\n# hdf5_path = './data/patches2_rgb_4b_5c.hdf5' \n# fd = h5py.File(hdf5_path, 'r')\n# fd.keys()\n# newimages=np.array(fd['image'])\n# newlabels=np.array(fd['label'])#3:tree,4:road,5:crop\n\n\n## 对数据进行归一化处理，并将label转成one-hot标签形式\nn_label=5\ndef post_normalize_image(images,labels,n_label=n_label):\n    msk = label_hot(labels,n_label)\n    img = images\n    return img,msk\nimg,msk=post_normalize_image(newimages,newlabels,n_label)\n# 将数据集按照7:3进行划分\nxtrain,xtest,ytrain,ytest=train_test_split(img,msk,test_size=0.2,random_state=42)\ndel img,msk #如果数据较大可以在此删除降低内存\nprint(xtrain.shape,xtest.shape,ytrain.shape,ytest.shape)\ni=0\n\n(3567, 256, 256, 3) (892, 256, 256, 3) (3567, 256, 256, 5) (892, 256, 256, 5)\n\n\n\nplot_func(xtrain[i:i+20,:,:,:3],np.argmax(ytrain,axis=-1)[i:i+20:,:,:])\ni+=500\n\n\n\n\n\n\n\n\n\n4.2 模型优化\n\n## 计算真值标签中各个类别的占比，作为损失函数的权重，权重值越大模型识别错误代价越大一定程度缓解数据不平衡问题。\n# from sklearn.utils.class_weight import compute_class_weight\n# classes = np.unique(labels)  \n# class_weight = compute_class_weight(class_weight='balanced', classes=classes, y=labels.reshape(-1))\nclass_weight=np.array([0.35,4.48,2.07,0.68,28.55])\nprint(class_weight)\n\n[ 0.35  4.48  2.07  0.68 28.55]\n\n\n\n## 采用带有权重的交叉熵损失函数\nfrom keras import backend as K\nimport tensorflow as tf\ndef weighted_categorical_crossentropy(weights):\n    \"\"\"\n    Usage:\n        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n        loss = weighted_categorical_crossentropy(weights)\n    \"\"\"\n    weights = K.variable(weights)\n    def loss(y_true, y_pred):\n        # scale predictions so that the class probas of each sample sum to 1\n        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n        # clip to prevent NaN's and Inf's\n        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n        # calc\n        loss = y_true * K.log(y_pred) * weights\n        loss = -K.sum(loss, -1)\n        return loss\n    return loss\n\n\n## 设定必要的参数\nloss=weighted_categorical_crossentropy(class_weight)\noptimizer=Adam()\nloss_weights=''\nmetrics=['accuracy']\nbatch_size=10\nepoch= 50\ninput_height=xtrain.shape[1]\ninput_width=xtrain.shape[2]\nnchannel=xtrain.shape[-1]\nnum_train=xtrain.shape[0]\nnum_val=xtest.shape[0]\nATM=all_model(loss,loss_weights,optimizer,metrics,input_height,input_width,n_label,nchannel)\nmodelname='convattunet'\n\n\n## unet下采样操作较多导致细小线状地物信息丢失，新的网络减少下采样，且在decoder部分采用注意力机制提升浅层特征的权重\nmodel=ATM.convattunet()#deeplabv3p,UNET,dlinknet,convattunet\n# model.summary()\n\n384\n\n\n\n## 开始训练\nbegin_time = time.time()          \nout_dir = \"./checkpoint/\"+modelname+'/'\nif not os.path.exists(out_dir):\n    os.makedirs(out_dir) \nmodel_checkpoint = ModelCheckpoint(filepath=out_dir+\"-{epoch:02d}e-val_loss{val_loss:2f}.hdf5\",monitor=\"val_loss\",save_best_only=True,mode='auto')\ncsvlogger =CSVLogger(filename=out_dir+modelname+'-'+str(epoch)+'-log.csv', separator=',', append=False)\nresult=model.fit(xtrain, ytrain, batch_size=batch_size, epochs=epoch, verbose=1, shuffle=True,validation_data=(xtest, ytest),callbacks=[model_checkpoint,csvlogger])\nend_time = time.time()\nprint('time lapsing {0} s \\n'.format(end_time - begin_time))\n\nEpoch 1/50\n357/357 [==============================] - 132s 344ms/step - loss: 1.7302 - accuracy: 0.3324 - val_loss: 2.7731 - val_accuracy: 0.0355\nEpoch 2/50\n357/357 [==============================] - 117s 328ms/step - loss: 1.1811 - accuracy: 0.4554 - val_loss: 1.1779 - val_accuracy: 0.4066\nEpoch 3/50\n357/357 [==============================] - 117s 329ms/step - loss: 1.3168 - accuracy: 0.3951 - val_loss: 1.1310 - val_accuracy: 0.5230\nEpoch 4/50\n357/357 [==============================] - 115s 323ms/step - loss: 1.2823 - accuracy: 0.4182 - val_loss: 1.2304 - val_accuracy: 0.4714\nEpoch 5/50\n357/357 [==============================] - 115s 323ms/step - loss: 1.0932 - accuracy: 0.4740 - val_loss: 2.0796 - val_accuracy: 0.4236\nEpoch 6/50\n357/357 [==============================] - 116s 324ms/step - loss: 1.0692 - accuracy: 0.4806 - val_loss: 1.8193 - val_accuracy: 0.2795\nEpoch 7/50\n357/357 [==============================] - 117s 328ms/step - loss: 0.9390 - accuracy: 0.5248 - val_loss: 0.8420 - val_accuracy: 0.5531\nEpoch 8/50\n357/357 [==============================] - 116s 324ms/step - loss: 0.9168 - accuracy: 0.5349 - val_loss: 1.3639 - val_accuracy: 0.4214\nEpoch 9/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.7919 - accuracy: 0.5759 - val_loss: 0.8654 - val_accuracy: 0.5469\nEpoch 10/50\n357/357 [==============================] - 116s 324ms/step - loss: 0.7120 - accuracy: 0.6098 - val_loss: 0.9407 - val_accuracy: 0.6363\nEpoch 11/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.6665 - accuracy: 0.6290 - val_loss: 0.9928 - val_accuracy: 0.5825\nEpoch 12/50\n357/357 [==============================] - 117s 329ms/step - loss: 0.6473 - accuracy: 0.6359 - val_loss: 0.7060 - val_accuracy: 0.6126\nEpoch 13/50\n357/357 [==============================] - 116s 324ms/step - loss: 0.7309 - accuracy: 0.6215 - val_loss: 1.2073 - val_accuracy: 0.6001\nEpoch 14/50\n357/357 [==============================] - 115s 324ms/step - loss: 0.7831 - accuracy: 0.5896 - val_loss: 1.0000 - val_accuracy: 0.4280\nEpoch 15/50\n357/357 [==============================] - 115s 324ms/step - loss: 0.6402 - accuracy: 0.6407 - val_loss: 0.8063 - val_accuracy: 0.6192\nEpoch 16/50\n357/357 [==============================] - 117s 328ms/step - loss: 0.5884 - accuracy: 0.6662 - val_loss: 0.6009 - val_accuracy: 0.7153\nEpoch 17/50\n357/357 [==============================] - 116s 324ms/step - loss: 0.5722 - accuracy: 0.6790 - val_loss: 0.6031 - val_accuracy: 0.6628\nEpoch 18/50\n357/357 [==============================] - 117s 328ms/step - loss: 0.5473 - accuracy: 0.6883 - val_loss: 0.5610 - val_accuracy: 0.6622\nEpoch 19/50\n357/357 [==============================] - 115s 324ms/step - loss: 0.6260 - accuracy: 0.6581 - val_loss: 0.6122 - val_accuracy: 0.6706\nEpoch 20/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.5256 - accuracy: 0.6956 - val_loss: 0.5667 - val_accuracy: 0.6594\nEpoch 21/50\n357/357 [==============================] - 117s 327ms/step - loss: 0.4587 - accuracy: 0.7269 - val_loss: 0.4607 - val_accuracy: 0.6873\nEpoch 22/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.4570 - accuracy: 0.7349 - val_loss: 5.9171 - val_accuracy: 0.2067\nEpoch 23/50\n357/357 [==============================] - 116s 324ms/step - loss: 0.7492 - accuracy: 0.6173 - val_loss: 0.5541 - val_accuracy: 0.6501\nEpoch 24/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.5090 - accuracy: 0.7099 - val_loss: 0.4747 - val_accuracy: 0.6905\nEpoch 25/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.4161 - accuracy: 0.7504 - val_loss: 0.4940 - val_accuracy: 0.7504\nEpoch 26/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.4415 - accuracy: 0.7416 - val_loss: 0.6152 - val_accuracy: 0.7500\nEpoch 27/50\n357/357 [==============================] - 117s 328ms/step - loss: 0.4088 - accuracy: 0.7558 - val_loss: 0.4202 - val_accuracy: 0.7457\nEpoch 28/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.3941 - accuracy: 0.7668 - val_loss: 0.4369 - val_accuracy: 0.7617\nEpoch 29/50\n357/357 [==============================] - 117s 328ms/step - loss: 0.3693 - accuracy: 0.7788 - val_loss: 0.3911 - val_accuracy: 0.7993\nEpoch 30/50\n357/357 [==============================] - 116s 324ms/step - loss: 0.3296 - accuracy: 0.7972 - val_loss: 0.4089 - val_accuracy: 0.8031\nEpoch 31/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.6094 - accuracy: 0.6927 - val_loss: 0.5222 - val_accuracy: 0.7067\nEpoch 32/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.6922 - accuracy: 0.6500 - val_loss: 0.4923 - val_accuracy: 0.7336\nEpoch 33/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.3982 - accuracy: 0.7583 - val_loss: 0.4426 - val_accuracy: 0.7489\nEpoch 34/50\n357/357 [==============================] - 117s 328ms/step - loss: 0.3531 - accuracy: 0.7851 - val_loss: 0.3565 - val_accuracy: 0.7895\nEpoch 35/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.3184 - accuracy: 0.8056 - val_loss: 0.3931 - val_accuracy: 0.7762\nEpoch 36/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.3268 - accuracy: 0.8041 - val_loss: 0.7312 - val_accuracy: 0.7174\nEpoch 37/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.5086 - accuracy: 0.7256 - val_loss: 0.4377 - val_accuracy: 0.7344\nEpoch 38/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.4898 - accuracy: 0.7339 - val_loss: 0.4392 - val_accuracy: 0.7486\nEpoch 39/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.3323 - accuracy: 0.7972 - val_loss: 0.4307 - val_accuracy: 0.7965\nEpoch 40/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.3074 - accuracy: 0.8101 - val_loss: 0.3669 - val_accuracy: 0.7966\nEpoch 41/50\n357/357 [==============================] - 117s 327ms/step - loss: 0.2872 - accuracy: 0.8219 - val_loss: 0.3196 - val_accuracy: 0.8116\nEpoch 42/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.2687 - accuracy: 0.8335 - val_loss: 0.3486 - val_accuracy: 0.7991\nEpoch 43/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.2602 - accuracy: 0.8381 - val_loss: 0.3334 - val_accuracy: 0.8044\nEpoch 44/50\n357/357 [==============================] - 116s 324ms/step - loss: 0.2730 - accuracy: 0.8322 - val_loss: 0.3430 - val_accuracy: 0.8112\nEpoch 45/50\n357/357 [==============================] - 117s 327ms/step - loss: 0.2481 - accuracy: 0.8450 - val_loss: 0.3174 - val_accuracy: 0.8323\nEpoch 46/50\n357/357 [==============================] - 116s 324ms/step - loss: 0.2362 - accuracy: 0.8527 - val_loss: 0.3275 - val_accuracy: 0.8271\nEpoch 47/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.2201 - accuracy: 0.8618 - val_loss: 0.3433 - val_accuracy: 0.8147\nEpoch 48/50\n357/357 [==============================] - 115s 324ms/step - loss: 0.2571 - accuracy: 0.8444 - val_loss: 0.3561 - val_accuracy: 0.8300\nEpoch 49/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.2231 - accuracy: 0.8617 - val_loss: 0.4232 - val_accuracy: 0.8120\nEpoch 50/50\n357/357 [==============================] - 115s 323ms/step - loss: 0.2010 - accuracy: 0.8733 - val_loss: 0.3685 - val_accuracy: 0.8746\ntime lapsing 5814.08594250679 s \n\n\n\n\n## 训练时长较短，模型为达到收敛因此最高精度不是很高，但训练曲线和验证曲线趋势十分吻合，且loss有明显的降低，表明模型性能有提升\nplot_fig(result,modelname+\"_Loss_Acc_epoch.png\")\n\n\n\n\n\nh5=glob.glob(\"./checkpoint/\"+modelname+\"/*.hdf5\")\nmodel_list=np.sort(h5)\n# model=load_model(model_list[-1])\nmodel=load_model(model_list[-1],custom_objects={'loss':weighted_categorical_crossentropy}) #loss作为自定义层需要指出\nprint(model_list[-1])\n\n./checkpoint/convattunet\\-45e-val_loss0.317361.hdf5\n\n\n\n## 首先对切片数据进行预测，查看效果\ni=100\npred=model.predict(xtest,batch_size=64)\npred=np.argmax(pred,axis=-1)\ngt=np.argmax(ytest,axis=-1)\n\n14/14 [==============================] - 15s 816ms/step\n\n\n\nval_plot_func(xtest[i:i+20],gt[i:i+20],pred[i:i+20])\ni+=20\n\n\n\n\n\n\n\n\n\n\n\n## 评估结果发现1类和4类地物较之前都有精度的提升，整体miou提升16%\ndf = call_matric(pred,gt, [0,1,2,3,4,'all'])\nprint(df)\n\n     precision    recall  F1-score       iou        oa      miou\n0     0.939834  0.777367  0.850915  0.740515  0.832341  0.679057\n1     0.544363  0.946242  0.691128  0.528033  0.832341  0.679057\n2     0.645177  0.880091  0.744544  0.593047  0.832341  0.679057\n3     0.923623  0.863848  0.892736  0.806254  0.832341  0.679057\n4     0.730599  0.994085  0.842215  0.727437  0.832341  0.679057\nall   0.756720  0.892327  0.804308  0.679057  0.832341  0.679057\n\n\n\n\n4.3 整景影像的预测\n\n## 加载整景的影像进行测试\ntest_data  = h5py.File('./data/kaggle_test1_3b_5c.hdf5', 'r')\ntestimg = np.array(test_data['image'])\ntestlabel=np.array(test_data['label'])\nprint(testimg.shape,testlabel.shape)\n\n(1, 3840, 3840, 3) (1, 3840, 3840)\n\n\n\n## 与训练数据采用相同的预处理方式\nimage=adjust_contrast(testimg)\nnp.max(image),np.max(testimg)\n\n(1.0, 9995)\n\n\n\n## 首先对影像做padding，保证其能够被crop_size整除，先沿着行列分别裁切样本，再统一进行预测，预测后数据按照原来的顺序再排列组合复原。需要注意的是这里采用的是膨胀预测的方法，喂给模型用来预测的切片大小是256，但放的时候只保留了中间的128×128，四周数据可靠度低，直接废弃\ndef center_predict(img,model,batch_size,n_label,strides=128,img_size=256):\n    corner_size=int(0.25*img_size)\n    h,w,c = img.shape\n    padding_h = (h//strides + 1) * strides+corner_size+corner_size\n    padding_w = (w//strides + 1) * strides+corner_size+corner_size\n    \n    padding_img = np.zeros((padding_h,padding_w,c),dtype=np.float16)\n    padding_img[corner_size:corner_size+h,corner_size:corner_size+w,:] = img[:,:,:]\n    mask_whole = np.zeros((padding_h,padding_w,n_label),dtype=np.float16)\n    crop_batch=[]\n    for i in range(h//strides+1):\n        for j in range(w//strides+1):\n            crop_img = padding_img[i*strides:i*strides+img_size,j*strides:j*strides+img_size,:]\n            ch,cw,c = crop_img.shape\n            \n            if ch != img_size or cw != img_size:\n                continue\n            crop_batch.append(crop_img)\n            \n    crop_batch=np.array(crop_batch)\n    start_time=time.time()\n    pred=model.predict(crop_batch,batch_size=batch_size)\n\n    for i in range(h//strides+1):\n        for j in range(w//strides+1):\n            mask_whole[i*strides+corner_size:i*strides+img_size-corner_size,j*strides+corner_size:j*strides+img_size-corner_size] = pred[(i+1-1)*(w//strides+1)+(j+1)-1,corner_size:img_size-corner_size,corner_size:img_size-corner_size]\n    score = mask_whole[corner_size:corner_size+h,corner_size:corner_size+w]\n    end_time=time.time()\n    print('pred_time:',end_time-start_time)\n    return score\n\n\nh_pred = center_predict(image[0],model,32,n_label)\nh_pred_mask=np.argmax(h_pred, axis = -1)\nprint(h_pred.shape,testlabel[0].shape)\n\n31/31 [==============================] - 11s 292ms/step\npred_time: 14.90808916091919\n(3840, 3840, 5) (3840, 3840)\n\n\n\nfig=plt.figure(figsize=(20,20)) \nplt.subplot(1,2,1)\nplt.imshow(testlabel[0,:,:])\nplt.subplot(1,2,2)\nplt.imshow(h_pred_mask)\nplt.show()"
  },
  {
    "objectID": "posts/2022-11-17wrsdp_1-3-1/index.html",
    "href": "posts/2022-11-17wrsdp_1-3-1/index.html",
    "title": "哨兵2号数据获取及处理",
    "section": "",
    "text": "哨兵-2号卫星携带一枚多光谱成像仪(MSI)，高度为786km，可覆盖13个光谱波段，幅宽达290千米。地面分辨率分别为10m、20m和60m、一颗卫星的重访周期为10天，两颗互补，重访周期为5天。从可见光和近红外到短波红外，具有不同的空间分辨率，在光学数据中，哨兵-2号数据是唯一一个在红边范围含有三个波段的数据，这对监测植被健康信息非常有效。\n蓝色 (B2)、绿色 (B3)、红色 (B4) 和近红外 (B8) 波段具有 10 米的分辨率；红端（B5）、近红外 NIR（B6、B7 和 B8A）以及短波红外 SWIR（B11和B12）的地面采样距离为20米；沿海大气气溶胶 (B1) 和卷云波段 (B10) 的像素大小为 60 米；\n\n环境要求： * GDAL==2.4.1 * numpy==1.19.5 * scikit-image==0.17.2 * sentinelsat==1.1.1 * zipp==3.6.0\n\nfrom sentinelsat import SentinelAPI, read_geojson, geojson_to_wkt\nfrom datetime import date\nimport os,glob,zipfile,rasterio\nimport rasterio.plot\nimport numpy as np\nfrom osgeo import gdal, osr, ogr, gdalconst\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport geopandas as gpd\n\n\n数据下载\n\nSentinelsat包提供搜索、下载和检索Sentinel数据的功能\n数据来源：哥白尼数据开放访问中心\n\n\n\n# 输入ESA用户名和密码\nuser_name = 'chidan'\npassword  = 'chidan123!'\n\n# 登录ESA API\napi = SentinelAPI(user_name, password, 'https://apihub.copernicus.eu/apihub')\n\n# 检索的地理范围坐标\nfootprint = \"POLYGON((115.5 40.2, 116.0 40.2, 116.0 40.5, 115.5 40.5, 115.5 40.2))\" #左上为起始点，顺时针旋转，回到起始点闭合\n\n# 使用API接口查询，更多接口信息查询https://www.strerr.com/geojson/geojson.html#map=5/31.386/115.329\nproducts = api.query(footprint,\n                     date = ('20220801', date(2022,9,1)),\n                     platformname = 'Sentinel-2',\n                     producttype  = \"S2MSI2A\", #S2MSI1C\",# S2MS2Ap\n                     cloudcoverpercentage = (0, 1))\n\nprint(f\"一共检索到{len(products)}景符合条件的数据\\n\")\nfor i, product in enumerate(products):\n    product_info = api.get_product_odata(product)\n    print(product_info['title'])\n\n一共检索到4景符合条件的数据\n\nS2B_MSIL2A_20220825T030519_N0400_R075_T50TLK_20220825T052806\nS2B_MSIL2A_20220825T030519_N0400_R075_T50TMK_20220825T052806\nS2A_MSIL2A_20220810T030531_N0400_R075_T50TMK_20220810T083201\nS2B_MSIL2A_20220805T030529_N0400_R075_T50TMK_20220805T053117\n\n\n下载数据并解压,只下载一景\n\n# 创建下载路径\nsave_path = './data/'\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\n\n# 下载检索到的最后一景数据，对于长期存档的数据（3-6个月以上），会出现offline情况，在下载的时候，需要先请求，后台将数据调档至在线，时间大概是半个小时，才能下载\nfor i, product in enumerate(products):\n    if i == 1:\n        break \n    product_info = api.get_product_odata(product)\n    # if product_info['title'] == \"S2B_MSIL2A_20220805T030529_N0400_R075_T50TMK_20220805T053117\":\n    if product_info['Online']:\n        print('数据在线，正在下载')\n        file_save_path = api.download(product_info['id'],save_path)\n        sentinel_data_path = file_save_path['path']\n    else:\n        print('数据未在线，请等待30分钟重试')\n\n数据在线，正在下载\n\n\n\n\n\n\n\n\n\n# 解压数据\nf = zipfile.ZipFile(sentinel_data_path,'r') \n\n# 解压到save_path\nfor file in f.namelist():\n    f.extract(file,save_path)               \nf.close()\n\nSentinel-2产品解压后为SAFE格式，SAFE文件包含以下几个内容：\n\n一个manifest.safe文件，其中包含 XML 格式的一般产品信息\nJPEG2000 格式的预览图像\n测量（传感器扫描成像）数据集的子文件夹，包括 GML-JPEG2000 格式的图像数据（颗粒/瓦片）\n数据条级别信息的子文件夹\n带有辅助数据的子文件夹（例如国际地球自转和参考系统 (IERS) 公告）\nHTML 预览\n\n\n获取数据的信息\n\n\nxml_path = sentinel_data_path[:-4] + '.SAFE' + os.sep +'MTD_MSIL2A.xml'\nroot_ds = gdal.Open(xml_path)\nds_list = root_ds.GetSubDatasets()  # 获取子数据集。该数据以数据集形式存储且以子数据集形式组织\nfor i in range(len(ds_list)):\n    visual_ds = gdal.Open(ds_list[i][0])  # 打开第i个数据子集的路径。ds_list有4个子集，内部前段是路径，后段是数据信息\n    print(ds_list[i][0])\n    print(f'数据波段为：{ds_list[i][1]}')\n    print(f'仿射矩阵信息：{visual_ds.GetGeoTransform()}')\n    print(f'投影信息：{visual_ds.GetProjection()}')\n    print(f'栅格波段数：{visual_ds.RasterCount}')\n    print(f'栅格列数（宽度）：{visual_ds.RasterXSize} 栅格行数（高度）：{visual_ds.RasterYSize}')\n    print(\"\\n\")\n\nSENTINEL2_L2A:data\\S2B_MSIL2A_20220825T030519_N0400_R075_T50TLK_20220825T052806.SAFE\\MTD_MSIL2A.xml:10m:EPSG_32650\n数据波段为：Bands B2, B3, B4, B8 with 10m resolution, UTM 50N\n仿射矩阵信息：(300000.0, 10.0, 0.0, 4500000.0, 0.0, -10.0)\n投影信息：PROJCS[\"WGS 84 / UTM zone 50N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32650\"]]\n栅格波段数：4\n栅格列数（宽度）：10980 栅格行数（高度）：10980\n\n\nSENTINEL2_L2A:data\\S2B_MSIL2A_20220825T030519_N0400_R075_T50TLK_20220825T052806.SAFE\\MTD_MSIL2A.xml:20m:EPSG_32650\n数据波段为：Bands B5, B6, B7, B8A, B11, B12, AOT, CLD, SCL, SNW, WVP with 20m resolution, UTM 50N\n仿射矩阵信息：(300000.0, 20.0, 0.0, 4500000.0, 0.0, -20.0)\n投影信息：PROJCS[\"WGS 84 / UTM zone 50N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32650\"]]\n栅格波段数：11\n栅格列数（宽度）：5490 栅格行数（高度）：5490\n\n\nSENTINEL2_L2A:data\\S2B_MSIL2A_20220825T030519_N0400_R075_T50TLK_20220825T052806.SAFE\\MTD_MSIL2A.xml:60m:EPSG_32650\n数据波段为：Bands B1, B9, AOT, CLD, SCL, SNW, WVP with 60m resolution, UTM 50N\n仿射矩阵信息：(300000.0, 60.0, 0.0, 4500000.0, 0.0, -60.0)\n投影信息：PROJCS[\"WGS 84 / UTM zone 50N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32650\"]]\n栅格波段数：7\n栅格列数（宽度）：1830 栅格行数（高度）：1830\n\n\nSENTINEL2_L2A:data\\S2B_MSIL2A_20220825T030519_N0400_R075_T50TLK_20220825T052806.SAFE\\MTD_MSIL2A.xml:TCI:EPSG_32650\n数据波段为：True color image, UTM 50N\n仿射矩阵信息：(300000.0, 10.0, 0.0, 4500000.0, 0.0, -10.0)\n投影信息：PROJCS[\"WGS 84 / UTM zone 50N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32650\"]]\n栅格波段数：3\n栅格列数（宽度）：10980 栅格行数（高度）：10980\n\n\n\n\n\n\n数据格式转换\n我们所需要的图像数据储存在IMG_DATA文件夹里，该文件夹内有三个子文件夹，分别存有三种不同分辨率的数据，数据格式为JPEG2000。接下来的任务为：\n\n在解压缩后的文件夹查找到jp2数据\n对各个波段的jp2数据进行解压缩，保存为GeoTiff格式数据\n\n\n# 检索解压目录下的所有子文件夹，找到IMG_DATA文件夹\nfor root, _, _ in os.walk(sentinel_data_path[:-4] + '.SAFE'):\n    if root.endswith(\"IMG_DATA\"):\n        IMG_DATA_path = root\n\n\ndef get_image_name(image_directory):\n    file = image_directory + '\\R10m'              \n    names = os.listdir(file)[0]\n    img_name = names[:22]\n    return img_name\n\n\n# 获得文件名\nimg_identifier = get_image_name(IMG_DATA_path)\n\n# 拼接成各个波段文件绝对路径\n\n# 20m分辨率数据\njp2_20m_list = [IMG_DATA_path + os.sep +'R20m\\%s_B8A_20m.jp2' %img_identifier ,\n                IMG_DATA_path + os.sep +'R20m\\%s_B11_20m.jp2' %img_identifier ,\n                IMG_DATA_path + os.sep +'R20m\\%s_B12_20m.jp2' %img_identifier,\n                IMG_DATA_path + os.sep +'R20m\\%s_B05_20m.jp2' %img_identifier,\n                IMG_DATA_path + os.sep +'R20m\\%s_B06_20m.jp2' %img_identifier,\n                IMG_DATA_path + os.sep +'R20m\\%s_B07_20m.jp2' %img_identifier]\n\n# 10m分辨率数据\njp2_10m_list = [IMG_DATA_path + os.sep +'R10m\\%s_B02_10m.jp2' %img_identifier,\n                IMG_DATA_path + os.sep +'R10m\\%s_B03_10m.jp2' %img_identifier,\n                IMG_DATA_path + os.sep +'R10m\\%s_B04_10m.jp2' %img_identifier,\n                IMG_DATA_path + os.sep +'R10m\\%s_B08_10m.jp2' %img_identifier]\n\ndef jp2Totif(jp2_path,save_path):\n    \"\"\"\n    格式转换：jp2 转 tif\n    :param jp2_path: jp2数据地址\n    :param save_path: 存储路径\n    :return: 存储地址\n    \"\"\"\n    file_name = os.path.basename(jp2_path)[:-4]\n    save_file = os.path.join(save_path,file_name)\n    save_file = save_file+'.tif'\n    dataset = gdal.Open(jp2_path)\n    rows = dataset.RasterYSize\n    cols = dataset.RasterXSize\n    projection = dataset.GetProjection()\n    trans = dataset.GetGeoTransform()\n    data = dataset.ReadAsArray()\n    if data.dtype == 'uint16':\n        driver = gdal.GetDriverByName('GTiff')\n        out_dataset = driver.Create(save_file, cols, rows, 1, gdal.GDT_UInt16)\n        out_dataset.SetProjection(projection)\n        out_dataset.SetGeoTransform(trans)\n        out_dataset.GetRasterBand(1).WriteArray(data)\n        out_dataset.GetRasterBand(1).SetNoDataValue(0)\n        out_dataset.FlushCache()\n        del dataset, out_dataset\n    elif data.dtype == 'uint8':\n        driver = gdal.GetDriverByName('GTiff')\n        out_dataset = driver.Create(save_file, cols, rows, 1, gdal.GDT_Byte)\n        out_dataset.SetProjection(projection)\n        out_dataset.SetGeoTransform(trans)\n        out_dataset.GetRasterBand(1).WriteArray(data)\n        out_dataset.GetRasterBand(1).SetNoDataValue(0)\n        out_dataset.FlushCache()\n        del dataset, out_dataset\n    return save_file\n\n仅堆叠可见波段NIR、RE和SWIR1和SWIR2（波段2、3、4、8、8A、11、12）。将20m波段（8A、11和12）重新采样至10m。\n\n# 创建20m分辨率数据存储路径\ntif_20m_save_path = \".\\\\data\"+ os.sep + img_identifier+ os.sep+ \"20m\"\nif not os.path.exists(tif_20m_save_path):\n    os.makedirs(tif_20m_save_path)\n\n# 将20m分辨率数据由jp2格式转换为tif格式，并记录地址到tif_20m_list\ntif_20m_list = []\nfor jp2_path in jp2_20m_list:\n    tif_path = jp2Totif(jp2_path, tif_20m_save_path)\n    tif_20m_list.append(tif_path)\n    \n# 创建10m分辨率数据存储路径\ntif_10m_save_path = \".\\\\data\"+ os.sep + img_identifier+ os.sep+\"10m\"\nif not os.path.exists(tif_10m_save_path):\n    os.makedirs(tif_10m_save_path)\n\n# 将10m分辨率数据由jp2格式转换为tif格式，并记录地址到tif_10m_list\ntif_10m_list = []\nfor jp2_path in jp2_10m_list:\n    tif_path = jp2Totif(jp2_path, tif_10m_save_path)\n    tif_10m_list.append(tif_path) \n\n\n\n数据重采样\n将20m分辨率数据重采样至10m分辨率\n\ndef ReprojectImages(inputfilePath,outputfilePath,referencefilefilePath):\n    # 获取输出影像信息\n    inputrasfile = gdal.Open(inputfilePath, gdal.GA_ReadOnly)\n    inputProj = inputrasfile.GetProjection()\n    # 获取参考影像信息\n    referencefile = gdal.Open(referencefilefilePath, gdal.GA_ReadOnly)\n    referencefileProj = referencefile.GetProjection()\n    referencefileTrans = referencefile.GetGeoTransform()\n    bandreferencefile = referencefile.GetRasterBand(1)\n    Width= referencefile.RasterXSize\n    Height = referencefile.RasterYSize\n    nbands = referencefile.RasterCount\n    # 创建重采样输出文件（设置投影及六参数）\n    driver = gdal.GetDriverByName('GTiff')\n    output = driver.Create(outputfilePath, Width,Height, nbands, bandreferencefile.DataType)\n    output.SetGeoTransform(referencefileTrans)\n    output.SetProjection(referencefileProj)\n    # 参数说明 输入数据集、输出文件、输入投影、参考投影、重采样方法(最邻近内插\\双线性内插\\三次卷积等)、回调函数\n    gdal.ReprojectImage(inputrasfile, output, inputProj, referencefileProj, gdalconst.GRA_Bilinear,0.0,0.0,)\n\n\n# 获取一景10m分辨率影像作为参考影像\nreference_tif = tif_10m_list[0] \n\n# 将20m分辨率数据重采样至10m,并保存至 tif_10m_save_path 路径下\nfor tif_path in tif_20m_list:\n    file_name = os.path.basename(tif_path)[:-7]\n    save_tif_path = tif_10m_save_path + os.sep + file_name +'10m.tif'\n    ReprojectImages(tif_path, save_tif_path, reference_tif)\n\n\n\n波段叠合\n\n将多个不同波段的的TIF文件合为一个多波段TIF文件\n叠合波段为可见波段NIR、RE和SWIR1和SWIR2（波段2、3、4、8、8A、11、12）\n\n\n# 读图像文件\ndef read_img(filename):\n\n    dataset = gdal.Open(filename)  # 打开文件\n\n    im_width = dataset.RasterXSize  # 栅格矩阵的列数\n    im_height = dataset.RasterYSize  # 栅格矩阵的行数\n    # im_bands = dataset.RasterCount  # 波段数\n    im_geotrans = dataset.GetGeoTransform()  # 仿射矩阵，左上角像素的大地坐标和像素分辨率\n    im_proj = dataset.GetProjection()  # 地图投影信息，字符串表示\n    im_data = dataset.ReadAsArray(0, 0, im_width, im_height)\n\n    del dataset   #关闭对象dataset，释放内存\n    # return im_width, im_height, im_proj, im_geotrans, im_data,im_bands\n    return  im_proj, im_geotrans, im_data, im_width,im_height\n \n# 遥感影像的存储\n# 写GeoTiff文件\ndef Write_Tiff(img_arr, geomatrix, projection,path):\n#     img_bands, img_height, img_width = img_arr.shape\n    if 'int8' in img_arr.dtype.name:\n        datatype = gdal.GDT_Byte\n    elif 'int16' in img_arr.dtype.name:\n        datatype = gdal.GDT_UInt16\n    else:\n        datatype = gdal.GDT_Float32\n\n    if len(img_arr.shape) == 3:\n        img_bands, img_height, img_width = img_arr.shape\n        driver = gdal.GetDriverByName(\"GTiff\")\n        dataset = driver.Create(path, int(img_width), int(img_height), int(img_bands), datatype)\n    #     print(path, int(img_width), int(img_height), int(img_bands), datatype)\n        if(dataset!= None) and (geomatrix !='') and (projection!=''):\n            dataset.SetGeoTransform(geomatrix) #写入仿射变换参数\n            dataset.SetProjection(projection) #写入投影\n        for i in range(img_bands):\n            dataset.GetRasterBand(i+1).WriteArray(img_arr[i])\n        del dataset\n\n    elif len(img_arr.shape) == 2:\n        # img_arr = np.array([img_arr])\n        img_height, img_width = img_arr.shape\n        img_bands=1\n        #创建文件\n        driver = gdal.GetDriverByName(\"GTiff\")\n        dataset = driver.Create(path, int(img_width), int(img_height), int(img_bands), datatype)\n    #     print(path, int(img_width), int(img_height), int(img_bands), datatype)\n        if(dataset!= None):\n            dataset.SetGeoTransform(geomatrix) #写入仿射变换参数\n            dataset.SetProjection(projection) #写入投影\n        dataset.GetRasterBand(1).WriteArray(img_arr)\n        del dataset\n \ndef merge_tif(tif_path_list, output_tif):\n    arr_list = []\n    for tif_path in tif_path_list:\n        proj, geotrans, data, row, column  = read_img(tif_path)\n        arr_list.append(data)\n    all_arr = np.array(arr_list)\n    Write_Tiff(all_arr,geotrans,proj,output_tif)\n    \n\n筛选数据\n\ntif_list = os.listdir(tif_10m_save_path)   # 获取所有10m分辨率tif数据的名称\ntif_path_list = []                        # 以列表的形式存储各个波段的路径\n\nband_names = [\"B02\",\"B03\",\"B04\",\"B08\",\"B8A\",\"B11\",\"B12\"]  # 需要堆叠的波段名\n\nfor band_name in band_names:\n    for tif_name in tif_list:\n        if band_name == tif_name[-11:-8]:\n            tif_path_list.append(tif_10m_save_path + os.sep + tif_name)\n\nprint(tif_path_list)\n\n['.\\\\data\\\\T50TLK_20220825T030519\\\\10m\\\\T50TLK_20220825T030519_B02_10m.tif', '.\\\\data\\\\T50TLK_20220825T030519\\\\10m\\\\T50TLK_20220825T030519_B03_10m.tif', '.\\\\data\\\\T50TLK_20220825T030519\\\\10m\\\\T50TLK_20220825T030519_B04_10m.tif', '.\\\\data\\\\T50TLK_20220825T030519\\\\10m\\\\T50TLK_20220825T030519_B08_10m.tif', '.\\\\data\\\\T50TLK_20220825T030519\\\\10m\\\\T50TLK_20220825T030519_B8A_10m.tif', '.\\\\data\\\\T50TLK_20220825T030519\\\\10m\\\\T50TLK_20220825T030519_B11_10m.tif', '.\\\\data\\\\T50TLK_20220825T030519\\\\10m\\\\T50TLK_20220825T030519_B12_10m.tif']\n\n\n\n# 执行叠加函数\nmerge_out =save_path+img_identifier+os.sep+img_identifier+\"_merge.tif\"\nmerge_tif(tif_path_list, merge_out)\n\n\n\n真彩色影像可视化\n\n对图像进行拉伸显示\n转换成0-255的快视图并保存\n\n\n# 显示叠加结果数据信息\n\n#   归一化函数\ndef stretch(band, lower_percent=2, higher_percent=98): #2和95表示分位数\n    band=np.array(band,dtype=np.float32)\n    c = np.percentile(band, lower_percent)*1.0\n    d = np.percentile(band, higher_percent)*1.0       \n    band[band<c] = c\n    band[band>d] = d\n    out =  (band - c)  / (d - c)  \n    return out.astype(np.float32)\ndef stretch_n(data, n_band=3):  #该操作讲改变原始数据，因此data用.copy，不对原始数据进行更改\n    data=np.array(data,dtype=np.float32)\n    for k in range(n_band):\n            data[:,:,k] = stretch(data[:,:,k])\n    return data\ndef rgb(img_data,iftran=True):\n    img_data_3b = img_data[:3,:,:]                  # 取前三个波段 B02,B03,B04\n    if iftran:\n        img_data_3b = img_data_3b[::-1,:,:]             # 将B02,B03,B04转成B04,B03,B02 (BGR转RGB)\n    img_data    = img_data_3b.transpose(1,2,0)     # C,H,W -> H,W,C\n    return img_data \n\nproj, geotrans, img_data, row, column  = read_img(merge_out)\nimg_data_r=rgb(img_data) #提取3波段改变rgb顺序和数据维度\nimg_data_rgb_s = np.uint8(stretch_n(img_data_r.copy())*255) # 数据值域缩放至（0~255）\n\nplt.figure(figsize=(8,8))\nplt.imshow(Image.fromarray(img_data_rgb_s))\nplt.show()\n\n仿射矩阵信息：(300000.0, 10.0, 0.0, 4500000.0, 0.0, -10.0)\n投影信息：PROJCS[\"WGS 84 / UTM zone 50N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32650\"]]\n栅格波段数：7\n栅格列数（宽度）：10980 栅格行数（高度）：10980\n\n\n\n\n\n\n\n\nquickimg =save_path+img_identifier+os.sep+img_identifier+\"_quickimg.tif\"\nWrite_Tiff(img_data_rgb_s.transpose(2,0,1),ds.GetGeoTransform(),ds.GetProjection(),quickimg)\n\n\n\n重投影\n如果需要对数据进行投影操作，可以利用gdal.warp实现\n\nds = gdal.Open(quickimg)       # 打开文件\nrojectedtmp =save_path+img_identifier+os.sep+img_identifier+\"_projected.tif\"\nds = gdal.Warp(rojectedtmp, ds, dstSRS='EPSG:4326')    # 有投影的需求可以使用warp命令，epsg可以通过https://epsg.io/查询，这里给出的是wgs84\n\n\nproj, geotrans, img_data, row, column  = read_img(rojectedtmp)\n\n# 显示重投影结果信息\nprint(f'仿射矩阵信息：{geotrans}',f'投影信息：{proj}')\nprint(\"\\n\")\n\nimg_data_ = img_data.transpose(1,2,0)          # C,H,W -> H,W,C\nplt.imshow(img_data_)\n\n仿射矩阵信息：(114.63531921298156, 0.00010513256977891853, 0.0, 40.64592849014489, 0.0, -0.00010513256977891853) 投影信息：GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]\n\n\n\n\n<matplotlib.image.AxesImage at 0x16971629430>\n\n\n\n\n\n\n\n数据裁剪\n\n利用rasterio库更容易将shp与影像进行叠加显示\n利用gdal.warp实现数据裁剪\n\n\nclip_shp = './data/clippolygon.shp'\nvector = gpd.read_file(clip_shp)\nimg=rasterio.open(quickimg)\n\nfig, ax = plt.subplots(figsize=(5,5))\np1 =rasterio.plot.show(img, ax=ax,title='Sentinel 2 image of Guanting Reservoir')\nvector.plot(ax=ax,edgecolor='red', linewidth=1,facecolor = \"none\") #如果shp与raster坐标投影不一致无法同时显示\nplt.show()\n\n\n\n\n按矢量轮廓裁剪\n\n# 执行裁剪\nclip_output =save_path+img_identifier+os.sep+img_identifier+\"_clip.tif\"\n# 按矢量轮廓裁剪\ngdal.Warp(clip_output, merge_out, cutlineDSName = clip_shp, format=\"GTiff\", cropToCutline = True)\n\n<osgeo.gdal.Dataset; proxy of <Swig Object of type 'GDALDatasetShadow *' at 0x00000169B2690810> >\n\n\n\nproj, geotrans, img_data, row, column  = read_img(clip_output)\nimg_data_=rgb(img_data)\nplt.imshow(stretch_n(img_data_.copy()))\n\n<matplotlib.image.AxesImage at 0x16971673eb0>"
  },
  {
    "objectID": "posts/2022-12-10wrsdp_1-3-2/index.html",
    "href": "posts/2022-12-10wrsdp_1-3-2/index.html",
    "title": "基于遥感指数提取水体",
    "section": "",
    "text": "阈值法是比较简单但却较为有效地水体提取方法，且多数地复杂方法地基础仍然是阈值法。目前，较为常用的阈值提取方法主要为NDWI、MNDWI以及部分针对特殊影像的阈值提取方法。此类方法是利用水体在部分波段中反射率较高，而在一些波段中反射率低的特点，通过波段之间的计算来凸显水体，并且通过设定阈值，从而达到提取水体的目的。\nNDWI（归一化差异水体指数）:\n\nNDWI = (GREEN-NIR)/(GREEN+NIR)\n\n式中GREEN表示绿光波段的反射率，NIR表示近红外波段的反射率。该方法尽管已经较为古老，但其是最为常用的水体提取方法（部分高分辨率数据仅有4个波段），并且目前很多的水体指数法都是在该方法地基础上进行地变化。该方法对于大部分的常规水体均可有效提取，但是同样受到其他因素的影响较大。\nNDWI（改进的归一化差异水体指数）：\n\nMNDWI=(GREEN-SWIR)/(GREEN+SWIR)\n\n式中GREEN表示绿光波段的反射率，SWIR表示中红外波段的反射率。该方法是NDWI的变种，但是总体提取精度要优于NDWI。但是对于部分高分辨率数据不适用（不存在中红外波段），因此在进行中低分辨率的水体自动提取时常用。且由于该方法对于部分湖泊湿地中的富营养化的水体提取也较为适用，因此也常用来提取此类水体。\n环境要求： \n\nGDAL==2.4.1\nnumpy==1.19.5\nPyCRS==1.0.2\nscikit-image==0.17.2\n\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nfrom osgeo import gdal, osr, ogr, gdalconst\nimport os,glob,time,rasterio\nimport shutil\nimport rasterio.plot\nimport geopandas as gpd\n\n\n## 读图像文件\ndef read_img(filename):\n\n    dataset = gdal.Open(filename)  # 打开文件\n\n    im_width = dataset.RasterXSize  # 栅格矩阵的列数\n    im_height = dataset.RasterYSize  # 栅格矩阵的行数\n    # im_bands = dataset.RasterCount  # 波段数\n    im_geotrans = dataset.GetGeoTransform()  # 仿射矩阵，左上角像素的大地坐标和像素分辨率\n    im_proj = dataset.GetProjection()  # 地图投影信息，字符串表示\n    im_data = dataset.ReadAsArray(0, 0, im_width, im_height)\n\n    del dataset   #关闭对象dataset，释放内存\n    # return im_width, im_height, im_proj, im_geotrans, im_data,im_bands\n    return  im_proj, im_geotrans, im_data, im_width,im_height\n \n## 将numpy形式的遥感影像写出为GeoTiff文件\ndef Write_Tiff(img_arr, geomatrix, projection,path):\n#     img_bands, img_height, img_width = img_arr.shape\n    if 'int8' in img_arr.dtype.name:\n        datatype = gdal.GDT_Byte\n    elif 'int16' in img_arr.dtype.name:\n        datatype = gdal.GDT_UInt16\n    else:\n        datatype = gdal.GDT_Float32\n\n    if len(img_arr.shape) == 3:\n        img_bands, img_height, img_width = img_arr.shape\n        driver = gdal.GetDriverByName(\"GTiff\")\n        dataset = driver.Create(path, int(img_width), int(img_height), int(img_bands), datatype)\n    #     print(path, int(img_width), int(img_height), int(img_bands), datatype)\n        if(dataset!= None) and (geomatrix !='') and (projection!=''):\n            dataset.SetGeoTransform(geomatrix) #写入仿射变换参数\n            dataset.SetProjection(projection) #写入投影\n        for i in range(img_bands):\n            dataset.GetRasterBand(i+1).WriteArray(img_arr[i])\n        del dataset\n\n    elif len(img_arr.shape) == 2:\n        # img_arr = np.array([img_arr])\n        img_height, img_width = img_arr.shape\n        img_bands=1\n        #创建文件\n        driver = gdal.GetDriverByName(\"GTiff\")\n        dataset = driver.Create(path, int(img_width), int(img_height), int(img_bands), datatype)\n    #     print(path, int(img_width), int(img_height), int(img_bands), datatype)\n        if(dataset!= None):\n            dataset.SetGeoTransform(geomatrix) #写入仿射变换参数\n            dataset.SetProjection(projection) #写入投影\n        dataset.GetRasterBand(1).WriteArray(img_arr)\n        del dataset\n        \n## 计算数据头尾分位数的方式进行归一化，剔除异常值\ndef stretch(band, lower_percent=2, higher_percent=98): #2和98表示分位数\n    band=np.array(band,dtype=np.float32)\n    c = np.percentile(band, lower_percent)*1.0\n    d = np.percentile(band, higher_percent)*1.0       \n    band[band<c] = c\n    band[band>d] = d\n    out =  (band - c)  / (d - c)  \n    return out.astype(np.float32)\ndef stretch_n(data, n_band=3):  #该操作讲改变原始数据，因此data用.copy，不对原始数据进行更改\n    data=np.array(data,dtype=np.float32)\n    for k in range(n_band):\n            data[:,:,k] = stretch(data[:,:,k])\n    return data\n\ndef rgb(img_data,iftran=True):\n    img_data_3b = img_data[:3,:,:]                  # 取前三个波段 B02,B03,B04\n    if iftran:\n        img_data_3b = img_data_3b[::-1,:,:]             # 将B02,B03,B04转成B04,B03,B02 (BGR转RGB)\n    img_data    = img_data_3b.transpose(1,2,0)     # C,H,W -> H,W,C\n    return img_data \n\n读取多波段影像，剔除异常值，并提取rgb信息进行可视化\n\nnew_stack = \"./data/T50TLK_20220825T030519/T50TLK_20220825T030519_clip.tif\"\nout_path='./data/T50TLK_20220825T030519/'\nproj, geotrans, img_data, row, column  = read_img(new_stack)\n# 显示重投影结果信息\nprint(f'仿射矩阵信息：{geotrans}',f'投影信息：{proj}',f'图像大小：{img_data.shape}')\nimg_data_=rgb(img_data)\nplt.imshow(stretch_n(img_data_.copy()))\n\n仿射矩阵信息：(371350.0, 10.0, 0.0, 4480810.0, 0.0, -10.0) 投影信息：PROJCS[\"WGS 84 / UTM zone 50N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32650\"]] 图像大小：(7, 2874, 3789)\n\n\n<matplotlib.image.AxesImage at 0x246c2062c40>\n\n\n\n\n\n\n\n\nNDWI（归一化差异水体指数）：NDWI = (GREEN-NIR)/(GREEN+NIR)\nMNDWI（改进的归一化差异水体指数）：MNDWI=(GREEN-SWIR)/(GREEN+SWIR)\n通过直方图观察数值分布情况，初步定位阈值区间\n\n\nGreen_arr = img_data[1,:,:]\nNIR_arr   = img_data[3,:,:]\n\ndenominator = np.array(Green_arr + NIR_arr, dtype=np.float32)\nnumerator = np.array(Green_arr - NIR_arr, dtype=np.float32)\nnodata = np.full((Green_arr.shape[0], Green_arr.shape[1]), -2, dtype=np.float32)\nndwi = np.true_divide(numerator, denominator, out=nodata, where=denominator != 0.0)\nprint(np.min(ndwi),np.max(ndwi))\nndwi[ndwi == -2.0]=None\n\nndwipath=out_path+'ndwi.tif'\nWrite_Tiff(np.uint8(ndwi), geotrans,proj, ndwipath)\n\nfig, axes = plt.subplots(1,3,figsize=(20,5))\nplt.subplot(1,3,1),plt.title('RGB'), plt.axis('off')\nplt.imshow(stretch_n(img_data_.copy()))\nplt.subplot(1,3,2),plt.title('NDWI'), plt.axis('off')\nplt.imshow(ndwi)\n# plt.style.use(\"ggplot\")\nplt.subplot(1,3,3),plt.title('Histogram'), plt.axis('off')\nplt.hist(ndwi.ravel(), bins=100, density=None, facecolor='green', alpha=0.75)\nplt.show()\n\n0.0 30.031637\n\n\n\n\n\n通过可视化查看不同阈值条件下水体提取效果\n\nfig = plt.figure(figsize=(20,6))\nfor i in range(5):\n    ndwi_ = ndwi.copy()\n    threshold = (i+1) *0.05\n    plt.subplot(1,5,i+1),plt.title('threshold=  %.3f' % threshold), plt.axis('off')\n    ndwi_ [ndwi_ >threshold] = 255\n    ndwi_ [ndwi_ <=threshold] = 0\n    plt.imshow(Image.fromarray(np.uint8(ndwi_)))\nplt.show()\nfig = plt.figure(figsize=(20,6))\nfor i in range(5):\n    ndwi_ = ndwi.copy()\n    threshold = (i+1) *2.5\n    plt.subplot(1,5,i+1),plt.title('threshold=  %.3f' % threshold), plt.axis('off')\n    ndwi_ [ndwi_ >threshold] = 255\n    ndwi_ [ndwi_ <=threshold] = 0\n    plt.imshow(Image.fromarray(np.uint8(ndwi_)))\nplt.show()\n\n\n\n\n\n\n\n对比NDWI和mNDWI的效果\n\nGreen_arr = img_data[1,:,:]\nSWIR_arr   = img_data[5,:,:]\n\ndenominator = np.array(Green_arr + SWIR_arr, dtype=np.float32)\nnumerator = np.array(Green_arr - SWIR_arr, dtype=np.float32)\nnodata = np.full((Green_arr.shape[0], Green_arr.shape[1]), -2, dtype=np.float32)\nmndwi = np.true_divide(numerator, denominator, out=nodata, where=denominator != 0.0)\nmndwi[mndwi == -2.0]=None\nfig, axes = plt.subplots(1,3,figsize=(20,5))\nplt.subplot(1,3,1),plt.title('RGB'), plt.axis('off')\nplt.imshow(stretch_n(img_data_.copy()))\nplt.subplot(1,3,2),plt.title('NDWI'), plt.axis('off')\nplt.imshow(ndwi)\nplt.subplot(1,3,3),plt.title('mNDWI'), plt.axis('off')\nplt.imshow(Image.fromarray(np.uint8(mndwi)))\nplt.show()\n\n\n\n\n\nfig = plt.figure(figsize=(20,6))\nfor i in range(5):\n    ndwi_ = mndwi.copy()\n    threshold = (i+1) *0.05\n    plt.subplot(1,5,i+1),plt.title('threshold=  %.3f' % threshold), plt.axis('off')\n    ndwi_ [ndwi_ >threshold] = 255\n    ndwi_ [ndwi_ <=threshold] = 0\n    plt.imshow(Image.fromarray(np.uint8(ndwi_)))\nplt.show()\nfig = plt.figure(figsize=(20,6))\nfor i in range(5):\n    ndwi_ = mndwi.copy()\n    threshold = (i+1) *2.5\n    plt.subplot(1,5,i+1),plt.title('threshold=  %.3f' % threshold), plt.axis('off')\n    ndwi_ [ndwi_ >threshold] = 255\n    ndwi_ [ndwi_ <=threshold] = 0\n    plt.imshow(Image.fromarray(np.uint8(ndwi_)))\nplt.show()\n\n\n\n\n\n\n\n阈值分割进行二值化处理得到水体掩膜\n\nndwi_ = ndwi.copy()\nthreshold = 2.5\nndwi_ [ndwi_ >threshold] = 255\nndwi_ [ndwi_ <=threshold] = 0\nplt.imshow(Image.fromarray(np.uint8(ndwi_)),cmap='gray')\nout_ndwi=out_path+'out_ndwi.tif'\nWrite_Tiff(np.uint8(ndwi_), geotrans,proj, out_ndwi)\n\n\n\n\n\n\n\n\n使用阈值分割法会使图像上布满不规则小图斑，影响分割精度\n利用滤波等候处理方法降低结果噪声\n\n\n \ndef Speckle_removal(tif_path, save_path,  remove_pixels =100, neighbours = 8 ):\n\n    filename = os.path.basename(tif_path)\n    output_path = os.path.join(save_path, filename[:-4] + '_sr.tif' )\n\n    if not os.path.exists(save_path):\n        os.makedirs(save_path)\n    shutil.copy(tif_path, output_path)\n \n    # remove_pixels =100    #碎斑像素\n    # neighbours = 8    #连通域， 4或8\n    Image = gdal.Open(output_path, 1)  # open image in read-write mode\n    Band = Image.GetRasterBand(1)\n    gdal.SieveFilter(srcBand=Band, maskBand=None, dstBand=Band,\n                    threshold= remove_pixels, \n                    connectedness= neighbours,\n                    callback=gdal.TermProgress_nocb)\n \n    del Image, Band  # close the datasets.\n    return output_path\n\n\nsr_out = Speckle_removal(out_ndwi, out_path,  remove_pixels =1000, neighbours = 8 )\nproj, geotrans, sr_arr, row, column  = read_img(sr_out)\nsr_arr.shape\n\n(2874, 3789)\n\n\n对比后处理前后变化\n\nproj, geotrans, sr_arr, row, column  = read_img(sr_out)\nfig, axes = plt.subplots(1,2,figsize=(20,5))\nplt.subplot(1,3,1),plt.title('NDWI'), plt.axis('off')\nplt.imshow(Image.fromarray(np.uint8(ndwi_)))\nplt.subplot(1,3,2),plt.title('NDWI_sr'), plt.axis('off')\nplt.imshow(sr_arr)\nplt.show()\n\nC:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_28856\\1761671902.py:3: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n  plt.subplot(1,3,1),plt.title('NDWI'), plt.axis('off')\nC:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_28856\\1761671902.py:5: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n  plt.subplot(1,3,2),plt.title('NDWI_sr'), plt.axis('off')\n\n\n\n\n\n二值化栅格转矢量输出\n\ndef tif_to_shp(tif_path, shp_save_path):\n    inraster = gdal.Open(tif_path)  # 读取路径中的栅格数据\n    inband = inraster.GetRasterBand(1)  # 这个波段就是最后想要转为矢量的波段，如果是单波段数据的话那就都是1\n    prj = osr.SpatialReference()\n    prj.ImportFromWkt(inraster.GetProjection())  # 读取栅格数据的投影信息，用来为后面生成的矢量做准备\n    outshp = shp_save_path + os.path.basename(tif_path)[:-4] + '.shp'  # 给后面生成的矢量准备一个输出文件名，这里就是把原栅格的文件名后缀名改成shp了\n\n    drv = ogr.GetDriverByName(\"ESRI Shapefile\")\n    if os.path.exists(outshp):  # 若文件已经存在，则删除它继续重新做一遍\n        drv.DeleteDataSource(outshp)\n    Polygon = drv.CreateDataSource(outshp)  # 创建一个目标文件\n    Poly_layer = Polygon.CreateLayer(os.path.basename(tif_path)[:-4], srs=prj, geom_type=ogr.wkbMultiPolygon)  # 对shp文件创建一个图层，定义为多个面类\n    newField = ogr.FieldDefn('value', ogr.OFTReal)  # 给目标shp文件添加一个字段，用来存储原始栅格的pixel value,浮点型，\n    Poly_layer.CreateField(newField)\n\n    gdal.Polygonize(inband, None, Poly_layer, 0)  # 核心函数，执行的就是栅格转矢量操作\n\n    Polygon.SyncToDisk()\n    Polygon = None\n    return outshp\n\n\n# 执行转换函数，获取存储路径\nndwi_shp = tif_to_shp(sr_out,out_path)\n\n\n\n\n\n利用rasterio更容易实现将shape文件与栅格影像进行叠加显示\nplt.imshow更适合临时出图，更专业的出版需求通常采用axs这种方式实现\n\n\nfig, axs = plt.subplots(2, 2, figsize=(12,10))\n\nvector = gpd.read_file(ndwi_shp)\nimg=rasterio.open(out_path+\"T50TLK_20220825T030519_quickclip.tif\")\n\nndwiimg=rasterio.open(ndwipath)\nsrimg=rasterio.open(sr_out)\np1 =rasterio.plot.show(img, ax=axs[0,0],title='RGB')\nndwi =rasterio.plot.show(ndwiimg, ax=axs[0,1],title='NDWI')\nsr =rasterio.plot.show(srimg, ax=axs[1,0],title='Water Mask')\np1 =rasterio.plot.show(img, ax=axs[1,1],title='Water Ploygon')\nvector.plot(ax=axs[1,1],edgecolor='red', linewidth=0.5,facecolor='none')\nfig.suptitle('Water Extraction Steps', fontsize=40)\n\nfor ax in fig.get_axes():\n    ax.label_outer()\n    ax.ticklabel_format(style ='plain') \nfig.tight_layout()\nplt.subplots_adjust(left=None, bottom=None, right=None, top=0.9, wspace=None, hspace=0.2)\n\nfig.savefig(out_path+\"T50TLK_20220825T030519_result.png\",facecolor='white')"
  },
  {
    "objectID": "talks/2022-11-23_NKRDP-DL/index.html",
    "href": "talks/2022-11-23_NKRDP-DL/index.html",
    "title": "Identification of Correlation Factors of Hidden Geohazard via Deep Learning",
    "section": "",
    "text": "Slides:"
  }
]